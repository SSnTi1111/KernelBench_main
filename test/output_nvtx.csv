==PROF== Connected to process 2894649 (/usr/bin/python3.12)
开始 Profiling...
==PROF== Disconnected from process 2894649
"ID","Process ID","Process Name","Host Name","Kernel Name","Context","Stream","Block Size","Grid Size","Device","CC","Section Name","Metric Name","Metric Unit","Metric Value","Rule Name","Rule Type","Rule Description","Estimated Speedup Type","Estimated Speedup"
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1434319526.63",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1007554945.05",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","5458",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","GPU Speed Of Light Throughput","Memory Throughput","%","2.44",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","GPU Speed Of Light Throughput","DRAM Throughput","%","1.49",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","GPU Speed Of Light Throughput","Duration","nsecond","5408",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","4.39",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","3.59",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2524.40",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","6.42",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details.","",""
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved  close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","0.51",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","0.24",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Compute Workload Analysis","Issue Slots Busy","%","13.87",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","0.55",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Compute Workload Analysis","SM Busy","%","13.87",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.","local","91.43"
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Memory Workload Analysis","Memory Throughput","byte/second","27266272189.35",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Memory Workload Analysis","Mem Busy","%","2.04",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Memory Workload Analysis","Max Bandwidth","%","2.44",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Memory Workload Analysis","L1/TEX Hit Rate","%","28.07",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Memory Workload Analysis","L2 Hit Rate","%","69.82",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Memory Workload Analysis","Mem Pipes Busy","%","1.96",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 1.5 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory request.","global","0.5176"
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Scheduler Statistics","One or More Eligible","%","14.08",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.14",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Scheduler Statistics","No Eligible","%","85.92",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Scheduler Statistics","Active Warps Per Scheduler","warp","2.36",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.19",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 7.1 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 2.36 active warps per scheduler, but only an average of 0.19 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp State Statistics and Source Counters sections.","local","85.92"
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","16.73",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","18.04",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Warp State Statistics","Avg. Active Threads Per Warp","","24.21",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","22.55",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 5.4 cycles being stalled waiting for an immediate constant cache (IMC) miss. A read from constant memory costs one memory read from device memory only on a cache miss; otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS instruction as 'c[bank][offset]'. Accesses to different addresses by threads within a warp are serialized, thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As such, the constant cache is best when threads in the same warp access only a few distinct locations. If all threads of a warp access the same location, then constant memory can be as fast as a register access. This stall type represents about 32.1% of the total average of 16.7 cycles between issuing two instructions.","global","32.11"
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 5.2 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 30.9% of the total average of 16.7 cycles between issuing two instructions.","global","30.9"
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","WarpStateStats","","","","ThreadDivergence","OPT","Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early thread completion, and divergent flow control can significantly lower the number of active threads in a warp per cycle. This kernel achieves an average of 24.2 threads being active per cycle. This is further reduced to 22.6 threads per warp due to predication. The compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Try to avoid different execution paths within a warp when possible.","global","1.897"
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","324.74",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Instruction Statistics","Executed Instructions","inst","140288",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","350.07",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Instruction Statistics","Issued Instructions","inst","151232",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 0 fused and 1152 non-fused FP32 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its current performance). Check the Source page to identify where this kernel executes FP32 instructions.","global","3.862"
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Launch Statistics","Block Size","","256",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Launch Statistics","Grid Size","","128",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Launch Statistics","Registers Per Thread","register/thread","32",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Launch Statistics","Shared Memory Configuration Size","byte","102400",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1024",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Launch Statistics","Static Shared Memory Per Block","byte/block","4224",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Launch Statistics","Threads","thread","32768",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Launch Statistics","Waves Per SM","","0.15",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","LaunchStats","","","","LaunchConfiguration","OPT","If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the hardware busy.","",""
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Occupancy","Block Limit SM","block","32",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Occupancy","Block Limit Registers","block","8",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Occupancy","Block Limit Shared Mem","block","19",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Occupancy","Block Limit Warps","block","8",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Occupancy","Theoretical Active Warps per SM","warp","64",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Occupancy","Theoretical Occupancy","%","100",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Occupancy","Achieved Occupancy","%","14.55",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Occupancy","Achieved Active Warps Per SM","warp","9.31",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (100.0%) and measured achieved occupancy (14.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","global","85.45"
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Source Counters","Branch Instructions Ratio","%","0.19",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Source Counters","Branch Instructions","inst","26112",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Source Counters","Branch Efficiency","%","92.31",
"0","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(1, 2, 64)","0","8.0","Source Counters","Avg. Divergent Branches","","2.37",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1501436835.23",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1049378298.86",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","361305",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","GPU Speed Of Light Throughput","Memory Throughput","%","78.98",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","GPU Speed Of Light Throughput","DRAM Throughput","%","78.98",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","GPU Speed Of Light Throughput","Duration","nsecond","342976",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","27.20",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","76.68",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","356877.20",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","34.77",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","SpeedOfLight","","","","SOLBottleneck","OPT","Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or whether there are values you can (re)compute.","",""
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 1% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.40",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.39",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Compute Workload Analysis","Issue Slots Busy","%","35.07",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.40",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Compute Workload Analysis","SM Busy","%","35.07",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","FMA is the highest-utilized pipeline (26.9%) based on active cycles, taking into account the rates of its different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD) operations. It is well-utilized, but should not be a bottleneck.","",""
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Memory Workload Analysis","Memory Throughput","byte/second","1517838775891.02",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Memory Workload Analysis","Mem Busy","%","40.94",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Memory Workload Analysis","Max Bandwidth","%","78.98",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Memory Workload Analysis","L1/TEX Hit Rate","%","0",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Memory Workload Analysis","L2 Hit Rate","%","62.14",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Memory Workload Analysis","Mem Pipes Busy","%","22.93",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Scheduler Statistics","One or More Eligible","%","34.91",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.35",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Scheduler Statistics","No Eligible","%","65.09",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Scheduler Statistics","Active Warps Per Scheduler","warp","15.06",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","1.09",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 2.9 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 15.06 active warps per scheduler, but only an average of 1.09 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","65.09"
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","43.13",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","43.17",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.60",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 31.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 72.0% of the total average of 43.1 cycles between issuing two instructions.","global","72.02"
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","125012.15",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Instruction Statistics","Executed Instructions","inst","54005248",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","125140.78",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Instruction Statistics","Issued Instructions","inst","54060817",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 0 fused and 2097152 non-fused FP32 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its current performance). Check the Source page to identify where this kernel executes FP32 instructions.","global","13.43"
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Launch Statistics","Block Size","","256",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Launch Statistics","Grid Size","","65536",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Launch Statistics","Registers Per Thread","register/thread","32",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Launch Statistics","Shared Memory Configuration Size","byte","102400",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1024",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Launch Statistics","Static Shared Memory Per Block","byte/block","4224",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Launch Statistics","Threads","thread","16777216",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Launch Statistics","Waves Per SM","","75.85",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Occupancy","Block Limit SM","block","32",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Occupancy","Block Limit Registers","block","8",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Occupancy","Block Limit Shared Mem","block","19",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Occupancy","Block Limit Warps","block","8",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Occupancy","Theoretical Active Warps per SM","warp","64",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Occupancy","Theoretical Occupancy","%","100",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Occupancy","Achieved Occupancy","%","94.52",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Occupancy","Achieved Active Warps Per SM","warp","60.49",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Source Counters","Branch Instructions Ratio","%","0.11",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Source Counters","Branch Instructions","inst","6030144",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Source Counters","Branch Efficiency","%","100",
"1","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nchwToNhwcKernel<float, float, float, (bool)0, (bool)1, (cudnnKernelDataType_t)2>(cudnn::engines_precompiled::nchw2nhwc_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8192, 2, 4)","0","8.0","Source Counters","Avg. Divergent Branches","","0",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1509415391.91",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1058044713.79",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","1145653",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","GPU Speed Of Light Throughput","Memory Throughput","%","80.66",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","GPU Speed Of Light Throughput","DRAM Throughput","%","25.18",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","GPU Speed Of Light Throughput","Duration","nsecond","1079424",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","81.29",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","51.10",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1133258.56",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","61.69",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","SpeedOfLight","","","","SOLBottleneck","INF","The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To further improve performance, work will likely need to be shifted from the most utilized to another unit. Start by analyzing L1 in the Memory Workload Analysis section.","",""
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 1% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.60",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.58",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Compute Workload Analysis","Issue Slots Busy","%","39.91",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.60",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Compute Workload Analysis","SM Busy","%","62.17",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","OPT","Tensor is the highest-utilized pipeline (62.2%) based on active cycles, taking into account the rates of its different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its Tensor (FP) sub-pipeline. The pipeline is well-utilized, but might become a bottleneck if more work is added. Based on the number of executed instructions, the highest utilized pipeline (33.1%) is LSU. It executes load/store memory operations. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-decoder) or hover over the pipeline name to understand the workloads handled by each pipeline. The Instruction Statistics section shows the mix of executed instructions in this kernel. Check the Warp State Statistics section for which reasons cause warps to stall.","",""
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Memory Workload Analysis","Memory Throughput","byte/second","486488675441.72",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Memory Workload Analysis","Mem Busy","%","80.66",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Memory Workload Analysis","Max Bandwidth","%","51.10",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Memory Workload Analysis","L1/TEX Hit Rate","%","18.74",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Memory Workload Analysis","L2 Hit Rate","%","87.33",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Memory Workload Analysis","Mem Pipes Busy","%","32.85",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global loads in L1TEX might not be optimal. On average, this kernel accesses 9.9 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 16.0 sectors per request, or 16.0*32 = 510.7 bytes of cache data transfers per request. The optimal thread address pattern for 9.9 byte accesses would result in 9.9*32 = 317.8 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global loads.","global","2.334"
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 9.9 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 16.0 sectors per request, or 16.0*32 = 512.0 bytes of cache data transfers per request. The optimal thread address pattern for 9.9 byte accesses would result in 9.9*32 = 317.8 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.","global","2.344"
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","MemoryWorkloadAnalysis_Tables","","","","SharedMemoryConflicts","OPT","The memory access pattern for shared stores might not be optimal and causes on average a 2.5 - way bank conflict across all 1056896 shared store requests.This results in 552006 bank conflicts,  which represent 20.72% of the overall 2664387 wavefronts for shared stores. Check the Source Counters section for uncoalesced shared stores.","global","16.84"
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Scheduler Statistics","One or More Eligible","%","39.93",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.40",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Scheduler Statistics","No Eligible","%","60.07",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Scheduler Statistics","Active Warps Per Scheduler","warp","1.97",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","0.52",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 2.5 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 1.97 active warps per scheduler, but only an average of 0.52 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","60.07"
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","4.94",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","4.94",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Warp State Statistics","Avg. Active Threads Per Warp","","32",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","29.90",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","452223.66",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Instruction Statistics","Executed Instructions","inst","195360620",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","452293.56",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Instruction Statistics","Issued Instructions","inst","195390820",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 2113792 fused and 2113792 non-fused FP32 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP32 performance could be increased by up to 25% (relative to its current performance). Check the Source page to identify where this kernel executes FP32 instructions.","global","1.757"
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Launch Statistics","Block Size","","128",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Launch Statistics","Grid Size","","8257",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Launch Statistics","Registers Per Thread","register/thread","167",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Launch Statistics","Shared Memory Configuration Size","byte","167936",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1024",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","73728",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Launch Statistics","Threads","thread","1056896",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Launch Statistics","Waves Per SM","","38.23",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Occupancy","Block Limit SM","block","32",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Occupancy","Block Limit Registers","block","3",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Occupancy","Block Limit Shared Mem","block","2",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Occupancy","Block Limit Warps","block","16",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Occupancy","Theoretical Occupancy","%","12.50",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Occupancy","Achieved Occupancy","%","12.34",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Occupancy","Achieved Active Warps Per SM","warp","7.90",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Occupancy","","","","TheoreticalOccupancy","OPT","The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 16. This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory.","global","87.5"
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Source Counters","Branch Instructions Ratio","%","0.01",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Source Counters","Branch Instructions","inst","1222036",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Source Counters","Branch Efficiency","%","100",
"2","2894649","python3.12","127.0.0.1","void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s1688dgrad_optimized_tf32_128x64_32x3_nhwc_unity_stride_align4>(T1::Params)","1","7","(128, 1, 1)","(8257, 1, 1)","0","8.0","Source Counters","Avg. Divergent Branches","","0",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/second","1495135450.04",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","GPU Speed Of Light Throughput","SM Frequency","cycle/second","1047986628.38",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","346266",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","GPU Speed Of Light Throughput","Memory Throughput","%","83.35",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","GPU Speed Of Light Throughput","DRAM Throughput","%","83.35",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","GPU Speed Of Light Throughput","Duration","nsecond","329568",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","35.88",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","91.23",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","342212.62",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","36.16",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","SpeedOfLight","","","","SOLBottleneck","INF","The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To further improve performance, work will likely need to be shifted from the most utilized to another unit. Start by analyzing DRAM in the Memory Workload Analysis section.","",""
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","SpeedOfLight_RooflineChart","","","","SOLFPRoofline","INF","The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 1% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline analysis.","",""
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Compute Workload Analysis","Executed Ipc Active","inst/cycle","1.46",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Compute Workload Analysis","Executed Ipc Elapsed","inst/cycle","1.45",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Compute Workload Analysis","Issue Slots Busy","%","36.49",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Compute Workload Analysis","Issued Ipc Active","inst/cycle","1.46",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Compute Workload Analysis","SM Busy","%","36.49",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","ComputeWorkloadAnalysis","","","","HighPipeUtilization","INF","FMA is the highest-utilized pipeline (24.7%) based on active cycles, taking into account the rates of its different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD) operations. It is well-utilized, but should not be a bottleneck.","",""
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Memory Workload Analysis","Memory Throughput","byte/second","1595148266822.02",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Memory Workload Analysis","Mem Busy","%","47.65",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Memory Workload Analysis","Max Bandwidth","%","83.35",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Memory Workload Analysis","L1/TEX Hit Rate","%","0.13",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Memory Workload Analysis","L2 Compression Success Rate","%","0",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Memory Workload Analysis","L2 Compression Ratio","","0",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Memory Workload Analysis","L2 Hit Rate","%","64.13",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Memory Workload Analysis","Mem Pipes Busy","%","24.08",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses 4.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between threads, results in 4.5 sectors per request, or 4.5*32 = 144.0 bytes of cache data transfers per request. The optimal thread address pattern for 4.0 byte accesses would result in 4.0*32 = 128.0 bytes of cache data transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for uncoalesced global stores.","global","0.3342"
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","MemoryWorkloadAnalysis_Tables","","","","MemoryCacheAccessPattern","OPT","The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only accesses an average of 2.4 sectors out of the possible 4 sectors per cache line. Check the Source Counters section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory request.","global","14.19"
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Scheduler Statistics","One or More Eligible","%","36.37",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Scheduler Statistics","Issued Warp Per Scheduler","","0.36",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Scheduler Statistics","No Eligible","%","63.63",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Scheduler Statistics","Active Warps Per Scheduler","warp","14.90",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Scheduler Statistics","Eligible Warps Per Scheduler","warp","1.23",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","SchedulerStats","","","","IssueSlotUtilization","OPT","Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only issues an instruction every 2.7 cycles. This might leave hardware resources underutilized and may lead to less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of 14.90 active warps per scheduler, but only an average of 1.23 warps were eligible per cycle. Eligible warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible warp results in no instruction being issued and the issue slot remains unused. To increase the number of eligible warps, avoid possible load imbalances due to highly different execution durations per warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.","local","63.63"
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Warp State Statistics","Warp Cycles Per Issued Instruction","cycle","40.96",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Warp State Statistics","Warp Cycles Per Executed Instruction","cycle","40.99",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Warp State Statistics","Avg. Active Threads Per Warp","","32.00",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Warp State Statistics","Avg. Not Predicated Off Threads Per Warp","","30.59",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","WarpStateStats","","","","CPIStall","OPT","On average, each warp of this kernel spends 25.1 cycles being stalled waiting for a scoreboard dependency on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to shared memory. This stall type represents about 61.3% of the total average of 41.0 cycles between issuing two instructions.","global","61.26"
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","WarpStateStats","","","","CPIStall","INF","Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on sampling data. The Kernel Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details on each stall reason.","",""
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Instruction Statistics","Avg. Executed Instructions Per Scheduler","inst","124779.11",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Instruction Statistics","Executed Instructions","inst","53904576",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Instruction Statistics","Avg. Issued Instructions Per Scheduler","inst","124884.27",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Instruction Statistics","Issued Instructions","inst","53950005",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","InstructionStats","","","","FPInstructions","OPT","This kernel executes 0 fused and 2113792 non-fused FP32 instructions. By converting pairs of non-fused instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its current performance). Check the Source page to identify where this kernel executes FP32 instructions.","global","12.33"
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Launch Statistics","Block Size","","256",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Launch Statistics","Grid Size","","66056",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Launch Statistics","Registers Per Thread","register/thread","32",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Launch Statistics","Shared Memory Configuration Size","byte","102400",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","1024",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Launch Statistics","Static Shared Memory Per Block","byte/block","4224",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Launch Statistics","Threads","thread","16910336",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Launch Statistics","Waves Per SM","","76.45",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Occupancy","Block Limit SM","block","32",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Occupancy","Block Limit Registers","block","8",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Occupancy","Block Limit Shared Mem","block","19",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Occupancy","Block Limit Warps","block","8",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Occupancy","Theoretical Active Warps per SM","warp","64",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Occupancy","Theoretical Occupancy","%","100",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Occupancy","Achieved Occupancy","%","93.55",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Occupancy","Achieved Active Warps Per SM","warp","59.87",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Source Counters","Branch Instructions Ratio","%","0.11",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Source Counters","Branch Instructions","inst","5813568",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Source Counters","Branch Efficiency","%","100",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","Source Counters","Avg. Divergent Branches","","0",
"3","2894649","python3.12","127.0.0.1","void cudnn::engines_precompiled::nhwcToNchwKernel<float, float, float, (bool)1, (bool)0, (cudnnKernelDataType_t)0>(cudnn::engines_precompiled::nhwc2nchw_params_t<T3>, const T1 *, T2 *)","1","7","(256, 1, 1)","(8257, 2, 4)","0","8.0","SourceCounters","","","","UncoalescedGlobalAccess","OPT","This kernel has uncoalesced global accesses resulting in a total of 1056768 excessive sectors (6% of the total 17965440 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The CUDA Programming Guide (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional information on reducing uncoalesced device memory accesses.","global","5.841"
