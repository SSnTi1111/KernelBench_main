import torch
from torch.utils.cpp_extension import load_inline
import os
import io
import re
import config
import time
import random
import subprocess 
import csv        
import io         
import json       
import sys        
import shutil
import importlib.util 
import tempfile 
import contextlib
import warnings
import traceback 
import weakref 
import numpy as np  
from typing import Dict, List, Any # <--- [ä¿®å¤] æ·»åŠ  List
import gc
import copy
import torch.nn as nn
from collections import defaultdict

# ç¼–è¯‘åçš„æ¨¡å—çš„å…¨å±€ç¼“å­˜
_gemm_module = None
# data_type_info = ""
# vvv --- [!!! å·²æ›´æ–° !!!] NCU æ¨¡æ¿ç°åœ¨æ˜¯é€šç”¨çš„ --- vvv
# NCU_TARGET_SCRIPT_TEMPLATE = """
# import torch
# import importlib.util
# import os
# import sys
# import traceback

# # 1. è·å–å‘½ä»¤è¡Œå‚æ•° (åªæœŸæœ› 2 ä¸ªå‚æ•°: è·¯å¾„å’Œæ¨¡å—å)
# MODULE_PATH = sys.argv[1]
# MODULE_NAME = sys.argv[2]
# # WRAPPER_FUNCTION_NAME = sys.argv[3] # <--- [å·²ç§»é™¤] ä¸å†éœ€è¦

# try:
#     # 2. åŠ è½½æ¨¡å—
#     spec = importlib.util.spec_from_file_location(MODULE_NAME, MODULE_PATH)
#     if spec is None:
#         print(f"Error: æ— æ³•ä» {MODULE_PATH} åŠ è½½ spec", file=sys.stderr)
#         sys.exit(1)
        
#     module = importlib.util.module_from_spec(spec)
#     spec.loader.exec_module(module)

#     # 3. å‡†å¤‡è®¾å¤‡å’Œæ•°æ®
#     torch.cuda.set_device(0)
#     device = torch.device("cuda")
    
#     try:
#         # ä»ä¿å­˜çš„æ–‡ä»¶ä¸­åŠ è½½è¾“å…¥
#         inputs = torch.load("_ncu_inputs.pt")
#         # ç¡®ä¿è¾“å…¥ç§»åŠ¨åˆ° GPU
#         gpu_inputs = [t.to(device) if isinstance(t, torch.Tensor) else t for t in inputs]
#     except Exception as e:
#         print(f"Failed to load _ncu_inputs.pt: {e}", file=sys.stderr)
#         traceback.print_exc()
#         sys.exit(1)

#     # 4. å®ä¾‹åŒ–æ¨¡å‹ (ModelNew)
#     # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ ModelNew çš„ __init__ ä¸éœ€è¦å‚æ•°ï¼Œæˆ–è€…å‚æ•°å·²ç¡¬ç¼–ç ã€‚
#     # å¯¹äº Level 1 çš„é—®é¢˜ï¼Œç”Ÿæˆçš„ä»£ç é€šå¸¸éµå¾ªè¿™ä¸€æ¨¡å¼ã€‚
#     if not hasattr(module, 'ModelNew'):
#         print(f"Error: æ¨¡å— {MODULE_NAME} ä¸­æœªæ‰¾åˆ° 'ModelNew' ç±»", file=sys.stderr)
#         sys.exit(1)
        
#     try:
#         model = module.ModelNew()
#         model.to(device)
#         model.eval() # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ (å½±å“æŸäº› layers å¦‚ Dropout/BatchNorm)
#     except Exception as e:
#         print(f"Error: å®ä¾‹åŒ– ModelNew å¤±è´¥: {e}", file=sys.stderr)
#         traceback.print_exc()
#         sys.exit(1)

#     torch.cuda.synchronize(device)
    
#     # --- 5. è¿è¡Œç›®æ ‡ (NCU åˆ†æåŒºåŸŸ) ---
#     # ä»…è¿è¡Œä¸€æ¬¡ï¼Œä¸è¿›è¡Œé¢„çƒ­ (NCU ä¸éœ€è¦é¢„çƒ­ï¼Œä¸” launch-count=1)
    
#     try:
#         model(*gpu_inputs)
#     except Exception as e:
#         print(f"Error: æ¨¡å‹æ‰§è¡Œå¤±è´¥: {e}", file=sys.stderr)
#         traceback.print_exc()
#         sys.exit(1)
        
#     # --- ç»“æŸåˆ†æ ---
    
#     torch.cuda.synchronize(device)

# except Exception as e:
#     print(f"NCU target script failed: {e}", file=sys.stderr)
#     traceback.print_exc()
#     sys.exit(1)
# """

NCU_TARGET_SCRIPT_TEMPLATE = """
import torch
import importlib.util
import os
import sys
import traceback

# 1. è·å–å‘½ä»¤è¡Œå‚æ•°
MODULE_PATH = sys.argv[1]
MODULE_NAME = sys.argv[2]

def move_to_cuda(item):
    if isinstance(item, torch.Tensor):
        return item.cuda()
    elif isinstance(item, (list, tuple)):
        # é€’å½’å¤„ç†åˆ—è¡¨æˆ–å…ƒç»„ï¼Œå¹¶ä¿æŒåŸæœ‰ç±»å‹
        return type(item)(move_to_cuda(x) for x in item)
    elif isinstance(item, dict):
        return {k: move_to_cuda(v) for k, v in item.items()}
    else:
        return item

try:
    # 2. åŠ è½½æ¨¡å—
    spec = importlib.util.spec_from_file_location(MODULE_NAME, MODULE_PATH)
    if spec is None:
        sys.exit(1)
        
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)

    # 3. å‡†å¤‡è®¾å¤‡å’Œæ•°æ®
    torch.cuda.set_device(0)
    device = torch.device("cuda")
    
    try:
        # åŠ è½½æ¨ç†è¾“å…¥
        inputs = torch.load("_ncu_inputs.pt")
        # [æ ¸å¿ƒä¿®æ”¹] åŠ è½½æ¨¡å‹åˆå§‹åŒ–å‚æ•°
        init_inputs = []
        if os.path.exists("_ncu_init_inputs.pt"):
            init_inputs = torch.load("_ncu_init_inputs.pt")
            
        gpu_inputs = [move_to_cuda(t) for t in inputs]
    except Exception as e:
        print(f"Failed to load data: {e}", file=sys.stderr)
        sys.exit(1)

    # 4. å®ä¾‹åŒ–æ¨¡å‹ (ModelNew)
    if not hasattr(module, 'ModelNew'):
        sys.exit(1)
        
    try:
        # [æ ¸å¿ƒä¿®æ”¹] ä½¿ç”¨ init_inputs å®ä¾‹åŒ–æ¨¡å‹ï¼Œè§£å†³å‚æ•°ç¼ºå¤±é—®é¢˜
        if isinstance(init_inputs, (list, tuple)):
            model = module.ModelNew(*init_inputs)
        elif isinstance(init_inputs, dict):
            model = module.ModelNew(**init_inputs)
        else:
            model = module.ModelNew(init_inputs)
            
        model.to(device)
        model.eval() 
    except Exception as e:
        print(f"Error: å®ä¾‹åŒ– ModelNew å¤±è´¥: {e}", file=sys.stderr)
        sys.exit(1)

    torch.cuda.synchronize(device)

    for _ in range(5):
        model(*gpu_inputs)
    torch.cuda.synchronize()
    
    # 5. è¿è¡Œç›®æ ‡ (NCU åˆ†æåŒºåŸŸ)
    print("Start Profiling...")
    try:
        torch.cuda.cudart().cudaProfilerStart()
        model(*gpu_inputs)
        torch.cuda.synchronize(device)
        torch.cuda.cudart().cudaProfilerStop()
    except Exception as e:
        print(f"Error: æ¨¡å‹æ‰§è¡Œå¤±è´¥: {e}", file=sys.stderr)
        sys.exit(1)
    print("Stop Profiling.")

except Exception as e:
    traceback.print_exc()
    sys.exit(1)
"""

def _named_tensors(model: nn.Module) -> dict[str, torch.Tensor]:
    """è·å–æ¨¡å‹ä¸­æ‰€æœ‰å‚æ•°å’Œç¼“å†²åŒºçš„æ‰å¹³åŒ–å­—å…¸"""
    named: dict[str, torch.Tensor] = {}
    for k, p in model.named_parameters(recurse=True):
        named[f"param::{k}"] = p
    for k, b in model.named_buffers(recurse=True):
        named[f"buffer::{k}"] = b
    return named

@torch.no_grad()
def _safe_copy_(dst: torch.Tensor, src: torch.Tensor) -> bool:
    """å°è¯•ç›´æ¥æ‹·è´ï¼ˆå½¢çŠ¶å¿…é¡»å®Œå…¨ä¸€è‡´ï¼‰"""
    if dst.shape != src.shape:
        return False
    dst.copy_(src.to(dtype=dst.dtype, device=dst.device))
    return True

@torch.no_grad()
def _try_map_shape_and_copy_(dst: torch.Tensor, src: torch.Tensor) -> bool:
    """
    å°è¯•å¤„ç†å½¢çŠ¶ä¸åŒ¹é…çš„æƒ…å†µï¼ˆä¾‹å¦‚ç”Ÿæˆçš„ Kernel ä½¿ç”¨äº†ä¸åŒçš„å†…å­˜å¸ƒå±€ï¼‰ã€‚
    æ”¯æŒï¼šè½¬ç½®ã€å‹ç¼©ç»´åº¦ç­‰å¸¸è§æ“ä½œã€‚
    """
    s = tuple(src.shape)
    d = tuple(dst.shape)

    # 1. å®Œå…¨ç›¸åŒï¼Œç›´æ¥æ‹·
    if s == d:
        dst.copy_(src.to(dtype=dst.dtype, device=dst.device))
        return True

    # 2. 5D æƒé‡é¦–ä¸¤ç»´äº¤æ¢ (å¸¸è§äº Conv3d: Out,In,... <-> In,Out,...)
    if len(s) == 5 and len(d) == 5 and s[0] == d[1] and s[1] == d[0] and s[2:] == d[2:]:
        dst.copy_(src.permute(1, 0, 2, 3, 4).contiguous().to(dtype=dst.dtype, device=dst.device))
        return True

    # 3. å‹ç¼©/è§£å‹ç»´åº¦ (ä¾‹å¦‚ Linear çš„ weight æ˜¯ 2Dï¼Œä½†æŸäº› Conv å®ç°å¯èƒ½æ˜¯ 4D (Out, In, 1, 1))
    if src.numel() == dst.numel():
        # å°è¯• reshape åæ‹·è´
        try:
            dst.copy_(src.to(dtype=dst.dtype, device=dst.device).reshape(d).contiguous())
            return True
        except:
            pass
            
    return False

@torch.no_grad()
def align_params_smart(ref_model: nn.Module, test_model: nn.Module):
    """
    æ™ºèƒ½å¯¹é½å‚æ•°ï¼š
    1. ä¼˜å…ˆå°è¯•åŒåæ‹·è´ã€‚
    2. å¦‚æœåå­—å¯¹ä¸ä¸Šï¼Œå°è¯•é€šè¿‡â€œå”¯ä¸€å½¢çŠ¶åŒ¹é…â€æ¥æ‹·è´ã€‚
    """
    if ref_model is None:
        return

    ref_named = _named_tensors(ref_model)
    test_named = _named_tensors(test_model)
    aligned_test_keys = set()

    print("--- Syncing Weights (Smart Alignment) ---")

    # 1. ç­–ç•¥ Aï¼šåŒååŒå½¢çŠ¶ (Name Match)
    for name, t_dst in test_named.items():
        t_src = ref_named.get(name, None)
        if t_src is not None:
            if _try_map_shape_and_copy_(t_dst, t_src):
                aligned_test_keys.add(name)
                # print(f"  [Sync] Matched by name: {name}")

    # 2. ç­–ç•¥ Bï¼šå”¯ä¸€å½¢çŠ¶åŒ¹é… (Unique Shape Match)
    # å¦‚æœç”Ÿæˆçš„ä»£ç æ”¹äº†å±‚åå­—ï¼ˆæ¯”å¦‚ self.conv æ”¹æˆäº† self.conv1ï¼‰ï¼Œload_state_dict ä¼šå¤±è´¥ã€‚
    # è¿™é‡Œé€šè¿‡å½¢çŠ¶æ¥â€œçŒœâ€å¯¹åº”å…³ç³»ã€‚
    shape2ref = defaultdict(list)
    shape2test = defaultdict(list)
    
    for n, t in ref_named.items():
        shape2ref[tuple(t.shape)].append((n, t))
    
    for n, t in test_named.items():
        if n not in aligned_test_keys: # åªå¤„ç†è¿˜æ²¡å¯¹é½çš„
            shape2test[tuple(t.shape)].append((n, t))

    for shp, items in shape2test.items():
        # å¦‚æœè¿™ä¸ªå½¢çŠ¶åœ¨ ref å’Œ test ä¸­éƒ½åªå‡ºç°äº†ä¸€æ¬¡ï¼Œé‚£å®ƒä»¬è‚¯å®šæ˜¯ä¸€å¯¹ï¼
        if len(items) == 1 and len(shape2ref.get(shp, [])) == 1:
            tname_dst, t_dst = items[0]
            _, t_src = shape2ref[shp][0]
            if _safe_copy_(t_dst, t_src):
                aligned_test_keys.add(tname_dst)
                print(f"  [Sync] Matched by unique shape: {shp}")

    # ç»Ÿè®¡
    print(f"  Synced {len(aligned_test_keys)} / {len(test_named)} tensors.")

# --- 1. æ·»åŠ  FDCapturer ç±» ---
class FDCapturer:
    def __init__(self):
        self._stdout_fd = sys.stdout.fileno()
        self._stderr_fd = sys.stderr.fileno()
        # ä¿å­˜åŸå§‹çš„æ–‡ä»¶æè¿°ç¬¦
        self._saved_stdout_fd = os.dup(self._stdout_fd)
        self._saved_stderr_fd = os.dup(self._stderr_fd)
        # åˆ›å»ºä¸€ä¸ªä¸´æ—¶æ–‡ä»¶æ¥æ¥æ”¶è¾“å‡º
        self._temp_file = tempfile.TemporaryFile(mode='w+b')

    def __enter__(self):
        # åˆ·æ–° Python ç¼“å†²åŒºï¼Œé˜²æ­¢ä¹‹å‰çš„ Python è¾“å‡ºæ··å…¥
        sys.stdout.flush()
        sys.stderr.flush()
        # å°† stdout (1) å’Œ stderr (2) é‡å®šå‘åˆ°ä¸´æ—¶æ–‡ä»¶
        os.dup2(self._temp_file.fileno(), self._stdout_fd)
        os.dup2(self._temp_file.fileno(), self._stderr_fd)
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        # å†æ¬¡åˆ·æ–°ï¼Œç¡®ä¿æ‰€æœ‰åº•å±‚è¾“å‡ºéƒ½å†™è¿›æ–‡ä»¶
        sys.stdout.flush()
        sys.stderr.flush()
        # æ¢å¤æ ‡å‡†è¾“å‡º/é”™è¯¯
        os.dup2(self._saved_stdout_fd, self._stdout_fd)
        os.dup2(self._saved_stderr_fd, self._stderr_fd)
        os.close(self._saved_stdout_fd)
        os.close(self._saved_stderr_fd)
    
    def get_output(self):
        # å›åˆ°æ–‡ä»¶å¼€å¤´è¯»å–æ‰€æœ‰å†…å®¹
        self._temp_file.seek(0)
        return self._temp_file.read().decode('utf-8', errors='replace')

import subprocess
def extract_error_and_next_line(text):
    # æŒ‰è¡Œåˆ†å‰²
    lines = text.splitlines()
    results = []
    for i, line in enumerate(lines):
        if "error:" in line:
            results.append(line)
            if i + 1 < len(lines):
                results.append(lines[i + 1])
    return "\n".join(results)

# --- 2. ä¿®æ”¹ load_module ---
def load_module(cuda_code, module_name, init_inputs, ref_model):
    # 1. å¼ºåˆ¶æ¸…ç†ç¼“å­˜
    shutil.rmtree(os.path.expanduser('~/.cache/torch_extensions'), ignore_errors=True)
    
    TEST_NN_MODEL_NAME = 'ModelNew'
    model_instance = None
    captured_log = ""
    
    # [æŒä¹…åŒ–è·¯å¾„] å¿…é¡»ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œå› ä¸º module.__file__ éœ€è¦å®ƒ
    file_path = os.path.abspath(f"{module_name}.py")
    
    try:
        # --- åŠ¨æ€é‡å‘½åé€»è¾‘ ---
        timestamp = int(time.time() * 1000)
        pattern = r"(name\s*=\s*['\"])([\w_]+)(['\"])"
        
        def replace_func(match):
            prefix = match.group(1)
            old_name = match.group(2)
            suffix = match.group(3)
            new_name = f"{old_name}_{timestamp}"
            # print(f"[DEBUG] Renaming: {old_name} -> {new_name}")
            return f"{prefix}{new_name}{suffix}"
            
        cuda_code_modified = re.sub(pattern, replace_func, cuda_code, count=1)
        
        # 2. å†™å…¥æ–‡ä»¶
        with open(file_path, "w") as f:
            f.write(cuda_code_modified)

        # 3. åŠ è½½æ¨¡å—
        spec = importlib.util.spec_from_file_location(TEST_NN_MODEL_NAME, file_path)
        if spec is None:
            print("ERROR in load_module: spec is None")
            # å¦‚æœåŠ è½½å¤±è´¥ï¼Œç«‹å³æ¸…ç†æ–‡ä»¶
            if os.path.exists(file_path):
                os.remove(file_path)
            return None, "", ""

        module = importlib.util.module_from_spec(spec)
        sys.modules[TEST_NN_MODEL_NAME] = module

        # 4. ç¼–è¯‘ & æ•è·è¾“å‡º
        capturer = FDCapturer()
        try:
            with capturer:
                spec.loader.exec_module(module)
        except Exception as e:
            print(f"Compilation Error: {e}")
            # ç¼–è¯‘å¤±è´¥ä¹Ÿæ¸…ç†æ–‡ä»¶
            if os.path.exists(file_path):
                os.remove(file_path)
            err_msg = extract_error_and_next_line(capturer.get_output())
            # ä¾ç„¶è¿”å›æ—¥å¿—ä¾›åˆ†æ
            return None, capturer.get_output(), err_msg
        
        captured_log = capturer.get_output()
        
        # 5. å®ä¾‹åŒ–æ¨¡å‹
        model_class = getattr(module, TEST_NN_MODEL_NAME, None)
        if model_class is None:
            print("ERROR: Model class not found")
            if os.path.exists(file_path):
                os.remove(file_path)
            return None, captured_log, captured_log

        try:
            if init_inputs is not None:
                if isinstance(init_inputs, (list, tuple)):
                    model_instance = model_class(*init_inputs)
                elif isinstance(init_inputs, dict):
                    model_instance = model_class(**init_inputs)
                else:
                    model_instance = model_class(init_inputs)
            else:
                model_instance = model_class()
                
            if torch.cuda.is_available():
                    model_instance = model_instance.cuda()

            if ref_model is not None:
                try:
                    align_params_smart(ref_model, model_instance)
                except Exception as e:
                    print(f"[Warning] Smart weight sync failed: {e}")
                    # å³ä½¿åŒæ­¥å¤±è´¥ï¼Œä¹Ÿè®©å®ƒç»§ç»­è·‘ï¼Œè¯´ä¸å®š LLM è¿æ°”å¥½
            
            # [!!! å…³é”®ä¿®å¤ 1 !!!] ç»‘å®š __file__ å±æ€§ï¼Œä¾› main.py ä¸­çš„ NCU ä½¿ç”¨
            model_instance.__file__ = file_path
            
            # [!!! å…³é”®ä¿®å¤ 2 !!!] æ³¨å†Œè‡ªåŠ¨æ¸…ç†é’©å­
            # å½“ model_instance è¢« del æˆ– åƒåœ¾å›æ”¶æ—¶ï¼Œè‡ªåŠ¨æ‰§è¡Œ lambda åˆ é™¤æ–‡ä»¶
            weakref.finalize(
                model_instance, 
                lambda p=file_path: os.remove(p) if os.path.exists(p) else None
            )
            
        except Exception as e:
            print(f"Instantiation Error: {e}")
            if os.path.exists(file_path):
                os.remove(file_path)
            return None, captured_log, captured_log

    except Exception as e:
        print(f"General Error inside load_module: {e}")
        traceback.print_exc()
        if os.path.exists(file_path):
            os.remove(file_path)
    
    return model_instance, captured_log, captured_log

# def load_module(cuda_code, module_name, init_inputs):
#     # 1. å¼ºåˆ¶æ¸…ç† Torch æ‰©å±•ç¼“å­˜ (ä¿æŒåŸæœ‰é€»è¾‘)
#     shutil.rmtree(os.path.expanduser('~/.cache/torch_extensions'), ignore_errors=True)
    
#     TEST_NN_MODEL_NAME = 'ModelNew'
#     model_instance = None
#     captured_log = ""
#     err_msg = ""
    
#     # å£°æ˜å˜é‡ä»¥ä¾¿åœ¨ finally ä¸­æ¸…ç†
#     module = None
#     spec = None
#     capturer = None
    
#     # [æŒä¹…åŒ–è·¯å¾„]
#     file_path = os.path.abspath(f"{module_name}.py")
    
#     try:
#         # --- åŠ¨æ€é‡å‘½åé€»è¾‘ ---
#         timestamp = int(time.time() * 1000)
#         pattern = r"(name\s*=\s*['\"])([\w_]+)(['\"])"
        
#         def replace_func(match):
#             prefix = match.group(1)
#             old_name = match.group(2)
#             suffix = match.group(3)
#             new_name = f"{old_name}_{timestamp}"
#             return f"{prefix}{new_name}{suffix}"
            
#         cuda_code_modified = re.sub(pattern, replace_func, cuda_code, count=1)
        
#         # 2. å†™å…¥æ–‡ä»¶
#         with open(file_path, "w") as f:
#             f.write(cuda_code_modified)

#         # 3. åŠ è½½æ¨¡å—
#         spec = importlib.util.spec_from_file_location(TEST_NN_MODEL_NAME, file_path)
#         if spec is None:
#             print("ERROR in load_module: spec is None")
#             if os.path.exists(file_path):
#                 os.remove(file_path)
#             return None, "", ""

#         module = importlib.util.module_from_spec(spec)
        
#         # [æ³¨æ„] è¿™é‡Œæ³¨å†Œåˆ°å…¨å±€ï¼Œå¦‚æœä¸æ¸…ç†ï¼Œæ¨¡å—æ°¸è¿œå­˜æ´»
#         sys.modules[TEST_NN_MODEL_NAME] = module

#         # 4. ç¼–è¯‘ & æ•è·è¾“å‡º
#         capturer = FDCapturer()
#         try:
#             with capturer:
#                 spec.loader.exec_module(module)
#         except Exception as e:
#             print(f"Compilation Error: {e}")
#             if os.path.exists(file_path):
#                 os.remove(file_path)
#             err_msg = extract_error_and_next_line(capturer.get_output())
#             return None, capturer.get_output(), err_msg
        
#         captured_log = capturer.get_output()
        
#         # 5. å®ä¾‹åŒ–æ¨¡å‹
#         model_class = getattr(module, TEST_NN_MODEL_NAME, None)
#         if model_class is None:
#             print("ERROR: Model class not found")
#             if os.path.exists(file_path):
#                 os.remove(file_path)
#             return None, captured_log, captured_log

#         try:
#             if init_inputs is not None:
#                 if isinstance(init_inputs, (list, tuple)):
#                     model_instance = model_class(*init_inputs)
#                 elif isinstance(init_inputs, dict):
#                     model_instance = model_class(**init_inputs)
#                 else:
#                     model_instance = model_class(init_inputs)
#             else:
#                 model_instance = model_class()
            
#             # ç»‘å®šæ–‡ä»¶è·¯å¾„
#             model_instance.__file__ = file_path
            
#             # æ³¨å†Œè‡ªåŠ¨æ¸…ç†é’©å­ (å½“ model_instance é”€æ¯æ—¶åˆ é™¤æ–‡ä»¶)
#             weakref.finalize(
#                 model_instance, 
#                 lambda p=file_path: os.remove(p) if os.path.exists(p) else None
#             )
            
#         except Exception as e:
#             print(f"Instantiation Error: {e}")
#             if os.path.exists(file_path):
#                 os.remove(file_path)
#             return None, captured_log, captured_log

#     except Exception as e:
#         print(f"General Error inside load_module: {e}")
#         traceback.print_exc()
#         if os.path.exists(file_path):
#             os.remove(file_path)
#         return None, "", str(e)

#     finally:
#         # ==========================================================
#         # [æ ¸å¿ƒä¼˜åŒ–] å‡½æ•°é€€å‡ºå‰çš„å¼ºåŠ›åƒåœ¾å›æ”¶
#         # ==========================================================
        
#         # 1. ä»å…¨å±€ sys.modules ä¸­ç§»é™¤æ¨¡å—å¼•ç”¨
#         # model_instance å·²ç»å®ä¾‹åŒ–ï¼Œå®ƒå†…éƒ¨çš„ __class__ ä¼šæŒæœ‰ module çš„å¼•ç”¨ï¼Œ
#         # æ‰€ä»¥åªè¦ model_instance æ´»ç€ï¼Œä»£ç å°±èƒ½è·‘ã€‚
#         # ä½†ä» sys.modules ç§»é™¤åï¼Œå½“ model_instance æ­»äº¡æ—¶ï¼Œmodule ä¹Ÿä¼šéšä¹‹æ­»äº¡ã€‚
#         if TEST_NN_MODEL_NAME in sys.modules:
#             del sys.modules[TEST_NN_MODEL_NAME]

#         # 2. åˆ é™¤å±€éƒ¨å¤§å¯¹è±¡å¼•ç”¨
#         if 'cuda_code_modified' in locals(): del cuda_code_modified
#         if 'capturer' in locals() and capturer is not None: del capturer
#         if 'spec' in locals(): del spec
        
#         # æ³¨æ„ï¼šä¸è¦ del model_instanceï¼Œè¿™æ˜¯æˆ‘ä»¬è¦è¿”å›çš„ï¼
#         # ä¹Ÿä¸è¦ del captured_logï¼Œè¿™æ˜¯è¦è¿”å›çš„æ—¥å¿— (strç±»å‹å å†…å­˜ä¸å¤§ï¼Œå¯ä»¥æ¥å—)

#         # 3. æ˜¾å¼æ–­å¼€ module å¼•ç”¨
#         if module is not None:
#             del module
            
#         # 4. å¼ºåˆ¶è§¦å‘åƒåœ¾å›æ”¶
#         # è¿™ä¼šæ¸…ç†æ‰åˆšæ‰äº§ç”Ÿçš„ç¼–è¯‘å›¾ã€ASTå¯¹è±¡ç­‰å¾ªç¯å¼•ç”¨
#         gc.collect()

#     return model_instance, captured_log, captured_log

# [!!! å·²æ›´æ–° !!!] æ¥å— wrapper_function_name
# def load_module(cuda_code, module_name,init_inputs):
#     shutil.rmtree(os.path.expanduser('~/.cache/torch_extensions'), ignore_errors=True)# IMPORTANTï¼šè°ƒç”¨load_moduleä¹‹å‰å¼ºåˆ¶æ¸…ç©ºç¼“å­˜ï¼Œå› ä¸ºpytorchä¼šæ ¹æ®cuda_codeä¸­load_inlineä¸­çš„nameé€‰é¡¹æ˜¯å¦ä¸€è‡´åˆ¤æ–­è¿™ä¸ªæ˜¯å¦ä¹‹å‰ç¼–è¯‘è¿‡ï¼Œå¦‚æœç¼–è¯‘è¿‡å°±ä¸ä¼šç¼–è¯‘å¯¼è‡´è·å–ä¸åˆ°PTSAXä¿¡æ¯ï¼ˆä½†æ˜¯å®é™…ä¸Šä¸ºäº†è·å–PTXASä¿¡æ¯é‡æ–°ç¼–è¯‘ä¼šå½±å“æ•´ä¸ªæµç¨‹çš„æ—¶é—´ï¼‰
#     TEST_NN_MODEL_NAME = 'ModelNew'
#     try:
#         with tempfile.TemporaryDirectory() as temp_dir:
#             temp_file = os.path.join(temp_dir, "cuda_code1.py")
#             with open(temp_file, "w") as f:
#                 f.write(cuda_code)

#             spec = importlib.util.spec_from_file_location(TEST_NN_MODEL_NAME, temp_file)
#             if spec is None:
#                 print("ERROR in load_module 1")

#             module = importlib.util.module_from_spec(spec)
#             sys.modules[TEST_NN_MODEL_NAME] = module
#             # ---------- æ‰§è¡Œæ¨¡å— & æ•è·æ‰€æœ‰è¾“å‡º ----------
#             stdout_capture = io.StringIO()
#             stderr_capture = io.StringIO()  
#             try:
#                 with contextlib.redirect_stdout(stdout_capture), contextlib.redirect_stderr(stderr_capture):
#                     spec.loader.exec_module(module)
#             except Exception as e:
#                 print(e)
#                 print("ERROR in load_module 2")

#             model_class = getattr(module, TEST_NN_MODEL_NAME, None)
#             if model_class is None:
#                 print("ERROR in load_module 3")

#             # å®ä¾‹åŒ–æ¨¡å‹
#             try:
#                 if init_inputs is not None:
#                     if isinstance(init_inputs, (list, tuple)):
#                         model_instance = model_class(*init_inputs)
#                     elif isinstance(init_inputs, dict):
#                         model_instance = model_class(**init_inputs)
#                     else:
#                         # å•å€¼åˆå§‹åŒ–
#                         model_instance = model_class(init_inputs)
#                 else:
#                     # æ— åˆå§‹åŒ–å‚æ•°
#                     model_instance = model_class()
#             except Exception as e:
#                 print(e)
#                 print("ERROR in load_module 4")
#     except Exception as e:
#         print(e)
#         print("ERROR in load_module 5")
#     return model_instance,stdout_capture,stderr_capture

#     """
#     (æ­¤å‡½æ•°å·²æ›´æ–°)
#     ä½¿ç”¨PyTorchçš„JITç¼–è¯‘C++/CUDAæºç ã€‚
#     è¿”å› (module, stdout_log, stderr_log)
#     """
#     global _gemm_module
    
#     block_size = 16 
#     try:
#         match = re.search(r'#define\s+BLOCK_SIZE\s+(\d+)', cuda_source)
#         if match:
#             block_size = int(match.group(1))
#     except:
#         pass 
        
#     cuda_flags = [
#         '-O3',
#         '-allow-unsupported-compiler',
#         f'-DBLOCK_SIZE={block_size}',
#         '--ptxas-options=-v', # <--- å…³é”®ï¼šè¯·æ±‚ ptxas è¯¦ç»†è¾“å‡º
#         '-gencode=arch=compute_80,code=sm_80' 
#     ]

#     original_stdout_fd = os.dup(1)
#     original_stderr_fd = os.dup(2)
#     r_out, w_out = os.pipe()
#     r_err, w_err = os.pipe()
#     os.dup2(w_out, 1)
#     os.dup2(w_err, 2)
#     os.close(w_out)
#     os.close(w_err)

#     stdout_log = ""
#     stderr_log = ""
#     _module = None

#     try:
#         _module = load_inline(
#             name=module_name, 
#             cpp_sources=cpp_source,
#             cuda_sources=cuda_source,
#             functions=[wrapper_function_name], # <--- [!!! å·²æ›´æ–° !!!] ä½¿ç”¨å‚æ•°
#             verbose=True, # <--- å…³é”®ï¼šå¿…é¡»ä¸º True æ‰èƒ½æ•è·æ—¥å¿—
#             extra_cflags=["-O3"],
#             extra_cuda_cflags=cuda_flags
#         )
        
#         os.dup2(original_stdout_fd, 1)
#         os.dup2(original_stderr_fd, 2)
#         stdout_bytes = os.read(r_out, 100000)
#         stderr_bytes = os.read(r_err, 100000)
#         stdout_log = stdout_bytes.decode('utf-8', errors='ignore')
#         stderr_log = stderr_bytes.decode('utf-8', errors='ignore')
        
#     except Exception as e:
#         os.dup2(original_stdout_fd, 1)
#         os.dup2(original_stderr_fd, 2)
#         stdout_bytes = os.read(r_out, 100000)
#         stderr_bytes = os.read(r_err, 100000)
#         stdout_log = stdout_bytes.decode('utf-8', errors='ignore')
#         stderr_log = stderr_bytes.decode('utf-8', errors='ignore')
        
#         detailed_error_msg = f"""CUDA C++ æ‰©å±•ç¼–è¯‘å¤±è´¥: {e}
# --- [ NVCC/Ninja STDOUT ] ---
# {stdout_log}
# --- [ NVCC/Ninja STDERR ] ---
# {stderr_log}
# -----------------------------
# """
#         raise RuntimeError(detailed_error_msg)

#     finally:
#         os.close(original_stdout_fd)
#         os.close(original_stderr_fd)
#         os.close(r_out)
#         os.close(r_err)

#     _gemm_module = _module
#     return _gemm_module, stdout_log, stderr_log

# [!!! å·²æ›´æ–° !!!] æ¥å—é€šç”¨è¾“å…¥
def run_gemm(inputs, module):
    """
    (æ­¤å‡½æ•°å·²æ›´æ–°)
    è¿è¡Œå½“å‰åŠ è½½çš„æ¨¡å—ã€‚
    """
    # if module is None:
    #     raise RuntimeError("æ¨¡å—æœªç¼–è¯‘ã€‚è¯·å…ˆè°ƒç”¨ load_module()")
    
    # ä½¿ç”¨ getattr åŠ¨æ€è°ƒç”¨ wrapper
    # wrapper_func = getattr(_gemm_module, wrapper_function_name)
    
    return module(*inputs)



# def check_correctness(inputs, ref_outputs, module):
#     """
#     (æ­¤å‡½æ•°å·²æ›´æ–°)
#     æ£€æŸ¥é€šç”¨å†…æ ¸çš„æ­£ç¡®æ€§ã€‚
#     è¿”å›: (is_correct: bool, error_msg: str)
#     """
#     print("Running evolved kernel for correctness check...")
#     data_type_info = ""
#     try:
#         # ç¡®ä¿è¾“å…¥åœ¨ GPU ä¸Š
#         # gpu_inputs = [t.cuda() if isinstance(t, torch.Tensor) and not t.is_cuda else t for t in inputs]
#         # gpu_ref_outputs = [t.cuda() if isinstance(t, torch.Tensor) and not t.is_cuda else t for t in ref_outputs]

#         C_evolved_outputs = run_gemm(inputs, module)
        
#         # ç¡®ä¿ C_evolved_outputs æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œä»¥ä¾¿è¿›è¡Œ zip
#         # if not isinstance(C_evolved_outputs, (list, tuple)):
#         #     C_evolved_outputs = [C_evolved_outputs]

#         # 1. æ£€æŸ¥è¾“å‡ºæ•°é‡
#         if len(C_evolved_outputs) != len(ref_outputs):
#             msg = (f"Failed (Correctness): Output count mismatch. "
#                    f"Expected {len(ref_outputs)}, got {len(C_evolved_outputs)}.")
#             print(f"--- KERNEL IS INCORRECT ---")
#             print(msg)
#             print("---------------------------")
#             return False, msg

#         is_correct = True
#         error_msgs = []

#         def compare_outputs(a, b, atol=1e-2, rtol=1e-2):
#             # global data_type_info
#             # tuple æƒ…å†µ
#             if isinstance(a, tuple) and isinstance(b, tuple):
#                 if len(a) != len(b):
#                     return False
#                 return all(compare_outputs(x, y, atol, rtol) for x, y in zip(a, b))

#             # tensor å¯¹ tensor
#             if isinstance(a, torch.Tensor) and isinstance(b, torch.Tensor):
#                 return torch.allclose(a, b, atol=atol, rtol=rtol)

#             # # æ ‡é‡å¯¹æ ‡é‡
#             if isinstance(a, (int, float)) and isinstance(b, (int, float)):
#                 return abs(a - b) <= (atol + rtol * abs(b))

#             print("è¾“å‡ºç±»å‹ä¸åŒ¹é…ï¼š", type(a), type(b))
#             data_type_info = f"The value type of some values in the return value is incorrect. The current value type is {type(b)} and the correct value type is f{type(a)}"
#             return False
        
#         if C_evolved_outputs.shape != ref_outputs.shape:
#             is_correct = False
#             msg = (f"Failed (Correctness): Shape mismatch at Output. "
#                     f"Expected {ref_outputs.shape}, got {C_evolved_outputs.shape}.")
#             error_msgs.append(msg)
#             print(msg)
#             return False,msg
#         if not compare_outputs(C_evolved_outputs,ref_outputs): 
#             is_correct = False
#             if not data_type_info:
#                return False, data_type_info
#             # --- [æ ¸å¿ƒä¿®æ”¹] æ•è·å‰ 5 ä¸ªé”™è¯¯å€¼ ---
#             diff = torch.abs(C_evolved_outputs - ref_outputs)
#             # è®¡ç®—å…è®¸çš„è¯¯å·®èŒƒå›´
#             tol = 1e-2 + 1e-2 * torch.abs(ref_outputs)
#             # æ‰¾å‡ºè¶…å‡ºè¯¯å·®çš„æ©ç 
#             error_mask = diff > tol
#             # è·å–é”™è¯¯ç´¢å¼•
#             error_indices = torch.nonzero(error_mask, as_tuple=False)
#             num_errors = error_indices.size(0)
            
#             msg_header = f"Failed (Correctness): Output has {num_errors} mismatches (total elements: {ref_outputs.numel()})."
#             error_details = [msg_header]
#             error_details.append("Top 5 Mismatches (Index | Reference Value | Actual Value):")
            
#             # å–å‰ 5 ä¸ª
#             for j in range(min(5, num_errors)):
#                 idx = error_indices[j]
#                 idx_tuple = tuple(idx.tolist())
#                 ref_val = ref_outputs[idx_tuple].item()
#                 act_val = C_evolved_outputs[idx_tuple].item()
#                 error_details.append(f"  [{j}] Index: {idx_tuple} | Ref: {ref_val:.6f} | Act: {act_val:.6f}")
            
#             full_msg = "\n".join(error_details)
#             error_msgs.append(full_msg)
            
#             print(f"--- KERNEL IS INCORRECT (Output) ---")
#             print(full_msg)
#             print("---------------------------")
#             # åªè¦å‘ç°ä¸€ä¸ªè¾“å‡ºä¸å¯¹ï¼Œé€šå¸¸å°±å¯ä»¥è¿”å›äº†ï¼Œæˆ–è€…æ”¶é›†æ‰€æœ‰é”™è¯¯
#             # è¿™é‡Œæˆ‘ä»¬æ”¶é›†ç¬¬ä¸€ä¸ªä¸»è¦é”™è¯¯åç›´æ¥è¿”å›ï¼Œé¿å… Prompt è¿‡é•¿
#             return False, full_msg
        
#         if is_correct:
#             return True, ""
#         else:
#             # åªæœ‰å½¢çŠ¶é”™è¯¯ä¼šèµ°åˆ°è¿™é‡Œ
#             return False, "\n".join(error_msgs)

#         # # 2. é€ä¸ªæ£€æŸ¥è¾“å‡ºå¼ é‡
#         # for i, (evolved_t, ref_t) in enumerate(zip(C_evolved_outputs, gpu_ref_outputs)):
#         #     # æ£€æŸ¥å½¢çŠ¶
#         #     if evolved_t.shape != ref_t.shape:
#         #         is_correct = False
#         #         msg = (f"Failed (Correctness): Shape mismatch at Output {i}. "
#         #                f"Expected {ref_t.shape}, got {evolved_t.shape}.")
#         #         error_msgs.append(msg)
#         #         print(msg)
#         #         continue # ç»§ç»­æ£€æŸ¥ä¸‹ä¸€ä¸ªè¾“å‡ºï¼Œæˆ–è€…ç›´æ¥è¿”å›ä¹Ÿå¯ä»¥

#         #     # æ£€æŸ¥æ•°å€¼ (atol=1e-2, rtol=1e-2)
#         #     if not torch.allclose(evolved_t, ref_t, atol=1e-2, rtol=1e-2):
#         #         is_correct = False
                
#         #         # --- [æ ¸å¿ƒä¿®æ”¹] æ•è·å‰ 5 ä¸ªé”™è¯¯å€¼ ---
#         #         diff = torch.abs(evolved_t - ref_t)
#         #         # è®¡ç®—å…è®¸çš„è¯¯å·®èŒƒå›´
#         #         tol = 1e-2 + 1e-2 * torch.abs(ref_t)
#         #         # æ‰¾å‡ºè¶…å‡ºè¯¯å·®çš„æ©ç 
#         #         error_mask = diff > tol
#         #         # è·å–é”™è¯¯ç´¢å¼•
#         #         error_indices = torch.nonzero(error_mask, as_tuple=False)
#         #         num_errors = error_indices.size(0)
                
#         #         msg_header = f"Failed (Correctness): Output {i} has {num_errors} mismatches (total elements: {ref_t.numel()})."
#         #         error_details = [msg_header]
#         #         error_details.append("Top 5 Mismatches (Index | Reference Value | Actual Value):")
                
#         #         # å–å‰ 5 ä¸ª
#         #         for j in range(min(5, num_errors)):
#         #             idx = error_indices[j]
#         #             idx_tuple = tuple(idx.tolist())
#         #             ref_val = ref_t[idx_tuple].item()
#         #             act_val = evolved_t[idx_tuple].item()
#         #             error_details.append(f"  [{j}] Index: {idx_tuple} | Ref: {ref_val:.6f} | Act: {act_val:.6f}")
                
#         #         full_msg = "\n".join(error_details)
#         #         error_msgs.append(full_msg)
                
#         #         print(f"--- KERNEL IS INCORRECT (Output {i}) ---")
#         #         print(full_msg)
#         #         print("---------------------------")
#         #         # åªè¦å‘ç°ä¸€ä¸ªè¾“å‡ºä¸å¯¹ï¼Œé€šå¸¸å°±å¯ä»¥è¿”å›äº†ï¼Œæˆ–è€…æ”¶é›†æ‰€æœ‰é”™è¯¯
#         #         # è¿™é‡Œæˆ‘ä»¬æ”¶é›†ç¬¬ä¸€ä¸ªä¸»è¦é”™è¯¯åç›´æ¥è¿”å›ï¼Œé¿å… Prompt è¿‡é•¿
#         #         return False, full_msg

#         # if is_correct:
#         #     return True, ""
#         # else:
#         #     # åªæœ‰å½¢çŠ¶é”™è¯¯ä¼šèµ°åˆ°è¿™é‡Œ
#         #     return False, "\n".join(error_msgs)

def check_correctness(inputs, ref_outputs, module):
    """
    (æ­¤å‡½æ•°å·²æ›´æ–° - å†…å­˜ä¼˜åŒ–ç‰ˆ)
    æ£€æŸ¥é€šç”¨å†…æ ¸çš„æ­£ç¡®æ€§ã€‚
    å‡½æ•°é€€å‡ºæ—¶ä¼šå¼ºåˆ¶æ¸…ç†æ‰€æœ‰ä¸­é—´å˜é‡å ç”¨çš„æ˜¾å­˜ã€‚
    """
    print("Running evolved kernel for correctness check...")
    
    # [å†…å­˜ç®¡ç†] åˆå§‹åŒ–æ‰€æœ‰å¯èƒ½äº§ç”Ÿçš„å¤§å¯¹è±¡å˜é‡ä¸º None
    # è¿™æ · finally å—å¯ä»¥å®‰å…¨åœ°æ£€æŸ¥å’Œåˆ é™¤å®ƒä»¬
    C_evolved_outputs = None
    diff = None
    tol = None
    error_mask = None
    error_indices = None
    
    # å†…éƒ¨è¾…åŠ©å‡½æ•° (ä¿æŒä¸å˜)
    def compare_outputs(a, b, atol=1e-2, rtol=1e-2):
        nonlocal data_type_info # ä½¿ç”¨ nonlocal ä¿®æ”¹å¤–éƒ¨å˜é‡
        if isinstance(a, tuple) and isinstance(b, tuple):
            if len(a) != len(b): return False
            return all(compare_outputs(x, y, atol, rtol) for x, y in zip(a, b))
        if isinstance(a, torch.Tensor) and isinstance(b, torch.Tensor):
            return torch.allclose(a, b, atol=atol, rtol=rtol)
        if isinstance(a, (int, float)) and isinstance(b, (int, float)):
            return abs(a - b) <= (atol + rtol * abs(b))
        
        # print("è¾“å‡ºç±»å‹ä¸åŒ¹é…ï¼š", type(a), type(b))
        data_type_info = f"Type mismatch: expected {type(b)}, got {type(a)}"
        return False

    data_type_info = ""
    cloned_inputs = None
    try:
        cloned_inputs = copy.deepcopy(inputs)
        # --- 1. æ‰§è¡Œ Kernel ---
        C_evolved_outputs = run_gemm(cloned_inputs, module)
        
        # --- 2. æ£€æŸ¥è¾“å‡ºæ•°é‡ ---
        # å¦‚æœæ˜¯å•ä¸ª Tensorï¼Œç»Ÿä¸€è½¬ä¸º list/tuple å¤„ç†å¯èƒ½ä¼šæ–¹ä¾¿äº›ï¼Œ
        # ä½†æ—¢ç„¶ä¸‹é¢ç”¨äº† shape å¯¹æ¯”ï¼Œè¿™é‡Œå‡è®¾ run_gemm è¿”å›çš„å’Œ ref_outputs ç»“æ„ä¸€è‡´
        current_len = len(C_evolved_outputs) if isinstance(C_evolved_outputs, (list, tuple)) else 1
        ref_len = len(ref_outputs) if isinstance(ref_outputs, (list, tuple)) else 1

        if current_len != ref_len:
            msg = (f"Failed (Correctness): Output count mismatch. "
                   f"Expected {ref_len}, got {current_len}.")
            print(f"--- KERNEL IS INCORRECT ---")
            print(msg)
            print("---------------------------")
            return False, msg

        # --- 3. æ£€æŸ¥ Shape ---
        # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ä¸¤è€…éƒ½æ˜¯ Tensor ç›´æ¥æ¯”è¾ƒ shape
        # å¦‚æœæ˜¯ list/tupleï¼Œè¿™é‡Œå¯èƒ½éœ€è¦è°ƒæ•´é€»è¾‘ï¼Œä½†ç…§æ¬ä½ åŸä»£ç çš„é€»è¾‘ï¼š
        if hasattr(C_evolved_outputs, 'shape') and hasattr(ref_outputs, 'shape'):
            if C_evolved_outputs.shape != ref_outputs.shape:
                msg = (f"Failed (Correctness): Shape mismatch at Output. "
                       f"Expected {ref_outputs.shape}, got {C_evolved_outputs.shape}.")
                print(msg)
                return False, msg

        # --- 4. æ£€æŸ¥æ•°å€¼ ---
        if not compare_outputs(C_evolved_outputs, ref_outputs): 
            # ç±»å‹ä¸åŒ¹é…
            if data_type_info:
               return False, data_type_info
            
            # --- [æ ¸å¿ƒä¿®æ”¹] é”™è¯¯åˆ†æ (äº§ç”Ÿå¤§é‡ä¸´æ—¶ Tensor) ---
            try:
                # è®¡ç®—å·®å€¼
                diff = torch.abs(C_evolved_outputs - ref_outputs)
                tol = 1e-2 + 1e-2 * torch.abs(ref_outputs)
                error_mask = diff > tol
                
                # è·å–é”™è¯¯ç´¢å¼• (GPU -> CPU è½¬æ¢å¯èƒ½åœ¨è¿™é‡Œéšå¼å‘ç”Ÿï¼Œäº§ç”ŸåŒæ­¥)
                error_indices = torch.nonzero(error_mask, as_tuple=False)
                num_errors = error_indices.size(0)
                
                msg_header = f"Failed (Correctness): Output has {num_errors} mismatches (total elements: {ref_outputs.numel()})."
                error_details = [msg_header, "Top 5 Mismatches (Index | Reference Value | Actual Value):"]
                
                # å–å‰ 5 ä¸ª (åªæå–æ•°å€¼ï¼Œä¸ä¿ç•™ Tensor å¼•ç”¨)
                for j in range(min(5, num_errors)):
                    idx = error_indices[j]
                    idx_tuple = tuple(idx.tolist())
                    
                    # ä½¿ç”¨ .item() å°† GPU æ ‡é‡è½¬ä¸º Python floatï¼Œæ–­å¼€è®¡ç®—å›¾å¼•ç”¨
                    ref_val = ref_outputs[idx_tuple].item()
                    act_val = C_evolved_outputs[idx_tuple].item()
                    
                    error_details.append(f"  [{j}] Index: {idx_tuple} | Ref: {ref_val:.6f} | Act: {act_val:.6f}")
                
                full_msg = "\n".join(error_details)
                print(f"--- KERNEL IS INCORRECT (Output) ---")
                print(full_msg)
                print("---------------------------")
                
                return False, full_msg
            
            finally:
                # [å†…éƒ¨æ¸…ç†] è¿™é‡Œçš„ä¸´æ—¶å˜é‡ç”¨å®Œå³å¼ƒ
                # è™½ç„¶å¤–å±‚ finally ä¹Ÿä¼šæ¸…ç†ï¼Œä½†å¦‚æœ error calculation è€—å°½äº†æ˜¾å­˜ï¼Œ
                # å°½æ—©é‡Šæ”¾æœ‰åŠ©äºé˜²æ­¢åç»­æ­¥éª¤ OOM
                if diff is not None: del diff
                if tol is not None: del tol
                if error_mask is not None: del error_mask
                if error_indices is not None: del error_indices
                diff, tol, error_mask, error_indices = None, None, None, None

        return True, ""

    except Exception as e:
        err_str = f"Runtime Error during check_correctness: {e}\n{traceback.format_exc()}"
        print(f"--- KERNEL RUNTIME FAILED ---")
        print(err_str)
        print("-----------------------------")
        return False, err_str

    finally:
        # ==========================================================
        # [å…³é”®ä¿®æ”¹] å‡½æ•°é€€å‡ºå‰çš„ç»ˆææ¸…ç†
        # ==========================================================
        
        # 1. åˆ é™¤ä¸»è¦çš„è®¡ç®—ç»“æœ Tensor
        if C_evolved_outputs is not None:
            del C_evolved_outputs

        if cloned_inputs is not None:
            # 1. è§£é™¤å˜é‡å¼•ç”¨ï¼Œä½¿ Tensor å¯¹è±¡çš„å¼•ç”¨è®¡æ•°å‡ 1
            del cloned_inputs

        # 2. åˆ é™¤é”™è¯¯åˆ†æé˜¶æ®µå¯èƒ½é—ç•™çš„ Tensor (å¦‚æœå†…éƒ¨ try æ²¡è·‘å®Œ)
        if diff is not None: del diff
        if tol is not None: del tol
        if error_mask is not None: del error_mask
        if error_indices is not None: del error_indices
        
        # 3. å¼ºåˆ¶åƒåœ¾å›æ”¶ Python å¯¹è±¡
        gc.collect()
        
        # 4. å¼ºåˆ¶æ¸…ç©º CUDA ç¼“å­˜ï¼Œå°†æ˜¾å­˜å½’è¿˜ç»™æ“ä½œç³»ç»Ÿ (ç»™ NCU è…¾åœ°)
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            
        # print("Memory released in check_correctness.")

#     except Exception as e:
#         err_str = f"Runtime Error during check_correctness: {e}\n{traceback.format_exc()}"
#         print(f"--- KERNEL RUNTIME FAILED ---")
#         print(err_str)
#         print("-----------------------------")
#         return False, err_str
        
# vvv --- PTXAS è§£æå™¨ (ä¿æŒä¸å˜) --- vvv
# def parse_ptxas_info(log_str: str) -> Dict[str, float]: #é’ˆå¯¹TODO3åšçš„ä¿®æ”¹ï¼Œè¯¦ç»†çš„ä¿®æ”¹å†…å®¹è§ğŸ‘‡
#     """
#     è§£æ PTXAS æ—¥å¿—ï¼Œè¿”å›æ‰å¹³åŒ–çš„æŒ‡æ ‡å­—å…¸ã€‚
#     é”®åä¼šè‡ªåŠ¨æ·»åŠ æ•°æ®ç±»å‹å‰ç¼€ï¼Œä¾‹å¦‚ 'float_registers_used', 'double_spill_bytes' ç­‰ã€‚
#     """
#     metrics = {}
    
#     try:
#         # 1. æŒ‰ "Compiling entry function" å°†æ—¥å¿—åˆ‡åˆ†ä¸ºä¸åŒçš„å†…æ ¸å—
#         # è¿™æ ·å¯ä»¥é˜²æ­¢ä¸åŒå†…æ ¸çš„æŒ‡æ ‡æ··æ·†
#         blocks = log_str.split("Compiling entry function")
        
#         for block in blocks:
#             if not block.strip():
#                 continue
                
#             # 2. è¯†åˆ«å†…æ ¸ç±»å‹ (é€šè¿‡ C++ Name Mangling)
#             # _Z...If... -> float
#             # _Z...Id... -> double
#             # _Z...Ih... -> half (fp16)
#             # _Z...Ib... -> bfloat16 (bf16)
#             kernel_type = "unknown"
            
#             # æå–å‡½æ•°åï¼Œä¾‹å¦‚ '_Z14sigmoid_kernelIfEvPKT_PS0_l'
#             # è¿™é‡Œçš„æ­£åˆ™åŒ¹é…å•å¼•å·å†…çš„ä¿®é¥°å
#             name_match = re.search(r"\'(_Z\w+)\'", block)
#             if name_match:
#                 mangled_name = name_match.group(1)
#                 if "If" in mangled_name:
#                     kernel_type = "float"
#                 elif "Id" in mangled_name:
#                     kernel_type = "double"
#                 elif "Ih" in mangled_name:
#                     kernel_type = "half"
#                 elif "Ib" in mangled_name:
#                     kernel_type = "bfloat16"
#                 else:
#                     # å¦‚æœæ— æ³•è¯†åˆ«å…·ä½“ç±»å‹ï¼Œå°±ä½¿ç”¨ "kernel" æˆ–è€…ä¿ç•™ä¸€éƒ¨åˆ†ç‰¹å¾
#                     kernel_type = "kernel" 
#             else:
#                 # å¦‚æœæ‰¾ä¸åˆ°å‡½æ•°åï¼Œå¯èƒ½æ˜¯å…¨å±€å…±æœ‰ä»£ç æˆ–å…¶ä»–éƒ¨åˆ†ï¼Œè·³è¿‡
#                 continue

#             # 3. è§£æè¯¥å—å†…çš„å…·ä½“æŒ‡æ ‡ï¼Œå¹¶æ„å»ºå¸¦å‰ç¼€çš„é”®å
            
#             # --- å¯„å­˜å™¨ (Registers) ---
#             reg_match = re.search(r'Used\s+(\d+)\s+registers', block)
#             if reg_match:
#                 metrics[f'{kernel_type}_registers_used'] = float(reg_match.group(1))

#             # --- å…±äº«å†…å­˜ (Shared Memory / smem) ---
#             smem_match = re.search(r'(\d+)\s+bytes\s+smem', block)
#             if smem_match:
#                 metrics[f'{kernel_type}_shared_mem_bytes'] = float(smem_match.group(1))
#             else:
#                 metrics[f'{kernel_type}_shared_mem_bytes'] = 0.0
            
#             # --- å¸¸é‡å†…å­˜ (Constant Memory / cmem) [æ–°å¢] ---
#             # å¯èƒ½ä¼šæœ‰å¤šæ®µ cmem (e.g., cmem[0], cmem[2])ï¼Œæˆ‘ä»¬éœ€è¦æ±‚å’Œ
#             cmem_matches = re.findall(r'(\d+)\s+bytes\s+cmem', block)
#             if cmem_matches:
#                 metrics[f'{kernel_type}_constant_mem_bytes'] = sum(float(x) for x in cmem_matches)
#             else:
#                 metrics[f'{kernel_type}_constant_mem_bytes'] = 0.0

#             # --- æº¢å‡º (Spill Stores/Loads) ---
#             spill_stores = re.search(r'(\d+)\s+bytes\s+spill\s+stores', block)
#             spill_loads = re.search(r'(\d+)\s+bytes\s+spill\s+loads', block)
            
#             spill_total = 0.0
#             if spill_stores: spill_total += float(spill_stores.group(1))
#             if spill_loads:  spill_total += float(spill_loads.group(1))
#             metrics[f'{kernel_type}_spill_bytes'] = spill_total

#     except Exception as e:
#         print(f"è­¦å‘Šï¼šè§£æ PTXAS æ—¥å¿—å¤±è´¥: {e}", file=sys.stderr)
    
#     print(f"--- [ PTXAS Metrics Parsed ] ---")
#     print(json.dumps(metrics, indent=2))
    
#     return metrics

def parse_ptxas_info(log_str: str) -> Dict[str, Any]:
    """
    [å‡çº§ç‰ˆ] é«˜çº§è§£æ PTXAS æ—¥å¿—ï¼Œç”Ÿæˆç»“æ„åŒ–è¡¨æ ¼ã€‚
    ä¿®å¤ï¼šæ”¯æŒä»å‡½æ•°åä¸­è§£æå‘é‡åŒ–å®½åº¦ (å¦‚ sigmoid_vec4 -> width=4)
    ä¿®å¤ï¼šæ”¯æŒä»å‡½æ•°å‚æ•°ä¸­æ¨æ–­æ•°æ®ç±»å‹ (å¦‚ PKf -> float, PK6__half -> Half)
    """
    metrics = {}
    
    # è¾…åŠ©å‡½æ•°ï¼šå¢å¼ºç‰ˆ Demangler
    def _demangle_info(mangled: str):
        # 1. æå–å‡½æ•°å (Itanium ABI: _Z + len + name)
        # ä¾‹å¦‚: _Z19sigmoid_kernel_vec4... -> len=19, name=sigmoid_kernel_vec4
        name_match = re.match(r'_Z(\d+)(\w+)', mangled)
        func_name = "unknown"
        suffix_part = "" # åŒ…å«æ¨¡æ¿å‚æ•° æˆ– å‡½æ•°å‚æ•°ç­¾å
        
        if name_match:
            length = int(name_match.group(1))
            full_string = name_match.group(2)
            func_name = full_string[:length]
            suffix_part = full_string[length:] # å‰©ä¸‹çš„éƒ¨åˆ†
        else:
            func_name = mangled

        # 2. è§£æå‘é‡åŒ–å®½åº¦ (Width)
        width = "Scalar"
        
        # ç­–ç•¥ A: æŸ¥æ‰¾æ¨¡æ¿å‚æ•°ä¸­çš„ Li (Literal Int)ï¼Œä¾‹å¦‚ <float, 4> -> Li4
        vec_match_template = re.search(r'Li(\d+)', suffix_part)
        
        # ç­–ç•¥ B: [æ–°å¢] æŸ¥æ‰¾å‡½æ•°åä¸­çš„ vecXï¼Œä¾‹å¦‚ sigmoid_vec4
        vec_match_name = re.search(r'vec(\d+)', func_name, re.IGNORECASE)
        
        if vec_match_template:
            width = vec_match_template.group(1)
        elif vec_match_name:
            width = vec_match_name.group(1)
        elif "vec" in func_name.lower():
            width = "?" # å³ä½¿æ˜¯å‘é‡åŒ–å‡½æ•°ï¼Œä¹Ÿæ²¡æ‰¾åˆ°å…·ä½“æ•°å­—
            
        # 3. è§£ææ•°æ®ç±»å‹ (Data Type)
        dtype = "Unknown"
        name_lower = func_name.lower()
        
        # ç­–ç•¥ A: [æ–°å¢] ä¼˜å…ˆæ£€æŸ¥å‡½æ•°åä¸­çš„æ˜¾å¼æ ‡è®° (å¦‚ fp16_vec4)
        if "fp16" in name_lower or "half" in name_lower:
            dtype = "Half(FP16)"
        elif "bf16" in name_lower or "bfloat16" in name_lower:
            dtype = "BFloat16"
        elif "fp64" in name_lower or "double" in name_lower:
            dtype = "double(FP64)"
        elif "fp32" in name_lower or "float" in name_lower:
            dtype = "float(FP32)"
        
        # ç­–ç•¥ B: æ£€æŸ¥ Mangled Suffix (æ¨¡æ¿å‚æ•° æˆ– å‡½æ•°å‚æ•°ç±»å‹)
        if dtype == "Unknown":
            # Half æ£€æµ‹: PyTorch ATen Half (N3c104HalfE) æˆ– CUDA __half (6__half)
            if 'Half' in suffix_part or '__half' in suffix_part:
                dtype = "Half(FP16)"
            elif 'BFloat16' in suffix_part or '__nv_bfloat16' in suffix_part:
                dtype = "BFloat16"
            
            # Double æ£€æµ‹: æ¨¡æ¿ Id, æŒ‡é’ˆ Pd/PKd
            elif 'Id' in suffix_part or 'Pd' in suffix_part or 'PKd' in suffix_part:
                 dtype = "double(FP64)"
            
            # Float æ£€æµ‹: æ¨¡æ¿ If, æŒ‡é’ˆ Pf/PKf
            elif 'If' in suffix_part or 'Pf' in suffix_part or 'PKf' in suffix_part:
                 dtype = "float(FP32)"
            
            # å…œåº•: ç®€å•å­—ç¬¦åŒ¹é… (æ…ç”¨ï¼Œé˜²æ­¢åŒ¹é…åˆ°å‡½æ•°åçš„ä¸€éƒ¨åˆ†)
            elif 'd' in suffix_part and 'f' not in suffix_part: # åªæœ‰dæ²¡æœ‰f
                 dtype = "double(FP64)"
            elif 'f' in suffix_part and 'd' not in suffix_part: # åªæœ‰fæ²¡æœ‰d
                 dtype = "float(FP32)"

        # æ„å»ºå¯è¯»å‡½æ•°å (Pretty Name)
        clean_func_name = func_name
        # å¦‚æœç±»å‹å·²çŸ¥ï¼Œç”Ÿæˆç±»ä¼¼ sigmoid<float, 4> çš„åå­—
        type_str = dtype.split('(')[0]
        if width != "Scalar" and width != "?":
            pretty_name = f"{clean_func_name}<{type_str}, {width}>"
        else:
            pretty_name = f"{clean_func_name}<{type_str}>"
            
        return func_name, pretty_name, dtype, width

    try:
        # 1. æŒ‰ Entry Function åˆ†å—
        blocks = log_str.split("Compiling entry function")
        
        # æ‰“å°è¡¨å¤´
        print(f"\n{'='*100}")
        print(f"{'å†…æ ¸å‡½æ•° (Mangled Name)':<45} | {'æ•°æ®ç±»å‹':<12} | {'å®½åº¦':<5} | {'å¯„å­˜å™¨':<6} | {'å¤‡æ³¨ (Local/Const/Shared)'}")
        print(f"{'-'*100}")

        for block in blocks:
            if not block.strip(): continue
            
            # æå– Mangled Name
            name_match = re.search(r"\'(_Z\w+)\'", block)
            if not name_match: continue
            
            mangled_name = name_match.group(1)
            func_base, pretty_name, dtype, width = _demangle_info(mangled_name)
            
            # æå–æŒ‡æ ‡
            regs = 0
            reg_match = re.search(r'Used\s+(\d+)\s+registers', block)
            if reg_match: regs = int(reg_match.group(1))
            
            # å†…å­˜æŒ‡æ ‡
            smem = 0
            smem_match = re.search(r'(\d+)\s+bytes\s+smem', block)
            if smem_match: smem = int(smem_match.group(1))
            
            cmem_matches = re.findall(r'(\d+)\s+bytes\s+cmem', block)
            cmem_str = "+".join(cmem_matches) if cmem_matches else "0"
            cmem_total = sum(int(x) for x in cmem_matches)
            
            spill_store = 0
            spill_load = 0
            spill_s = re.search(r'(\d+)\s+bytes\s+spill\s+stores', block)
            spill_l = re.search(r'(\d+)\s+bytes\s+spill\s+loads', block)
            if spill_s: spill_store = int(spill_s.group(1))
            if spill_l: spill_load = int(spill_l.group(1))
            spill_total = spill_store + spill_load
            
            # æ„å»ºå¤‡æ³¨
            remarks = []
            if cmem_total > 0: remarks.append(f"Cmem: {cmem_str}")
            if smem > 0: remarks.append(f"Smem: {smem}")
            if spill_total > 0: remarks.append(f"SPILL: {spill_total}B")
            remark_str = ", ".join(remarks)
            
            # æ‰“å°è¡¨æ ¼è¡Œ
            display_mangled = (mangled_name[:42] + '..') if len(mangled_name) > 44 else mangled_name
            print(f"{display_mangled:<45} | {dtype:<12} | {width:<5} | {regs:<6} | {remark_str}")

            # å­˜å…¥ metrics å­—å…¸
            metrics[pretty_name] = {
                "registers": regs,
                "spill_bytes": spill_total,
                "cmem_bytes": cmem_total,
                "smem_bytes": smem,
                "type": dtype,
                "width": width
            }
            
        print(f"{'='*100}\n")

    except Exception as e:
        print(f"è­¦å‘Šï¼šè§£æ PTXAS æ—¥å¿—å¤±è´¥: {e}", file=sys.stderr)
    
    return metrics


def get_kernel_name(cuda_code_string):
    """
    ä» CUDA æºä»£ç å­—ç¬¦ä¸²ä¸­è§£æå‡ºç¬¬ä¸€ä¸ª __global__ void å†…æ ¸å‡½æ•°çš„åç§°ã€‚
    
    å‚æ•°:
        cuda_code_string (str): åŒ…å« CUDA æºä»£ç çš„å­—ç¬¦ä¸²ã€‚
    
    è¿”å›:
        str: æ‰¾åˆ°çš„å†…æ ¸å‡½æ•°åç§°ã€‚å¦‚æœæœªæ‰¾åˆ°ï¼Œè¿”å› Noneã€‚
    """
    # æ­£åˆ™è¡¨è¾¾å¼è§£é‡Šï¼š
    # __global__  : åŒ¹é… __global__ å…³é”®å­—
    # \s+         : åŒ¹é…ä¸€ä¸ªæˆ–å¤šä¸ªç©ºæ ¼
    # void        : åŒ¹é… void å…³é”®å­—
    # \s+         : åŒ¹é…ä¸€ä¸ªæˆ–å¤šä¸ªç©ºæ ¼
    # ([a-zA-Z0-9_]+) : æ•è·ç»„ï¼ŒåŒ¹é…å†…æ ¸åç§°ï¼ˆå­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿ï¼‰
    # \s*\(       : åŒ¹é…é›¶ä¸ªæˆ–å¤šä¸ªç©ºæ ¼åè·Ÿå·¦æ‹¬å· (ï¼Œæ ‡å¿—ç€å‡½æ•°å‚æ•°çš„å¼€å§‹
    pattern = r"__global__\s+void\s+([a-zA-Z0-9_]+)\s*\("
    
    match = re.search(pattern, cuda_code_string)
    
    if match:
        return match.group(1)
    else:
        return None

# vvv --- [!!! å·²æ›´æ–° !!!] çœŸå® NCU åˆ†æå™¨ (ç°åœ¨æ˜¯é€šç”¨çš„) --- vvv
def get_real_ncu_metrics(module_path, module_name, inputs, init_inputs=None, cuda_code=None) -> Dict[str, float]:
    """
    åŠ¨æ€åˆ›å»ºä¸€ä¸ªç›®æ ‡è„šæœ¬ï¼Œè¿è¡Œ ncuï¼Œè§£æ CSV è¾“å‡ºï¼Œå¹¶è¿”å›æŒ‡æ ‡ã€‚
    [!!! å·²æ›´æ–° !!!] æ¥å—é€šç”¨è¾“å…¥å’Œå†…æ ¸/wrapper åç§°ã€‚
    """
    kernel_name = get_kernel_name(cuda_code)
    ncu_metrics = {}
    target_script_path = f"_ncu_target_{module_name}.py"
    temp_csv_path = f"_ncu_output_{module_name}.csv"
    
    try:
        # 1. å†™å…¥ ncu ç›®æ ‡è„šæœ¬
        with open(target_script_path, "w", encoding="utf-8") as f:
            f.write(NCU_TARGET_SCRIPT_TEMPLATE)

        # [!!! å·²æ›´æ–° !!!] ä¿å­˜è¾“å…¥ä»¥ä¾› ncu è„šæœ¬åŠ è½½
        torch.save(inputs, '_ncu_inputs.pt')
        if init_inputs is not None:
            torch.save(init_inputs, '_ncu_init_inputs.pt')

        # 2. æ„å»º ncu å‘½ä»¤ (ä¸å¸¦ --metrics ä»¥è·å–å…¨é›†)
        ncu_command = [
            'ncu',
            '--csv',
            '--profile-from-start', 'off',
            # '--kernel-name', kernel_name, # <--- [!!! å·²åˆ é™¤ !!!]
            # '--launch-count', '1',
            '--kernel-name',f'{kernel_name}',
            '--clock-control', 'none', # é¿å… ncu é”å®šé¢‘ç‡
            '--target-processes', 'all',
            'python', 
            target_script_path,
            module_path, 
            module_name
            # wrapper_function_name # <--- [!!! å·²æ›´æ–° !!!]
            # [!!! å·²ç§»é™¤ !!!] str(matrix_n)
        ]
        
        print(f"--- [ æ­£åœ¨è¿è¡Œ NCU (å…¨é›†)... ] ---")
        # print(f"å‘½ä»¤: {' '.join(ncu_command)}") # è°ƒè¯•æ—¶å–æ¶ˆæ³¨é‡Š

        # 3. è¿è¡Œ ncu
        proc = subprocess.run(
            ncu_command, 
            capture_output=True, 
            text=True, 
            encoding="utf-8", 
            errors="ignore",
            timeout=300 # NCU (å…¨é›†) å¯èƒ½éå¸¸æ…¢
        )

        if proc.returncode != 0:
            print(f"è­¦å‘Šï¼šNCU è¿è¡Œå¤±è´¥ã€‚è¿”å›ç : {proc.returncode}", file=sys.stderr)
            print(f"NCU Stderr: {proc.stderr}", file=sys.stderr)
            return ncu_metrics

        try:
            with open(temp_csv_path, "w", encoding="utf-8") as debug_f:
                debug_f.write(proc.stdout)
            print(f"--- [DEBUG] NCU CSV å†…å®¹å·²ä¿å­˜è‡³: {temp_csv_path} ---")
        except Exception as e:
            print(f"è­¦å‘Šï¼šä¿å­˜è°ƒè¯• CSV æ–‡ä»¶å¤±è´¥: {e}", file=sys.stderr)

        # 4. è§£æ CSV è¾“å‡º
        csv_reader = csv.reader(io.StringIO(proc.stdout))
        metric_name_idx = -1
        metric_value_idx = -1

        for row in csv_reader:
            if "Metric Name" in row and "Metric Value" in row:
                header = [h.strip().strip('"') for h in row]
                try:
                    metric_name_idx = header.index("Metric Name")
                    metric_value_idx = header.index("Metric Value")
                except ValueError:
                    print(f"è­¦å‘Šï¼šåœ¨ NCU CSV è¡¨å¤´ä¸­æ‰¾ä¸åˆ° 'Metric Name' æˆ– 'Metric Value'ã€‚", file=sys.stderr)
                    return ncu_metrics
                continue 

            if metric_name_idx != -1 and len(row) > max(metric_name_idx, metric_value_idx):
                
                # [!!! å·²åˆ é™¤ !!!] 
                # if kernel_name not in str(row):
                #     continue

                metric_name = row[metric_name_idx].strip().strip('"')
                val_str = row[metric_value_idx].strip().strip('"')
                
                if not metric_name or not val_str:
                    continue

                try:
                    # æ¸…ç†æŒ‡æ ‡åç§°
                    cleaned_name = re.sub(r'[^a-zA-Z0-9_.]', '', metric_name)
                    
                    val_str_cleaned = val_str.replace(',', '')
                    if val_str_cleaned == "N/A":
                        val = 0.0
                    else:
                        val = float(val_str_cleaned)

                    ncu_metrics[cleaned_name] = val
                
                except (ValueError, IndexError):
                    pass
        
        if not ncu_metrics:
            print(f"è­¦å‘Šï¼šæ— æ³•ä» NCU CSV è¾“å‡ºä¸­è§£æä»»ä½• {kernel_name} æŒ‡æ ‡æ•°æ®ã€‚", file=sys.stderr)
            # print(f"NCU STDOUT: {proc.stdout}") # è°ƒè¯•æ—¶å–æ¶ˆæ³¨é‡Š
            # print(f"NCU STDERR: {proc.stderr}") # è°ƒè¯•æ—¶å–æ¶ˆæ³¨é‡Š
            return ncu_metrics

    except FileNotFoundError:
        print("="*50, file=sys.stderr)
        print("è¯„ä¼°å™¨é”™è¯¯ï¼šæ‰¾ä¸åˆ° 'ncu' (Nsight Compute)ã€‚", file=sys.stderr)
        print("è¯·ç¡®ä¿ NVIDIA Nsight Compute å·²å®‰è£…å¹¶åœ¨æ‚¨çš„ç³»ç»Ÿ PATH ä¸­ã€‚", file=sys.stderr)
        print("="*50, file=sys.stderr)
        sys.exit(1) # è¿™æ˜¯ä¸€ä¸ªå…³é”®é”™è¯¯ï¼Œç»ˆæ­¢ç¨‹åº
    except Exception as e:
        print(f"è­¦å‘Šï¼šNCU åˆ†ææœŸé—´å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", file=sys.stderr)
        traceback.print_exc(file=sys.stderr)
    
    finally:
        if os.path.exists(target_script_path):
            os.remove(target_script_path)
        # [!!! æ–°å¢ !!!] æ¸…ç† ncu è¾“å…¥æ–‡ä»¶
        if os.path.exists("_ncu_inputs.pt"):
            os.remove("_ncu_inputs.pt")
        if os.path.exists("_ncu_init_inputs.pt"):
            os.remove("_ncu_init_inputs.pt")
        if os.path.exists(temp_csv_path):
            os.remove(temp_csv_path)
            
    print(f"--- [ NCU æŒ‡æ ‡å·²è§£æ (å…± {len(ncu_metrics)} ä¸ª) ] ---")
    if ncu_metrics:
        sample_keys = random.sample(list(ncu_metrics.keys()), min(5, len(ncu_metrics)))
        sample_metrics = {k: ncu_metrics[k] for k in sample_keys}
        print(json.dumps(sample_metrics, indent=2))
        
    return ncu_metrics
# ^^^ --- NCU å‡½æ•°ç»“æŸ --- ^^^


# vvv --- [!!! å·²æ›´æ–° !!!] çœŸå®æ€§èƒ½è¯„æµ‹å‡½æ•° (ç°åœ¨æ˜¯é€šç”¨çš„) --- vvv
def benchmark_kernel(inputs, module, warmup_runs=5, benchmark_runs=10):
    """
    å¯¹å½“å‰åŠ è½½çš„ _gemm_module æ‰§è¡Œé¢„çƒ­å’ŒåŸºå‡†æµ‹è¯•ã€‚
    [!!! å·²æ›´æ–° !!!] æ¥å—é€šç”¨è¾“å…¥ã€‚
    """
    # if _gemm_module is None:
    #     raise RuntimeError("æ¨¡å—æœªç¼–è¯‘ã€‚")
    
    # gpu_inputs = [t.cuda() if isinstance(t, torch.Tensor) and not t.is_cuda else t for t in inputs]
    cloned_inputs = None
    try:
        cloned_inputs = copy.deepcopy(inputs)
        print(f"Warming up evolved kernel ({warmup_runs} runs)...")
        for _ in range(warmup_runs):
            _ = run_gemm(cloned_inputs, module)
        torch.cuda.synchronize()

        # æµ‹é‡
        start = torch.cuda.Event(enable_timing=True)
        end = torch.cuda.Event(enable_timing=True)

        start.record()
        for _ in range(benchmark_runs):
            _ = run_gemm(cloned_inputs, module)
        end.record()

        torch.cuda.synchronize()
        avg_time_ms = start.elapsed_time(end) / benchmark_runs
        print(f"Evolved kernel benchmark: {avg_time_ms:.3f} ms")
        return avg_time_ms
    finally:
        # ==========================================================
        # [å…³é”®ä¿®æ”¹] è‡ªåŠ¨æ¸…ç†é€»è¾‘
        # ==========================================================
        # æ— è®º try å—ä¸­å‘ç”Ÿäº†ä»€ä¹ˆï¼ˆæ­£å¸¸ return outputï¼Œæˆ–è€…æŠ›å‡º Exceptionï¼‰ï¼Œ
        # finally å—ä¸­çš„ä»£ç æ°¸è¿œä¼šåœ¨å‡½æ•°é€€å‡ºå‰æœ€åæ‰§è¡Œã€‚
        
        if cloned_inputs is not None:
            # 1. è§£é™¤å˜é‡å¼•ç”¨ï¼Œä½¿ Tensor å¯¹è±¡çš„å¼•ç”¨è®¡æ•°å‡ 1
            del cloned_inputs
            
            # 2. (å¯é€‰) å¦‚æœæ˜¾å­˜æåº¦ç´§å¼ ï¼Œå¯ä»¥æ‰‹åŠ¨è§¦å‘ Python GC
            # è¿™èƒ½ç¡®ä¿ PyTorch çš„ C++ åç«¯æ›´å¿«æ”¶åˆ°â€œæ˜¾å­˜å¯é‡Šæ”¾â€çš„ä¿¡å·
            # gc.collect()
    
# ^^^ --- æ€§èƒ½è¯„æµ‹å‡½æ•°ç»“æŸ --- ^^^


def get_pytorch_performance(A_torch, B_torch):
    """(æ­¤å‡½æ•°ä¿æŒä¸å˜, ä»…ç”¨äºåŸå§‹ main() çš„åå‘å…¼å®¹)"""
    print("Warming up PyTorch...")
    for _ in range(10):
        _ = torch.matmul(A_torch, B_torch)
    torch.cuda.synchronize()

    start = torch.cuda.Event(enable_timing=True)
    end = torch.cuda.Event(enable_timing=True)
    
    start.record()
    for _ in range(50):
        _ = torch.matmul(A_torch, B_torch)
    end.record()
    
    torch.cuda.synchronize()
    avg_time_ms = start.elapsed_time(end) / 50
    return avg_time_ms