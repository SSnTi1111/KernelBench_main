import config
import kernels
import cuda_utils
import llm_api as agents 
import prompts            
import torch
from tqdm import tqdm
import os
import re
import ast
import sys
import json 
from typing import List, Dict # [!!! æ–°å¢ !!!]

import gc
try:
    import numpy as np
    from sentence_transformers import SentenceTransformer
    from sklearn.metrics.pairwise import cosine_similarity
    HAS_EMBEDDING_LIB = True
except ImportError:
    print("[Warning] 'sentence_transformers' or 'sklearn' not found. Embedding similarity check will be disabled.")
    HAS_EMBEDDING_LIB = False

ROOT = os.path.dirname(os.path.dirname(__file__))
sys.path.append(os.path.join(ROOT, "test_mini_version"))

from test_script import validate_extracted_code, correct_cuda_kernel



# [!!! æ–°å¢ !!!] å…¨å±€ Embedding æ¨¡å‹å®ä¾‹ (å•ä¾‹æ¨¡å¼)
_EMBEDDING_MODEL = None
EMBEDDING_MODEL_PATH = "/home/lxt/models/all-MiniLM-L6-v2" # ç”¨æˆ·æŒ‡å®šçš„è·¯å¾„

def get_embedding_model():
    """
    æ‡’åŠ è½½ Embedding æ¨¡å‹ï¼Œç¡®ä¿åªåŠ è½½ä¸€æ¬¡ã€‚
    """
    global _EMBEDDING_MODEL
    if not HAS_EMBEDDING_LIB:
        return None

    if _EMBEDDING_MODEL is None:
        try:
            if os.path.exists(EMBEDDING_MODEL_PATH):
                # print(f"[System] Loading embedding model from {EMBEDDING_MODEL_PATH} ...")
                _EMBEDDING_MODEL = SentenceTransformer(EMBEDDING_MODEL_PATH)
            else:
                print(f"[Warning] Embedding model path not found: {EMBEDDING_MODEL_PATH}. Falling back to exact match.")
                # ä¹Ÿå¯ä»¥å°è¯•è‡ªåŠ¨ä¸‹è½½: _EMBEDDING_MODEL = SentenceTransformer('all-MiniLM-L6-v2')
        except Exception as e:
            print(f"[Warning] Failed to load embedding model: {e}")
            
    return _EMBEDDING_MODEL

def extract_code(response_text):
    """(æ­¤å‡½æ•°ä¿æŒä¸å˜)"""
    if not response_text: return None 
    match = re.search(r'```cuda\n(.*?)```', response_text, re.DOTALL)
    if not match:
        if "torch::Tensor gemm_cuda" in response_text: 
             return response_text
        print("[Coder Agent] Error: No CUDA code block found in response.")
        return None
            
    return match.group(1).strip()

def extract_metrics(response_text):
    """(æ­¤å‡½æ•°ä¿æŒä¸å˜)"""
    if not response_text: return None 
    try:
        metrics_list_str = response_text.split("METRICS:")[1].strip()
        metrics_list = ast.literal_eval(metrics_list_str) 
        return metrics_list
    except Exception as e:
        print(f"[Tool Agent] Error parsing metrics list: {e}\nResponse was: {response_text}")
        return None

# def get_diverse_champions(history: list, current_best_code: str, num_kernels=2) -> str: # é’ˆå¯¹TODO2 åšçš„ä¿®æ”¹ï¼Œå®Œæ•´çš„ä¿®æ”¹åœ¨ä¸‹é¢ğŸ‘‡
#     """
#     (æ­¤å‡½æ•°ä¿æŒä¸å˜)
#     """
    
#     # 1. æŸ¥æ‰¾æ‰€æœ‰æˆåŠŸçš„æ¡ç›® (ä¸åŒ…æ‹¬ Round 0)
#     success_entries = [
#         h for h in history 
#         if "Success" in h['status'] and h['round'] > 0 and h.get('code')
#     ]
    
#     # 2. æŒ‰æ€§èƒ½æ’åº
#     success_entries.sort(key=lambda x: x['time_ms'])
    
#     diverse_str = "--- Diverse Successful Kernel Examples (Best first) ---\n"
#     count = 0
    
#     # 3. æå–ä»£ç  (ç¡®ä¿å®ƒä¸å½“å‰æœ€ä½³ä»£ç  *ä¸åŒ*)
#     for entry in success_entries:
#         if entry['code'] == current_best_code:
#             continue # è·³è¿‡ä¸å½“å‰æœ€ä½³å®Œå…¨ç›¸åŒçš„ä»£ç 
            
#         diverse_str += f"\n\n--- Example {count+1} (From Round {entry['round']}) ---\n"
#         diverse_str += f"// Goal: {entry['goal']}\n"#è¿™ä¸ªç›®æ ‡æ˜¯çˆ¶èŠ‚ç‚¹ä»£ç çš„ä¼˜åŒ–ç›®æ ‡ï¼Œä¼˜åŒ–åçš„ä»£ç æ˜¯å½“å‰çš„ä»£ç 
#         diverse_str += f"// Performance: {entry['time_ms']:.3f} ms\n"#æ˜¯å½“å‰ä»£ç çš„æ‰§è¡Œæ—¶é—´
        
#         # æ·»åŠ  PTXAS æŒ‡æ ‡
#         ptxas = entry.get('ptxas_metrics', {})# æ˜¯å½“å‰ä»£ç æ‰§è¡Œè¿‡ç¨‹ä¸­çš„PTXASä¿¡æ¯
#         for k, v in sorted(ptxas.items()):
#             diverse_str += f"// {k}: {v}\n"
        
#         # [!!! æ–°å¢ !!!] ä»…æ·»åŠ è¯¥è½®é€‰æ‹©çš„ NCU æŒ‡æ ‡
#         selected_metrics = entry.get('selected_ncu_metrics')
#         all_ncu = entry.get('all_ncu_metrics')
        
#         if isinstance(selected_metrics, list) and isinstance(all_ncu, dict) and selected_metrics:
#             diverse_str += f"// Selected NCU Metrics (for Goal):\n"
#             for metric_name in selected_metrics:
#                 value = all_ncu.get(metric_name, 'N/A')
#                 diverse_str += f"//  - {metric_name}: {value}\n"
#         # [!!! ç»“æŸæ–°å¢ !!!]

#         diverse_str += entry['code']
#         count += 1

#         if count >= num_kernels:
#             break
            
#     if count == 0:
#         return "No other diverse successful examples available in history."
#     return diverse_str

# def summarize_history(history: list) -> str: # ç”±äºTODO 1 åšçš„ä¿®æ”¹ï¼Œå®Œæ•´çš„å®ç°è§ğŸ‘‡
#     """
#     (æ­¤å‡½æ•°ä¿æŒä¸å˜)
#     """
#     if not history:
#         return "No previous attempts."
    
#     summary = "Previous Optimization Attempts:\n"
#     for i, entry in enumerate(history):
#         summary += f"  Round {entry['round']}:\n"
#         summary += f"    Goal: {entry['goal']}\n"
#         summary += f"    Status: {entry['status']}\n"
        
#         perf_str = "N/A"
#         if entry['time_ms'] is not None:
#             perf_str = f"{entry['time_ms']:.3f} ms"
#         summary += f"    Performance: {perf_str}\n"

#         # æ·»åŠ  PTXAS æŒ‡æ ‡
#         if entry.get('ptxas_metrics'):
#             # ä½¿ç”¨ sorted ä¿è¯è¾“å‡ºé¡ºåºç¨³å®šï¼Œæ–¹ä¾¿é˜…è¯»
#             for k, v in sorted(entry['ptxas_metrics'].items()):
#                 summary += f"    {k}: {v}\n"

#         # [!!! æ–°å¢ !!!] ä»…æ·»åŠ è¯¥è½®é€‰æ‹©çš„ NCU æŒ‡æ ‡
#         selected_metrics = entry.get('selected_ncu_metrics')
#         all_ncu = entry.get('all_ncu_metrics')
        
#         # æ£€æŸ¥ 'selected_metrics' æ˜¯å¦æ˜¯åˆ—è¡¨ï¼Œ'all_ncu' æ˜¯å¦æ˜¯å­—å…¸ï¼Œå¹¶ä¸” 'selected_metrics' ä¸ä¸ºç©º
#         if isinstance(selected_metrics, list) and isinstance(all_ncu, dict) and selected_metrics:
#             summary += f"    Selected NCU Metrics (for Goal):\n"
#             for metric_name in selected_metrics:
#                 value = all_ncu.get(metric_name, 'N/A')
#                 summary += f"      - {metric_name}: {value}\n"
#         # [!!! ç»“æŸæ–°å¢ !!!]

#         elif "Error" in entry['status'] or "Failed" in entry['status']:
#             details = entry.get('details', 'No details')
#             if len(details) > 200:
#                 details = details[:200] + "..."
#             summary += f"    Error Details: {details}\n"
#     return summary

def get_diverse_champions(history: list, current_best_code: str, num_kernels=2) -> str:
    """
    (ä¼˜åŒ–ç‰ˆ) è·å–å¤šæ ·åŒ–çš„æˆåŠŸæ¡ˆä¾‹ï¼Œæ„å»ºæ¸…æ™°çš„ [é—®é¢˜ -> æ–¹æ¡ˆ -> ä»£ç  -> ç»“æœ] å› æœé“¾ã€‚
    """
    
    # 1. æŸ¥æ‰¾æ‰€æœ‰æˆåŠŸçš„æ¡ç›® (ä¸åŒ…æ‹¬ Round 0ï¼Œå› ä¸º Round 0 æ²¡æœ‰ä¼˜åŒ–åŠ¨æœº)
    success_entries = [
        h for h in history 
        if ("Success" in h['status'] or "Failed (Performance Regression)" == h['status']) and h['round'] > 0 and h.get('code')
    ]
    
    # 2. æŒ‰æ€§èƒ½æ’åº (è¶Šå¿«è¶Šå‰)
    success_entries.sort(key=lambda x: x['time_ms'])
    
    diverse_str = "--- Diverse Successful Kernel Examples (Best first) ---\n"
    count = 0

    model = get_embedding_model()
    current_best_emb = None
    similarity_threshold = 0.95 # ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œå¤§äºæ­¤å€¼è§†ä¸ºâ€œç›¸åŒ/å¤ªç›¸ä¼¼â€

    if model:
        try:
            # é¢„å…ˆè®¡ç®— current_best_code çš„ embedding
            current_best_emb = model.encode(current_best_code).reshape(1, -1)
        except Exception as e:
            print(f"[Warning] Embedding calculation failed for current best code: {e}")
    
    # 3. æå–ä»£ç 
    for entry in success_entries:
        # DONE: è¿™é‡Œåº”è¯¥æ”¹æˆç”¨embeddingåšè®¡ç®—ï¼Œç›´æ¥åˆ¤æ–­ç›¸ç­‰å¤ªç»å¯¹äº†
        # if entry['code'] == current_best_code:
        #     continue # è·³è¿‡ä¸å½“å‰æœ€ä½³å®Œå…¨ç›¸åŒçš„ä»£ç 
        is_too_similar = False
        
        # --- Embedding ç›¸ä¼¼åº¦è®¡ç®—é€»è¾‘ ---
        if model and current_best_emb is not None:
            try:
                entry_code = entry.get('code', '')
                if not entry_code: continue
                
                entry_emb = model.encode(entry_code).reshape(1, -1)
                sim = cosine_similarity(current_best_emb, entry_emb)[0][0]
                
                # å¦‚æœç›¸ä¼¼åº¦è¿‡é«˜ï¼Œè®¤ä¸ºç¼ºä¹å¤šæ ·æ€§ï¼Œè·³è¿‡
                if sim > similarity_threshold:
                    print(f"[Info] Skipping similar kernel (Sim: {sim:.4f})")# å› ä¸ºå†å²ä¿¡æ¯ä¸­åŒ…å«è‡ªå·±ï¼Œå› æ­¤è‚¯å®šæœ‰ä¸€ä¸ªç”¨ä¾‹çš„ç›¸ä¼¼åº¦æ˜¯1ä¼šè·³è¿‡
                    is_too_similar = True
            except Exception as e:
                # å‘ç”Ÿå¼‚å¸¸é™çº§ä¸ºå­—ç¬¦ä¸²æ¯”è¾ƒ
                if entry['code'] == current_best_code:
                    is_too_similar = True
        else:
            # é™çº§ï¼šå¦‚æœæ²¡æœ‰æ¨¡å‹ï¼Œä½¿ç”¨åŸå§‹å­—ç¬¦ä¸²å®Œå…¨ç›¸ç­‰åˆ¤æ–­
            if entry['code'] == current_best_code:
                is_too_similar = True

        if is_too_similar:
            continue

        print("æ‰¾åˆ°ä¸€ä¸ªæ­£ç¡®çš„ä¸”ä¸ç›¸ä¼¼çš„æ ·ä¾‹!!!!")
            
        diverse_str += f"\n\n/* ==================================================================================\n"
        diverse_str += f" * Example {count+1} (From Round {entry['round']})\n"
        diverse_str += f" * ================================================================================== */\n"
        
        # --- 1. [Before] ä¿®æ”¹å‰çš„ç¼ºé™· (Motivation) ---
        # è§£é‡Šï¼šè¿™æ˜¯å¯¹â€œä¸Šä¸€ç‰ˆä»£ç â€çš„è¯Šæ–­ï¼Œæ˜¯äº§ç”Ÿå½“å‰è¿™æ®µä»£ç çš„æ ¹æœ¬åŸå› 
        diag = entry.get('bottleneck_analysis', 'N/A')
        diverse_str += f"// [1. Motivation] Bottleneck of the Previous Kernel (Before Modification):\n"
        diverse_str += f"//    Diagnosis: {diag}\n"
        
        # --- 2. [Action] ä¼˜åŒ–ç›®æ ‡ä¸ç­–ç•¥ (Strategy) ---
        # è§£é‡Šï¼šä¸ºäº†è§£å†³ä¸Šè¿°ç“¶é¢ˆï¼Œæˆ‘ä»¬è®¾å®šäº†è¿™ä¸ªç›®æ ‡ï¼Œå¹¶ç”Ÿæˆäº†ä¸‹é¢çš„ä»£ç 
        goal = entry.get('goal', 'N/A')
        plan = entry.get('detailed_plan', 'N/A').replace('\n', ' ')[:120] + "..." # æˆªå–éƒ¨åˆ†è®¡åˆ’
        diverse_str += f"// [2. Strategy] Optimization Goal & Plan (Target of this Code):\n"
        diverse_str += f"//    Goal: {goal}\n"
        diverse_str += f"//    Plan Snippet: {plan}\n"
        
        # --- 3. [Code] ä¼˜åŒ–åçš„ä»£ç  (Implementation) ---
        diverse_str += f"\n/* [3. Implementation] Optimized CUDA Kernel (Addressing the Goal above) */\n"
        diverse_str += entry['code'] + "\n"
        
        # --- 4. [After] ä¼˜åŒ–åçš„æŒ‡æ ‡ (Outcome) ---
        diverse_str += f"\n/* [4. Outcome] Performance Metrics of THIS Code (After Modification) */\n"
        diverse_str += f"// Execution Time: {entry['time_ms']:.3f} ms\n"
        
        # PTXAS æŒ‡æ ‡ (ç¼–è¯‘å™¨åé¦ˆ)
        # ptxas = entry.get('ptxas_metrics', {})# é’ˆå¯¹TODO3åšçš„ä¿®æ”¹ï¼Œå®Œæ•´çš„ä¿®æ”¹å†…å®¹åœ¨ğŸ‘‡
        # if ptxas:
        #     diverse_str += "// PTXAS Compiler Stats:\n"
        #     for k, v in sorted(ptxas.items()):
        #         diverse_str += f"//   - {k}: {v}\n"
        ptxas = entry.get('ptxas_metrics', {})
        if ptxas:
            diverse_str += "// PTXAS Compiler Stats:\n"
            for kernel_name, stats in sorted(ptxas.items()):
                if isinstance(stats, dict):
                    # æ–°æ ¼å¼ï¼šæå–å…³é”®æŒ‡æ ‡å¹¶ç´§å‡‘å±•ç¤º
                    info_parts = []
                    # 1. å¯„å­˜å™¨
                    if 'registers' in stats: 
                        info_parts.append(f"Regs={stats['registers']}")
                    
                    # 2. æº¢å‡º (é‡ç‚¹å…³æ³¨)
                    spill = stats.get('spill_bytes', 0)
                    if spill > 0: 
                        info_parts.append(f"SPILL={spill}B") # æº¢å‡ºæ—¶æ˜¾å¼æ˜¾ç¤º
                    else:
                        info_parts.append("Spill=0B")
                        
                    # 3. å‘é‡åŒ–å®½åº¦ (å¦‚æœæœ‰)
                    width = stats.get('width')
                    if width and width not in ["Scalar", "?", ""]: 
                        info_parts.append(f"Vec={width}")
                    
                    # 4. å¸¸é‡/å…±äº«å†…å­˜ (å¯é€‰ï¼Œçœ‹ä½ éœ€è¦)
                    if stats.get('smem_bytes', 0) > 0:
                        info_parts.append(f"SMem={stats['smem_bytes']}B")
                    
                    details = ", ".join(info_parts)
                    # ç®€åŒ– Kernel åå­—æ˜¾ç¤ºï¼Œå»é™¤è¿‡é•¿çš„æ¨¡æ¿å‚æ•°
                    short_name = kernel_name.split('<')[0] if '<' in kernel_name else kernel_name
                    if width and str(width).isdigit(): short_name += f"(vec{width})"
                    
                    diverse_str += f"//   - [{short_name}]: {details}\n"
                else:
                    # æ—§æ ¼å¼å…¼å®¹ (Fallback)
                    diverse_str += f"//   - {kernel_name}: {stats}\n"
        
        # NCU æŒ‡æ ‡ (è¿è¡Œæ—¶åé¦ˆ - ä»…å±•ç¤ºä¸ºäº†éªŒè¯ç›®æ ‡è€Œé€‰æ‹©çš„æŒ‡æ ‡)
        selected_metrics = entry.get('selected_ncu_metrics')
        all_ncu = entry.get('all_ncu_metrics')
        
        if isinstance(selected_metrics, list) and isinstance(all_ncu, dict) and selected_metrics:
            diverse_str += "// Key NCU Hardware Metrics (Verified against Goal):\n"
            for metric_name in selected_metrics:
                value = all_ncu.get(metric_name, 'N/A')
                diverse_str += f"//   - {metric_name}: {value}\n"
        
        count += 1
        if count >= num_kernels:
            break
            
    if count == 0:
        return "No other diverse successful examples available in history."
    return diverse_str

def summarize_history(history: list) -> str:
    """
    (ä¼˜åŒ–ç‰ˆ) ç”Ÿæˆå…·æœ‰æ˜ç¡®å› æœå…³ç³»é“¾çš„å†å²æ‘˜è¦ã€‚
    ç»“æ„ï¼š[Before: åŠ¨æœº] -> [Action: åŠ¨ä½œ] -> [After: ç»“æœ]
    """
    if not history:
        return "No previous attempts."
    
    summary = "=== Optimization Experiments Knowledge Base ===\n"
    
    # 1. Baseline å•ç‹¬å±•ç¤º
    baseline = history[0]
    summary += f"[Round 0 - Baseline] Performance: {baseline.get('time_ms', 'N/A'):.3f} ms\n"
    # å¦‚æœ Baseline æœ‰ NCU æŒ‡æ ‡ï¼Œä¹Ÿç¨å¾®å±•ç¤ºä¸€ä¸‹å…³é”®çš„
    if baseline.get('all_ncu_metrics'):
        base_ncu = baseline['all_ncu_metrics']
        
        # [!!! ä¿®å¤ !!!] ä½¿ç”¨å®é™…å­˜åœ¨çš„é”®å
        # ä¼˜å…ˆå°è¯•ç®€åŒ–çš„é”®åï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å°è¯•åŸå§‹é”®åï¼Œæœ€åå›é€€åˆ° N/A
        dram_val = base_ncu.get('DRAMThroughput', base_ncu.get('dram__throughput.avg.pct_of_peak_sustained_elapsed', 'N/A'))
        sm_val = base_ncu.get('ComputeSMThroughput', base_ncu.get('sm__throughput.avg.pct_of_peak_sustained_elapsed', 'N/A'))
        
        # å¦‚æœå–åˆ°äº†æ•°å€¼ï¼Œæ ¼å¼åŒ–ä¸€ä¸‹
        dram_str = f"{dram_val}%" if isinstance(dram_val, (int, float)) else "N/A"
        sm_str = f"{sm_val}%" if isinstance(sm_val, (int, float)) else "N/A"
        
        summary += f"  > Baseline Context: DRAM={dram_str}, SM={sm_str}\n"
        
    summary += "-" * 50 + "\n"

    # 2. å±•ç¤ºåç»­è¿­ä»£
    for entry in history[1:]:
        r = entry['round']
        status = entry['status']
        time_ms = entry.get('time_ms')
        perf_str = f"{time_ms:.3f} ms" if time_ms else "N/A"
        
        summary += f"[Round {r}] Status: {status} | Time: {perf_str}\n"
        
        # --- [Before: åŠ¨æœº] ---
        # è§£é‡Šï¼šè¿™æ˜¯é’ˆå¯¹â€œä¸Šä¸€è½®ä»£ç â€çš„è¯Šæ–­ï¼Œæ˜¯è¿™ä¸€è½®ä¼˜åŒ–çš„èµ·å› 
        diag = entry.get('bottleneck_analysis', 'N/A')
        summary += f"  > [Motivation] Bottlenecks in the kernel before optimization: {diag}\n"
        
        # --- [Action: åŠ¨ä½œ] ---
        # è§£é‡Šï¼šè¿™æ˜¯è¿™ä¸€è½®å…·ä½“åšäº†ä»€ä¹ˆ
        goal = entry.get('goal', 'N/A')
        summary += f"  > [Action] Optimization Goal: {goal}\n"
        
        # Plan æ‘˜è¦
        plan_text = entry.get('detailed_plan', '')
        if plan_text:
            plan_snippet = plan_text.replace('\n', ' ')[:150] + "..."
            summary += f"  > [Action] Plan Details: {plan_snippet}\n"

        # --- [After: ç»“æœ] ---
        # è§£é‡Šï¼šè¿™æ˜¯è¿™ä¸€è½®ä»£ç ç¼–è¯‘å’Œè¿è¡Œåçš„å®¢è§‚æ•°æ®
        summary += "  > [Result] According to the optimization objective, the information of each index of the optimized CUDA kernel is as follows:(PTXAS & NCU):\n"
        
        # 1. PTXAS (ç¼–è¯‘å™¨ç»“æœ)
        # ptxas = entry.get('ptxas_metrics', {})# é’ˆå¯¹ä¸TODO3çš„ä¿®æ”¹,ä¿®æ”¹å†…å®¹å¦‚ä¸‹ğŸ‘‡
        # metrics_shown = False
        # for k, v in sorted(ptxas.items()):
        #     # åªæ˜¾ç¤ºéé›¶çš„æº¢å‡ºæˆ–å…³é”®å¯„å­˜å™¨ä¿¡æ¯ï¼Œå‡å°‘å™ªéŸ³
        #     if 'spill' in k and v > 0:
        #         summary += f"    ! PTXAS {k}: {v} (SPILL DETECTED)\n"
        #         metrics_shown = True
        #     elif 'registers' in k:
        #         summary += f"    - PTXAS {k}: {v}\n"
        #         metrics_shown = True

        ptxas = entry.get('ptxas_metrics', {})
        if ptxas:
            for kernel_name, stats in sorted(ptxas.items()):
                # é˜²å¾¡æ€§æ£€æŸ¥ï¼šç¡®ä¿ stats æ˜¯å­—å…¸ï¼ˆå…¼å®¹æ—§æ—¥å¿—æ ¼å¼ï¼‰
                if not isinstance(stats, dict):
                    # æ—§æ ¼å¼å›é€€å¤„ç†
                    if 'spill' in kernel_name and stats > 0:
                        summary += f"    ! PTXAS {kernel_name}: {stats} (SPILL!)\n"
                    continue

                # æ–°æ ¼å¼è§£æ
                regs = stats.get('registers', 'N/A')
                spill = stats.get('spill_bytes', 0)
                width = stats.get('width', '')
                
                # æ„å»ºæ˜¾ç¤ºçš„å­—ç¬¦ä¸²
                info_parts = [f"Regs={regs}"]
                if spill > 0:
                    info_parts.append(f"SPILL={spill} bytes")
                if width and width not in ["Scalar", "?", ""]:
                    info_parts.append(f"Vec={width}") # æ˜¾ç¤ºå‘é‡åŒ–å®½åº¦å¯¹ä¼˜åŒ–å¾ˆæœ‰ç”¨
                
                info_str = ", ".join(info_parts)
                
                # å¦‚æœæœ‰æº¢å‡ºï¼Œç”¨æ„Ÿå¹å·å¼€å¤´
                prefix = "!" if spill > 0 else "-"
                warning = " (SPILL DETECTED!)" if spill > 0 else ""
                
                # ç®€åŒ– Kernel åå­—æ˜¾ç¤º (å»é™¤è¿‡é•¿çš„æ¨¡æ¿å‚æ•°ï¼Œä¿ç•™æ ¸å¿ƒ)
                # ä¾‹å¦‚: sigmoid_kernel_vec<float, 4> -> sigmoid_kernel_vec
                short_name = kernel_name.split('<')[0] if '<' in kernel_name else kernel_name
                if width and width.isdigit(): short_name += f"(vec{width})"
                
                summary += f"    {prefix} PTXAS [{short_name}]: {info_str}{warning}\n"
                metrics_shown = True
        
        # 2. NCU (è¿è¡Œæ—¶ç¡¬ä»¶ç»“æœ - é—­ç¯éªŒè¯)
        # è¿™é‡Œå±•ç¤º Tool Agent ä¸“é—¨æŒ‘é€‰æ¥éªŒè¯ Goal çš„æŒ‡æ ‡
        selected_ncu = entry.get('selected_ncu_metrics') # List[str]
        all_ncu = entry.get('all_ncu_metrics') # Dict
        
        if isinstance(selected_ncu, list) and isinstance(all_ncu, dict) and selected_ncu:
            for metric_name in selected_ncu:
                val = all_ncu.get(metric_name, 'N/A')
                # ç®€åŒ–æŒ‡æ ‡åç§°ï¼Œå»æ‰è¿‡é•¿çš„å‰ç¼€ä»¥ä¾¿ LLM é˜…è¯»
                short_name = metric_name.split('.')[-1] if '.' in metric_name else metric_name
                summary += f"    - NCU {short_name}: {val}\n"
            metrics_shown = True
            
        if not metrics_shown:
            summary += "    (No significant metrics available)\n"

        # 3. å¤±è´¥åŸå›  (å¦‚æœæœ‰)
        if "Error" in status or "Failed" in status:
            err = entry.get('details', '')
            summary += f"  > [Result] The reason why the optimized kernel failed: {err[:250]}...\n"
        
        summary += "\n"
        
    return summary

def format_metrics_for_llm(ptxas_metrics: dict, ncu_metrics: dict) -> str:
    if not ncu_metrics:
        return "Hardware metrics are not yet available."
        
    summary = "=== PTXAS Compiler Metrics ===\n"
    summary += json.dumps(ptxas_metrics, indent=2)
    
    # [!!! æ›´æ”¹ !!!] ç›´æ¥ä½¿ç”¨å®Œæ•´çš„ ncu_metrics å­—å…¸ï¼Œå¹¶å°†æ ‡é¢˜æ›´æ”¹ä¸º "Full Set"
    summary += "\n\n=== NCU Hardware Metrics (Full Set) ===\n" 
    summary += json.dumps(ncu_metrics, indent=2)
    
    return summary


# [!!! å·²æ›´æ–° !!!] main() å·²é‡æ„ä¸ºé€šç”¨å‡½æ•°
# def run_optimization_on_problem(
#     problem_name: str,
#     cpp_source: str, 
#     initial_cuda_code: str, 
#     inputs: List[torch.Tensor], 
#     ref_outputs: List[torch.Tensor],
#     kernel_name: str,           # __global__ å‡½æ•°å
#     wrapper_function_name: str, # C++ wrapper å‡½æ•°å
#     iteration_rounds: int,
#     history_file_path: str,
#     baseline_time_ms: float = float('inf') # [!!! å·²ä¿®æ”¹ !!!] æ¥æ”¶PytorchåŸºå‡†
# ):

def final_extract(content):
    # æ–¹æ³•1ï¼šä½¿ç”¨ç‰¹æ®Šæ ‡è®°æå–ï¼ˆæ¨èï¼‰
    def extract_final_code_method1(content):
        start_marker = "### FINAL_CUDA_CODE_START"
        end_marker = "### FINAL_CUDA_CODE_END"
        
        start_idx = content.find(start_marker)
        if start_idx == -1:
            return None
            
        # æ‰¾åˆ°èµ·å§‹æ ‡è®°åçš„ç¬¬ä¸€ä¸ª```python
        start_idx = content.find("```python", start_idx)
        if start_idx == -1:
            start_idx = content.find("```", start_idx)
            if start_idx == -1:
                return None
            start_idx += 3
        else:
            start_idx += 9  # len("```cpp")
        
        # æ‰¾åˆ°ç»“æŸæ ‡è®°å‰çš„æœ€åä¸€ä¸ª```
        end_idx = content.find(end_marker)
        if end_idx == -1:
            return None
            
        # åœ¨ç»“æŸæ ‡è®°å‰æŸ¥æ‰¾```
        temp_content = content[start_idx:end_idx]
        code_end = temp_content.rfind("```")
        if code_end == -1:
            code_end = temp_content.rfind('"""')
        if code_end == -1:
            return None
            
        return temp_content[:code_end].strip()
    
    extracted_code = extract_final_code_method1(content)
    if extracted_code:
        return extracted_code
    else:
        print("Warning: Could not extract code properly. Returning full response for debugging.")
        return content
        
def run_optimization_on_problem(
    problem_name,
    initial_cuda_code,
    inputs, 
    init_inputs,
    ref_outputs,
    # kernel_name: str,           # __global__ å‡½æ•°å
    # wrapper_function_name: str, # C++ wrapper å‡½æ•°å
    iteration_rounds,
    history_file_path,
    baseline_time_ms, # [!!! å·²ä¿®æ”¹ !!!] æ¥æ”¶PytorchåŸºå‡†
    full_pytorch_source_code,
    pytorch_kernel_module
):
    """
    è¿è¡Œé€šç”¨çš„ã€çº¿æ€§çš„å¤šæ™ºèƒ½ä½“ä¼˜åŒ–å¾ªç¯ã€‚
    """
    
    print(f"Starting optimization for problem: {problem_name}")
    if not torch.cuda.is_available():
        print("âŒ é”™è¯¯ï¼šæœªæ£€æµ‹åˆ° CUDAã€‚æ— æ³•è¿›è¡Œæœ¬åœ°æµ‹è¯•ã€‚")
        return {"error": "No CUDA detected"}
        
    print(f"Running on device: {config.DEVICE}")
    print(f"Total iteration rounds: {iteration_rounds}")
    if config.MOCK_LLM_CALLS:
        print("--- è­¦å‘Š: MOCK LLM CALLS ARE ENABLED (in config.py) ---")
    
    # 1. åˆå§‹åŒ–
    device = torch.device(config.DEVICE)
    
    best_kernel_code_cuda = initial_cuda_code  
    best_time_ms = float('inf')
    best_ptxas_metrics = {}
    best_ncu_metrics = {}
    current_ncu_metrics = {}
    
    optimization_history = []
    
    if os.path.exists(history_file_path): 
        print(f"Loading existing history from {history_file_path}")
        try:
            with open(history_file_path, 'r', encoding='utf-8') as f:  
                optimization_history = json.load(f)
            
            found_best = False
            for entry in sorted(optimization_history, key=lambda x: x.get('time_ms') if x.get('time_ms') is not None else float('inf')):
                 if ("Success" in entry['status']) and entry.get('code'):
                    best_time_ms = entry['time_ms']
                    best_ptxas_metrics = entry['ptxas_metrics']
                    best_kernel_code_cuda = entry['code'] # <--- ä»å†å²ä¸­æ¢å¤ä»£ç 
                    best_ncu_metrics = entry.get('all_ncu_metrics', {}) 
                    
                    print(f"Restored best kernel from history (Round {entry['round']}, Time: {best_time_ms:.3f} ms)")
                    found_best = True
                    break
            if not found_best:
                 print("No successful kernel found in history, starting from baseline.")
                 optimization_history = [] 
        except json.JSONDecodeError:
            print(f"Warning: Corrupt history file {history_file_path}. Starting from scratch.")
            optimization_history = []
             
    # 2. è·å–åŸºçº¿æ€§èƒ½ (Round 0)
    if not optimization_history: 
        print("\n--- Round 0: Compiling and analyzing baseline (naive) kernel ---")
        current_module_name = f"{problem_name}_0"  
        try:
            module, stdout_log, stderr_log = cuda_utils.load_module(
                # cpp_source, 
                best_kernel_code_cuda, 
                current_module_name,
                init_inputs,
                pytorch_kernel_module
                # wrapper_function_name=wrapper_function_name  
            )
            print("Baseline kernel compiled successfully.")
            best_ptxas_metrics = cuda_utils.parse_ptxas_info(stdout_log)
            
            # [!!! å·²æ›´æ–° !!!]
            is_correct = cuda_utils.check_correctness(inputs, ref_outputs, module)
            if not is_correct:
                print("âŒ Baseline kernel is INCORRECT. Exiting.")
                return {"error": "Baseline kernel incorrect."}  
                
            print("Baseline kernel is correct. Benchmarking...")
            # [!!! å·²æ›´æ–° !!!]
            best_time_ms = cuda_utils.benchmark_kernel(inputs, module)
            
            print("Analyzing baseline kernel with NCU (this may take a while)...")
            # [!!! å·²æ›´æ–° !!!]
            best_ncu_metrics = cuda_utils.get_real_ncu_metrics(
                module.__file__, 
                current_module_name, 
                # kernel_name,            
                # wrapper_function_name,  
                inputs                  
            )
            current_ncu_metrics = best_ncu_metrics # liuxitai:åˆ°è¿™é‡Œç›®å‰å·²ç»æ”¹å¥½äº†
            
            history_entry = {
                "round": 0, "goal": "Baseline", "status": "Success",
                "time_ms": best_time_ms, 
                "ptxas_metrics": best_ptxas_metrics,
                "all_ncu_metrics": best_ncu_metrics,
                "selected_ncu_metrics": [],
                "details": "Initial baseline measurement",
                "code": best_kernel_code_cuda 
            }
            optimization_history.append(history_entry)
            print(f"Baseline performance: {best_time_ms:.3f} ms")

            try:
                # 1. ä¿å­˜ History
                with open(history_file_path, 'w', encoding='utf-8') as f:
                    json.dump(optimization_history, f, indent=2)

                # 2. ä¿å­˜ Best Kernel
                # æ—¢ç„¶ Round 0 æˆåŠŸäº†ä¸”æ˜¯å½“å‰å”¯ä¸€çš„å†…æ ¸ï¼Œå®ƒå°±æ˜¯ Best Kernel
                best_kernel_path = history_file_path.replace(
                    "_optimization_history.json", 
                    "_best_kernel.cu"
                )
                with open(best_kernel_path, "w", encoding='utf-8') as f:
                    f.write(best_kernel_code_cuda)
                print(f"âœ… Real-time save (Round 0): Initial kernel saved to {best_kernel_path}")

                # 3. ä¿å­˜ Best Stats
                # å¦‚æœä¼ å…¥äº† baseline_time_msï¼Œåˆ™è®¡ç®—åŠ é€Ÿæ¯”å¹¶ä¿å­˜
                if baseline_time_ms != float('inf') and best_time_ms > 0:
                    speedup = baseline_time_ms / best_time_ms
                    stats = {
                        "problem_name": problem_name,
                        "baseline_ms": baseline_time_ms,
                        "best_cuda_ms": best_time_ms,
                        "speedup": speedup,
                        "last_updated_round": 0
                    }
                    stats_file_path = history_file_path.replace(
                        "_optimization_history.json",
                        "_best_stats.json"
                    )
                    with open(stats_file_path, "w", encoding='utf-8') as f:
                        json.dump(stats, f, indent=2)
                    print(f"âœ… Real-time save (Round 0): Initial stats saved to {stats_file_path} (Speedup: {speedup:.2f}x)")
            
            except Exception as e:
                print(f"[Warning] Round 0: Failed to real-time save initial results: {e}")

        except Exception as e:
            print(f"âŒ Baseline kernel failed compilation or runtime. Exiting. \n{e}")
            return {"error": f"Baseline failed: {e}"}  
        
        finally:
            if module is not None:
                del module
    
    if not current_ncu_metrics: 
        current_ncu_metrics = best_ncu_metrics if best_ncu_metrics else {}
    # if module is not None:
    #     del module
    torch.cuda.empty_cache()


    # 3. å¼€å§‹ä¼˜åŒ–å¾ªç¯
    # [!!! å·²æ›´æ–° !!!]
    for i in tqdm(range(len(optimization_history), iteration_rounds + 1), desc="Optimization Rounds"):
        if i == 0: continue # Round 0 å·²ç»å®Œæˆ
        
        print(f"\n--- Round {i}/{iteration_rounds} ---")
        
        history_summary = summarize_history(optimization_history)# å¦‚æœæ˜¯ä¼˜åŒ–ä¹‹å‰çš„é¦–è½®ï¼Œè¿™é‡Œé¢åªæœ‰ptxas_metricsï¼Œæ²¡æœ‰ncuä¿¡æ¯ï¼Œå…¨éƒ¨çš„ncuä¿¡æ¯ä¹Ÿæ²¡æœ‰(æ˜¯æ‰€æœ‰å†å²ä¿¡æ¯)
        metrics_summary = format_metrics_for_llm(best_ptxas_metrics, best_ncu_metrics)# æ˜¯å½“å‰æœ€å¥½çš„ptxaxä¿¡æ¯å’Œncuä¿¡æ¯ï¼Œæ˜¯å…¨éƒ¨
        
        print("------------------LXT:metrics_summary (to Planner)----------------------")
        print(metrics_summary)
        print("------------------LXT:metrics_summary (to Planner)----------------------")
        
        opt_goal = "N/A"
        bottleneck_analysis = "N/A" 
        detailed_plan = "N/A"
        new_kernel_code_full = None # [!!! å·²æ›´æ–° !!!]
        new_kernel_code_cuda_only = None # [!!! å·²æ›´æ–° !!!]
        status = "Failed (Unknown)"
        details = ""
        new_time_ms = float('inf')
        new_ptxas_metrics = {}
        new_ncu_metrics = {}
        relevant_metric_names = [] 
        
        try:
            # 1. Planner Agent
            print("[Planner Agent] Analyzing hardware metrics and history...")
            
            # [!!! å·²æ›´æ–° !!!] åˆå¹¶ C++ å’Œ CUDA ä»¥è·å–å®Œæ•´ä¸Šä¸‹æ–‡
            parent_kernel_code = best_kernel_code_cuda
            
            planner_response = agents.call_llm(
                "planner", 
                prompts.PLANNER_SYSTEM_PROMPT,
                f"Optimization History:\n{history_summary}\n\n"#DONE 1 è¿™ä¸ªæç¤ºè¯ä¿¡æ¯éœ€è¦é‡æ–°è®¾è®¡ä¸€ä¸‹ï¼Œhistory_summaryå…‰æœ‰è¿™äº›ä¿¡æ¯æ²¡æœ‰ç”¨å•Šï¼Œä¸çŸ¥é“æ¯ä¸ªæŒ‡æ ‡çš„å¯¹åº”çš„ä»£ç æ˜¯ä»€ä¹ˆå•Šï¼Œæ¯ä¸ªè®°å½•çš„ä»£ç æ˜¯åœ¨å“ªä¸ªç‰ˆæœ¬ä¸Šåšçš„ä¿®æ”¹å•Šï¼Œè¿™äº›éƒ½ä¸çŸ¥é“
                f"=== Hardware Metrics for Current Best Kernel(Need to be optimized) ===\n{metrics_summary}\n\n"
                f"Current Best C++/CUDA Source (Time: {best_time_ms:.3f} ms):\n{parent_kernel_code}"  
            )
            if not planner_response or "OPTIMIZATION_GOAL:" not in planner_response:
                status, details = "Failed (Planner)", "Planner did not return a valid goal."
                print(f"âŒ {status} {details}")
                continue 
            
            if "BOTTLENECK_ANALYSIS:" in planner_response:
                 bottleneck_analysis = planner_response.split("BOTTLENECK_ANALYSIS:")[1].split("OPTIMIZATION_GOAL:")[0].strip()
                 print(f"[Planner Agent] Bottleneck identified: {bottleneck_analysis}")
            else:
                status, details = "Failed (Planner)", "Planner did not output BOTTLENECK_ANALYSIS."
                print(f"âŒ {status} {details}")
                continue
                 
            opt_goal = planner_response.split("OPTIMIZATION_GOAL:")[1].strip()
            print(f"[Planner Agent] Goal: {opt_goal}")
            print("-----------------------LXT:planner_response----------------------")
            print(planner_response)
            print("-----------------------LXT:planner_response----------------------")
            
            # 2. Tool Agent
            print("[Tool Agent] Selecting metrics...")
            all_metric_names = list(current_ncu_metrics.keys())
            print("-----------------------LXT:all_metric_names----------------------")
            print(all_metric_names)
            print("-----------------------LXT:all_metric_names----------------------")
            if not all_metric_names:
                all_metric_names = config.BASE_NCU_METRICS_LIST_EXAMPLE
                
            # tool_response = agents.call_llm(# DONEğŸ‘‡:è¿™ä¸ªæç¤ºè¯ä¹Ÿè¦é‡æ–°è®¾è®¡ä¸€ä¸‹ï¼Œåªæœ‰27ä¸ªæŒ‡æ ‡å’Œä¼˜åŒ–ç›®æ ‡ï¼Œè®©LLMä»ä¸­é€‰5ä¸ªï¼Œå¯æ˜¯ä¸çŸ¥é“ç°åœ¨è¦ä¼˜åŒ–çš„ä»»åŠ¡ä»£ç æ˜¯ä»€ä¹ˆå•Šï¼Ÿ
            #     "tool", 
            #     prompts.TOOL_SYSTEM_PROMPT,
            #     f"All Available NCU Metric Names ({len(all_metric_names)}): {all_metric_names}\n\nOptimization Goal: {opt_goal}"
            # )
            tool_response = agents.call_llm(
                "tool", 
                prompts.TOOL_SYSTEM_PROMPT,
                f"Optimization Goal: {opt_goal}\n\n"
                f"Planner's Bottleneck Analysis: {bottleneck_analysis}\n\n"
                f"Current C++/CUDA Source:\n{parent_kernel_code}\n\n" 
                f"All Available NCU Metric Names ({len(all_metric_names)}): {all_metric_names}"
            )

            print("-----------------------LXT:tool_response----------------------")
            print(tool_response)
            print("-----------------------LXT:tool_response----------------------")
            
            relevant_metric_names = extract_metrics(tool_response)# å°†äº”ä¸ªæŒ‡æ ‡çš„åç§°ä»å›å¤ä¸­æå–å‡ºæ¥ã€‚
            
            if not relevant_metric_names:
                status, details = "Failed (Tool)", "Tool Agent did not return a valid metric list."
                print(f"âŒ {status} {details}")
                continue 
            print(f"[Tool Agent] Selected {len(relevant_metric_names)} metrics: {relevant_metric_names}")
            
            relevant_metrics_dict = {
                metric: current_ncu_metrics.get(metric, 0.0) 
                for metric in relevant_metric_names
            }# å°†é€‰æ‹©çš„äº”ä¸ªæŒ‡æ ‡æå–å‡ºæ¥
            
            diverse_kernels_str = get_diverse_champions(optimization_history, best_kernel_code_cuda)# ä»å†å²ä¿¡æ¯ä¸­æ‰¾åˆ°å’Œå½“å‰çš„æœ€å¥½ç‰ˆæœ¬best_kernel_code_cudaä¸æ˜¯å¾ˆç›¸ä¼¼çš„æœ€å¥½çš„ä¸¤ä¸ªä»£ç å’Œç›¸å…³æŒ‡æ ‡ã€‚
            
            # 3. Analysis Agent [!!! å·²æ›´æ–° !!!]
            print("[Analysis Agent] Formulating plan...")
            analysis_response = agents.call_llm(
                "analysis", 
                prompts.ANALYSIS_SYSTEM_PROMPT,
                f"Planner's Bottleneck Analysis: {bottleneck_analysis}\n\n"
                f"Optimization Goal: {opt_goal}\n\n"
                f"Optimization History:\n{history_summary}\n\n"# DONEï¼šå’Œplannerçš„åŒç†ï¼Œè¿™ä¸ªhistory_summaryçš„ä¿¡æ¯æ˜¯å¦è¶³å¤Ÿæœ‰ç”¨ï¼Ÿ
                f"Diverse Successful Kernel Examples:\n{diverse_kernels_str}\n\n"#DONE 2 è¿™é‡Œçš„ä¿¡æ¯æ˜¯ä¸æ˜¯åº”è¯¥å¥½å¥½ç»„ç»‡ä¸€ä¸‹ï¼Œä¸ç„¶LLMåˆ†ä¸æ¸…è¿™äº›æŒ‡æ ‡å’Œä¼˜åŒ–ç›®æ ‡æ˜¯å½“å‰ä»£ç çš„è¿˜æ˜¯å½“å‰ä»£ç çš„çˆ¶èŠ‚ç‚¹çš„
                f"Current C++/CUDA Source need to be optimized:\n{parent_kernel_code}\n\n" # parent_kernel_codeå°±æ˜¯å½“å‰æœ€å¥½çš„kernel,ä¹Ÿå°±æ˜¯æ­£åœ¨æ”¹çš„ç‰ˆæœ¬ï¼Œæ˜¯å½“å‰è¿™ä¸ªï¼æ¯æ¬¡æ”¹çš„éƒ½æ˜¯æœ€å¥½çš„é‚£ä¸ª
                f"Current Hardware Metrics (Full Set): {metrics_summary}\n\n"# æ˜¯å½“å‰kernelçš„å…¨éƒ¨PTXASä¿¡æ¯å’ŒNCUä¿¡æ¯
                f"Tool-Selected Metrics from *Previous* Run (Values): {relevant_metrics_dict}" # æ˜¯å½“å‰kernelçš„é€‰æ‹©å‡ºæ¥çš„äº”ä¸ªç›¸å…³NCUæŒ‡æ ‡ã€‚
            )
            print("-----------------------LXT:analysis_response----------------------")
            print(analysis_response)#DONE:å½“å‰çš„è¾“å‡ºéƒ¨åˆ†æ²¡æœ‰thinkçš„è¿‡ç¨‹
            print("-----------------------LXT:analysis_response----------------------")
            if not analysis_response or "DETAILED_PLAN:" not in analysis_response:
                status, details = "Failed (Analysis)", "Analysis Agent did not return a valid plan."
                print(f"âŒ {status} {details}")
                continue 
            detailed_plan = analysis_response.split("DETAILED_PLAN:")[1].strip()

            # 4. Coder Agent
            print("[Coder Agent] Generating new kernel...")
            coder_response = agents.call_llm(# DONE:coder agentæ²¡æœ‰æ€è€ƒè¿‡ç¨‹
                "coder", 
                prompts.CODER_SYSTEM_PROMPT,
                f"Original C++/CUDA Source:\n{parent_kernel_code}\n\nDetailed Plan:\n{detailed_plan}" 
            )
            print("-----------------------LXT:coder_response----------------------")
            print(coder_response)
            print("-----------------------LXT:coder_response----------------------")
            
            new_kernel_code_full = final_extract(coder_response) 
            #å¥½
            # new_kernel_code_full = '''import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\n# ---------------------------------------------------------------------------\n# CUDA source (kernels + C++/ATen host wrappers)\n# ---------------------------------------------------------------------------\nsource = r\'\'\'\n#include <torch/extension.h>\n#include <ATen/cuda/CUDAContext.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t sigmoid_func(scalar_t x) {\n    return scalar_t(1) / (scalar_t(1) + exp(-x));\n}\n\n/* ---------------------------------------------------------\n * Scalar fallback kernel : one-element per thread\n * ------------------------------------------------------- */\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel_scalar(const scalar_t* __restrict__ input,\n                                      scalar_t* __restrict__ output,\n                                      const int64_t numel) {\n    const int64_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < numel) {\n        output[idx] = sigmoid_func(input[idx]);\n    }\n}\n\n/* ---------------------------------------------------------\n * Vectorised kernel : VEC elements per thread\n * VEC = 4 for float (float4, 16-byte transaction)\n *     = 2 for double (double2, 16-byte transaction)\n * The last (numel % VEC) elements are processed by a\n * single thread (vec_idx == 0) inside the same kernel.\n * ------------------------------------------------------- */\ntemplate <typename scalar_t , int VEC>\n__global__ void sigmoid_kernel_vec(const scalar_t* __restrict__ input,\n                                   scalar_t*       __restrict__ output,\n                                   const int64_t   vec_elems,\n                                   const int64_t   tail_start,\n                                   const int64_t   tail_size) {\n    using VecT = typename std::conditional< (sizeof(scalar_t)==4),\n                                            float4,              // 4 x fp32 = 16 B\n                                            double2               // 2 x fp64 = 16 B\n                                          >::type;\n\n    const int64_t vec_idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    /* ---------------- Aligned, vectorised path ---------------- */\n    if (vec_idx < vec_elems) {\n        VecT v = reinterpret_cast<const VecT*>(input)[vec_idx];\n\n        scalar_t* v_elem = reinterpret_cast<scalar_t*>(&v);\n        #pragma unroll\n        for (int i = 0; i < VEC; ++i) {\n            v_elem[i] = sigmoid_func(v_elem[i]);\n        }\n\n        reinterpret_cast<VecT*>(output)[vec_idx] = v;\n    }\n\n    /* ---------------- Tail handling by one thread ------------- */\n    if (tail_size && vec_idx == 0) {\n        for (int64_t j = 0; j < tail_size; ++j) {\n            const int64_t idx = tail_start + j;\n            output[idx] = sigmoid_func(input[idx]);\n        }\n    }\n}\n\n/* ---------------------------------------------------------\n * Host launcher\n * ------------------------------------------------------- */\ntorch::Tensor sigmoid_forward(torch::Tensor input) {\n    TORCH_CHECK(input.is_cuda(), "Input must reside on CUDA device");\n    TORCH_CHECK(input.is_contiguous(), "Input must be contiguous");\n\n    auto output = torch::empty_like(input);\n    const int64_t numel = input.numel();\n    const int threads = 256;\n    auto stream = at::cuda::getCurrentCUDAStream();\n\n    // Fast path : fp32 / fp64 with vectorised kernel\n    if (input.scalar_type() == at::kFloat || input.scalar_type() == at::kDouble) {\n\n        if (input.scalar_type() == at::kFloat) {\n            using scalar_t = float;\n            constexpr int  VEC = 4;\n            const int64_t  vec_elems  = numel / VEC;\n            const int64_t  tail_start = vec_elems * VEC;\n            const int64_t  tail_sz    = numel - tail_start;\n            const int64_t  blocks     = (vec_elems + threads - 1) / threads;\n\n            if (blocks > 0) {\n                sigmoid_kernel_vec<scalar_t, VEC><<<blocks, threads, 0, stream>>>(\n                    input.data_ptr<scalar_t>(),\n                    output.data_ptr<scalar_t>(),\n                    vec_elems,\n                    tail_start,\n                    tail_sz);\n            } else if (tail_sz) {\n                // Fallback to scalar kernel if vector part is empty\n                const int64_t blocks_tail = (tail_sz + threads - 1) / threads;\n                sigmoid_kernel_scalar<scalar_t><<<blocks_tail, threads, 0, stream>>>(\n                    input.data_ptr<scalar_t>() + tail_start,\n                    output.data_ptr<scalar_t>() + tail_start,\n                    tail_sz);\n            }\n        } else { // double\n            using scalar_t = double;\n            constexpr int  VEC = 2;\n            const int64_t  vec_elems  = numel / VEC;\n            const int64_t  tail_start = vec_elems * VEC;\n            const int64_t  tail_sz    = numel - tail_start;\n            const int64_t  blocks     = (vec_elems + threads - 1) / threads;\n\n            if (blocks > 0) {\n                sigmoid_kernel_vec<scalar_t, VEC><<<blocks, threads, 0, stream>>>(\n                    input.data_ptr<scalar_t>(),\n                    output.data_ptr<scalar_t>(),\n                    vec_elems,\n                    tail_start,\n                    tail_sz);\n            } else if (tail_sz) {\n                const int64_t blocks_tail = (tail_sz + threads - 1) / threads;\n                sigmoid_kernel_scalar<scalar_t><<<blocks_tail, threads, 0, stream>>>(\n                    input.data_ptr<scalar_t>() + tail_start,\n                    output.data_ptr<scalar_t>() + tail_start,\n                    tail_sz);\n            }\n        }\n\n    } else {\n        /* Generic scalar kernel for remaining dtypes (half, bfloat16, etc.) */\n        const int64_t blocks = (numel + threads - 1) / threads;\n        AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.scalar_type(),\n                                            "sigmoid_forward_cuda_scalar", ([&] {\n            sigmoid_kernel_scalar<scalar_t><<<blocks, threads, 0, stream>>>(\n                input.data_ptr<scalar_t>(),\n                output.data_ptr<scalar_t>(),\n                numel);\n        }));\n    }\n\n    cudaError_t err = cudaGetLastError();\n    TORCH_CHECK(err == cudaSuccess, "sigmoid_kernel launch failed with error code ", err);\n\n    return output;\n}\n\'\'\'\n\n# ---------------------------------------------------------------------------\n# C++ function prototypes\n# ---------------------------------------------------------------------------\ncpp_src = r\'\'\'\ntorch::Tensor sigmoid_forward(torch::Tensor input);\n\'\'\'\n\n# ---------------------------------------------------------------------------\n# Build & load extension\n# ---------------------------------------------------------------------------\nsigmoid_module = load_inline(\n    name         = \'sigmoid_cuda_opt\',\n    cpp_sources  = cpp_src,\n    cuda_sources = source,\n    functions    = [\'sigmoid_forward\'],\n    with_cuda    = True,\n    verbose      = True,\n    extra_cuda_cflags=[\'-O3\', \'--ptxas-options=-v\']\n)\n\n# ---------------------------------------------------------------------------\n# PyTorch Module wrapper\n# ---------------------------------------------------------------------------\nclass ModelNew(nn.Module):\n    """\n    CUDA-accelerated model that applies element-wise Sigmoid.\n    Mirrors the original Model interface.\n    """\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        self.sigmoid = sigmoid_module\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.sigmoid.sigmoid_forward(x)'''
            # è¿™ä¸ªæ˜¯æœ‰ç»“æœé—®é¢˜çš„å¥½ç”¨ä¾‹
            # new_kernel_code_full = '''import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\n# ---------------------------------------------------------------------------\n# CUDA source (kernels + C++/ATen host wrappers)\n# ---------------------------------------------------------------------------\nsource = r\'\'\'\n#include <torch/extension.h>\n#include <ATen/cuda/CUDAContext.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t sigmoid_func(scalar_t x) {\n    return scalar_t(1) / (scalar_t(1) + exp(x));\n}\n\n/* ---------------------------------------------------------\n * Scalar fallback kernel : one-element per thread\n * ------------------------------------------------------- */\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel_scalar(const scalar_t* __restrict__ input,\n                                      scalar_t* __restrict__ output,\n                                      const int64_t numel) {\n    const int64_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < numel) {\n        output[idx] = sigmoid_func(input[idx]);\n    }\n}\n\n/* ---------------------------------------------------------\n * Vectorised kernel : VEC elements per thread\n * VEC = 4 for float (float4, 16-byte transaction)\n *     = 2 for double (double2, 16-byte transaction)\n * The last (numel % VEC) elements are processed by a\n * single thread (vec_idx == 0) inside the same kernel.\n * ------------------------------------------------------- */\ntemplate <typename scalar_t , int VEC>\n__global__ void sigmoid_kernel_vec(const scalar_t* __restrict__ input,\n                                   scalar_t*       __restrict__ output,\n                                   const int64_t   vec_elems,\n                                   const int64_t   tail_start,\n                                   const int64_t   tail_size) {\n    using VecT = typename std::conditional< (sizeof(scalar_t)==4),\n                                            float4,              // 4 x fp32 = 16 B\n                                            double2               // 2 x fp64 = 16 B\n                                          >::type;\n\n    const int64_t vec_idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    /* ---------------- Aligned, vectorised path ---------------- */\n    if (vec_idx < vec_elems) {\n        VecT v = reinterpret_cast<const VecT*>(input)[vec_idx];\n\n        scalar_t* v_elem = reinterpret_cast<scalar_t*>(&v);\n        #pragma unroll\n        for (int i = 0; i < VEC; ++i) {\n            v_elem[i] = sigmoid_func(v_elem[i]);\n        }\n\n        reinterpret_cast<VecT*>(output)[vec_idx] = v;\n    }\n\n    /* ---------------- Tail handling by one thread ------------- */\n    if (tail_size && vec_idx == 0) {\n        for (int64_t j = 0; j < tail_size; ++j) {\n            const int64_t idx = tail_start + j;\n            output[idx] = sigmoid_func(input[idx]);\n        }\n    }\n}\n\n/* ---------------------------------------------------------\n * Host launcher\n * ------------------------------------------------------- */\ntorch::Tensor sigmoid_forward(torch::Tensor input) {\n    TORCH_CHECK(input.is_cuda(), "Input must reside on CUDA device");\n    TORCH_CHECK(input.is_contiguous(), "Input must be contiguous");\n\n    auto output = torch::empty_like(input);\n    const int64_t numel = input.numel();\n    const int threads = 256;\n    auto stream = at::cuda::getCurrentCUDAStream();\n\n    // Fast path : fp32 / fp64 with vectorised kernel\n    if (input.scalar_type() == at::kFloat || input.scalar_type() == at::kDouble) {\n\n        if (input.scalar_type() == at::kFloat) {\n            using scalar_t = float;\n            constexpr int  VEC = 4;\n            const int64_t  vec_elems  = numel / VEC;\n            const int64_t  tail_start = vec_elems * VEC;\n            const int64_t  tail_sz    = numel - tail_start;\n            const int64_t  blocks     = (vec_elems + threads - 1) / threads;\n\n            if (blocks > 0) {\n                sigmoid_kernel_vec<scalar_t, VEC><<<blocks, threads, 0, stream>>>(\n                    input.data_ptr<scalar_t>(),\n                    output.data_ptr<scalar_t>(),\n                    vec_elems,\n                    tail_start,\n                    tail_sz);\n            } else if (tail_sz) {\n                // Fallback to scalar kernel if vector part is empty\n                const int64_t blocks_tail = (tail_sz + threads - 1) / threads;\n                sigmoid_kernel_scalar<scalar_t><<<blocks_tail, threads, 0, stream>>>(\n                    input.data_ptr<scalar_t>() + tail_start,\n                    output.data_ptr<scalar_t>() + tail_start,\n                    tail_sz);\n            }\n        } else { // double\n            using scalar_t = double;\n            constexpr int  VEC = 2;\n            const int64_t  vec_elems  = numel / VEC;\n            const int64_t  tail_start = vec_elems * VEC;\n            const int64_t  tail_sz    = numel - tail_start;\n            const int64_t  blocks     = (vec_elems + threads - 1) / threads;\n\n            if (blocks > 0) {\n                sigmoid_kernel_vec<scalar_t, VEC><<<blocks, threads, 0, stream>>>(\n                    input.data_ptr<scalar_t>(),\n                    output.data_ptr<scalar_t>(),\n                    vec_elems,\n                    tail_start,\n                    tail_sz);\n            } else if (tail_sz) {\n                const int64_t blocks_tail = (tail_sz + threads - 1) / threads;\n                sigmoid_kernel_scalar<scalar_t><<<blocks_tail, threads, 0, stream>>>(\n                    input.data_ptr<scalar_t>() + tail_start,\n                    output.data_ptr<scalar_t>() + tail_start,\n                    tail_sz);\n            }\n        }\n\n    } else {\n        /* Generic scalar kernel for remaining dtypes (half, bfloat16, etc.) */\n        const int64_t blocks = (numel + threads - 1) / threads;\n        AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.scalar_type(),\n                                            "sigmoid_forward_cuda_scalar", ([&] {\n            sigmoid_kernel_scalar<scalar_t><<<blocks, threads, 0, stream>>>(\n                input.data_ptr<scalar_t>(),\n                output.data_ptr<scalar_t>(),\n                numel);\n        }));\n    }\n\n    cudaError_t err = cudaGetLastError();\n    TORCH_CHECK(err == cudaSuccess, "sigmoid_kernel launch failed with error code ", err);\n\n    return output;\n}\n\'\'\'\n\n# ---------------------------------------------------------------------------\n# C++ function prototypes\n# ---------------------------------------------------------------------------\ncpp_src = r\'\'\'\ntorch::Tensor sigmoid_forward(torch::Tensor input);\n\'\'\'\n\n# ---------------------------------------------------------------------------\n# Build & load extension\n# ---------------------------------------------------------------------------\nsigmoid_module = load_inline(\n    name         = \'sigmoid_cuda_opt\',\n    cpp_sources  = cpp_src,\n    cuda_sources = source,\n    functions    = [\'sigmoid_forward\'],\n    with_cuda    = True,\n    verbose      = True,\n    extra_cuda_cflags=[\'-O3\', \'--ptxas-options=-v\']\n)\n\n# ---------------------------------------------------------------------------\n# PyTorch Module wrapper\n# ---------------------------------------------------------------------------\nclass ModelNew(nn.Module):\n    """\n    CUDA-accelerated model that applies element-wise Sigmoid.\n    Mirrors the original Model interface.\n    """\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        self.sigmoid = sigmoid_module\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.sigmoid.sigmoid_forward(x)'''
            # è¿™ä¸ªæ˜¯ä¼šå¯¹è¾“å…¥æ•°æ®è¿›è¡Œä¿®æ”¹çš„ç”¨ä¾‹
            # new_kernel_code_full = """import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\nsource = r'''\n#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <stdint.h>\n\n// ---------------------------------------------------------------------\n// Fast sigmoid for FP32 (uses CUDA fast-math intrinsic __expf)\n// ---------------------------------------------------------------------\n__device__ __forceinline__ float sigmoidf(float v)\n{\n    return 1.f / (1.f + __expf(-v));\n}\n\n// ---------------------------------------------------------------------\n// Scalar in-place kernel (generic, used for FP64 fallback)\n// ---------------------------------------------------------------------\ntemplate<typename scalar_t>\n__global__ __launch_bounds__(256, 2)\nvoid sigmoid_kernel_scalar_ip(scalar_t* __restrict__ data,\n                              const int64_t          numel)\n{\n    const int idx    = blockDim.x * blockIdx.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n\n    for (int64_t i = idx; i < numel; i += stride)\n    {\n        scalar_t v = data[i];\n        data[i] = scalar_t(1.0) / (scalar_t(1.0) + exp(-v));\n    }\n}\n\n// ---------------------------------------------------------------------\n// Vectorised FP32 kernel \u2013 processes 4 elements at a time, in-place\n// ---------------------------------------------------------------------\n__global__ __launch_bounds__(256, 2)\nvoid sigmoid_kernel_vec4_ip(float*       __restrict__ data,\n                            const int64_t            numel)\n{\n    const int idx    = blockDim.x * blockIdx.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n\n    // Number of float4 elements\n    const int64_t vec_elems = numel >> 2;  // numel / 4\n\n    // Reinterpret data pointer as float4*\n    float4* __restrict__ d4 = reinterpret_cast<float4*>(data);\n\n    // ---- main vectorised loop ----\n    for (int64_t i = idx; i < vec_elems; i += stride)\n    {\n        float4 v = d4[i];   // 128-bit load\n        v.x = sigmoidf(v.x);\n        v.y = sigmoidf(v.y);\n        v.z = sigmoidf(v.z);\n        v.w = sigmoidf(v.w);\n        d4[i] = v;          // 128-bit store (same location)\n    }\n\n    // ---- scalar tail (handles numel % 4) ----\n    for (int64_t i = (vec_elems << 2) + idx; i < numel; i += stride)\n    {\n        data[i] = sigmoidf(data[i]);\n    }\n}\n\n// ---------------------------------------------------------------------\n// Host wrapper \u2013 launches in-place kernels\n// ---------------------------------------------------------------------\ntorch::Tensor sigmoid_cuda_inplace(torch::Tensor x)\n{\n    TORCH_CHECK(x.is_cuda(),  \"Input must reside on CUDA device\");\n    TORCH_CHECK(x.dtype() == torch::kFloat32 || x.dtype() == torch::kFloat64,\n                \"Supported dtypes are float32 and float64\");\n\n    // Ensure contiguous layout; otherwise make a contiguous copy\n    auto x_contig = x.contiguous();\n    const int64_t numel = x_contig.numel();\n\n    if (x_contig.scalar_type() == torch::kFloat32)\n    {\n        const int threads = 256;\n        // Each thread handles 4 elements via float4\n        const int blocks  = (((numel + 3) >> 2) + threads - 1) / threads;\n\n        sigmoid_kernel_vec4_ip<<<blocks, threads>>>(\n            x_contig.data_ptr<float>(),\n            numel);\n    }\n    else    // FP64 path\n    {\n        const int threads = 256;\n        const int blocks  = (numel + threads - 1) / threads;\n\n        sigmoid_kernel_scalar_ip<double><<<blocks, threads>>>(\n            x_contig.data_ptr<double>(),\n            numel);\n    }\n\n    // Check for launch / runtime errors\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess)\n        throw std::runtime_error(std::string(\"sigmoid_cuda_inplace failed: \")\n                                 + cudaGetErrorString(err));\n\n    return x_contig;\n}\n'''\n\ncpp_src = r'''\ntorch::Tensor sigmoid_cuda_inplace(torch::Tensor x);\n'''\n\nsigmoid_mod = load_inline(\n    name         = 'sigmoid_mod_ip',\n    cpp_sources  = cpp_src,\n    cuda_sources = source,\n    functions    = ['sigmoid_cuda_inplace'],\n    with_cuda    = True,\n    verbose      = True,\n    extra_cuda_cflags=['-O3', '--use_fast_math', '--ptxas-options=-v'],\n)\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Drop-in replacement using an in-place, vectorised CUDA sigmoid.\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.sigmoid_cuda = sigmoid_mod.sigmoid_cuda_inplace\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.sigmoid_cuda(x)"""

            #å
            # new_kernel_code_full = '''import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\nsource = r\'\'\'\n#include <torch/extension.h>\n#include <ATen/cuda/CUDAContext.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\ntemplate <typename scalar_t>\n__device__ __forceinline__ scalar_t sigmoid_func(scalar_t x) {\n    return scalar_t(1) / (scalar_t(1) + exp(-x));\n}\n\n// Kernel: element-wise Sigmoid\ntemplate <typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ input,\n                               scalar_t* __restrict__ output,\n                               const int64_t numel) {\n    const int64_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < numel) {\n        scalar_t val = input[idx];\n        output[idx] = sigmoid_func(val);\n    }\n}\n\ntorch::Tensor sigmoid_forward(torch::Tensor input) {\n    TORCH_CHECK(input.is_cuda(), "Input must reside on CUDA device");\n    TORCH_CHECK(input.is_contiguous(), "Input must be contiguous");\n    auto output = torch::empty_like(input);\n\n    const int64_t numel = input.numel();\n    const int threads = 256;\n    const int64_t blocks = (numel + threads - 1) / threads;\n\n    AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), "sigmoid_forward_cuda", ([&] {\n        sigmoid_kernel<scalar_t><<<blocks, threads, 0,\n                                   at::cuda::getCurrentCUDAStream()>>>(\n            input.data_ptr<scalar_t>(),\n            output.data_ptr<scalar_t>(),\n            numel);\n    }));\n\n    cudaError_t err = cudaGetLastError();\n    TORCH_CHECK(err == cudaSuccess, "sigmoid_kernel launch failed with error code ", err);\n    return output;\n}\n\'\'\'\n\ncpp_src = r\'\'\'\ntorch::Tensor sigmoid_forward(torch::Tensor input);\n\'\'\'\n\nsigmoid_module = load_inline(\n    name=\'sigmoid_cuda\',\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=[\'sigmoid_forward\'],\n    with_cuda=True,\n verbose=True,\n    extra_cuda_cflags=[\'-O2\',\'--ptxas-options=-v\']\n)\n\n\nclass ModelNew(nn.Module):\n    """\n    CUDA-accelerated model that applies element-wise Sigmoid.\n    Mirrors the original Model interface.\n    """\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        self.sigmoid = sigmoid_module\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.sigmoid.sigmoid_forward(x)'''

            if not new_kernel_code_full: 
                status, details = "Failed (Coder)", "Coder Agent did not produce valid code."
                print(f"âŒ {status} {details}")
                continue 
            print("[Coder Agent] New kernel source generated.")
                
            # 5. éªŒè¯å’Œåˆ†æ
            current_module_name = f"{problem_name}_{i}" # å½“å‰çš„new_kernel_code_fullçš„æ–°åå­—
            print(f"Compiling new kernel (module: {current_module_name})...")
            
            try:
                # current_code_is_correct = False
                for j in range(5):
                    # gc.collect()
                    verResult, errMessage = validate_extracted_code(new_kernel_code_full, init_inputs, inputs, ref_outputs, pytorch_kernel_module)# è¿™ä¸ªerrMessageä¸­å¯¹äºç»“æœé”™è¯¯çš„ä¿¡æ¯æ²¡æœ‰åšå‰äº”ä¸ªé”™è¯¯æ•°æ®æå–ï¼Œæ˜¯å…¨éƒ¨çš„é”™è¯¯æ•°æ®
                    if not verResult:
                        print(f"å°è¯•ä¿®æ­£å½“å‰é”™è¯¯ï¼Œç¬¬{j}æ¬¡å°è¯•")
                        err_str = str(errMessage)
                        print(f"--- Error Snippet ---\n{err_str[:500]}...\n---------------------")
                        if len(err_str) > 4000:
                            err_str = err_str[:2000] + "\n...[TRUNCATED]...\n" + err_str[-2000:]
                        new_kernel_code_full = correct_cuda_kernel(
                            full_pytorch_source_code,
                            new_kernel_code_full,
                            errMessage
                        )
                        if new_kernel_code_full:
                            print("Code corrected by LLM. Retrying verification...")
                        else:
                            print("LLM correction failed (did not return valid code). Aborting.")
                            break
                    else:
                        # current_code_is_correct = True
                        break
                # gc.collect()
                module, stdout_log, err_msg = cuda_utils.load_module(
                    new_kernel_code_full,
                    current_module_name,
                    init_inputs, 
                    pytorch_kernel_module
                )
                # print("Compilation successful.")
                
                new_ptxas_metrics = cuda_utils.parse_ptxas_info(stdout_log)# DONE3 é’ˆå¯¹21ç”¨ä¾‹è¿™é‡Œæå–çš„PTXASä¿¡æ¯ä¸å¤ªå¯¹åŠ²
                if not module:
                    status, details = "Failed (Compilation)", f"New kernel is COMPILATION INCORRECT.{err_msg}"
                    print(f"âŒ {status}")
                    continue 
                is_correct, err_str = cuda_utils.check_correctness(inputs, ref_outputs, module)
                if not is_correct:
                    status, details = "Failed (Correctness)", f"New kernel is OUTPUT RESULT INCORRECT.{err_str}"
                    print(f"âŒ {status}")
                    continue 



                # module, stdout_log, err_msg = cuda_utils.load_module(
                #     new_kernel_code_full,
                #     current_module_name,
                #     init_inputs, 
                # )
                # # print("Compilation successful.")
                
                # new_ptxas_metrics = cuda_utils.parse_ptxas_info(stdout_log)# DONE3 é’ˆå¯¹21ç”¨ä¾‹è¿™é‡Œæå–çš„PTXASä¿¡æ¯ä¸å¤ªå¯¹åŠ²
                # current_code_is_correct = True
                # if not module:
                #     status, details = "Failed (Compilation)", f"New kernel is COMPILATION INCORRECT.{err_msg}"
                #     print(f"âŒ {status}")
                #     current_code_is_correct = False
                #     # continue 
                # else: 
                #     is_correct, err_str = cuda_utils.check_correctness(inputs, ref_outputs, module)
                #     if not is_correct:
                #         status, details = "Failed (Correctness)", f"New kernel is OUTPUT RESULT INCORRECT.{err_str}"
                #         print(f"âŒ {status}")
                #         current_code_is_correct = False
                #         # continue 
                # if not current_code_is_correct:
                #     for i in range(5):
                #         verResult, errMessage = validate_extracted_code(new_kernel_code_full, init_inputs, inputs, ref_outputs)# è¿™ä¸ªerrMessageä¸­å¯¹äºç»“æœé”™è¯¯çš„ä¿¡æ¯æ²¡æœ‰åšå‰äº”ä¸ªé”™è¯¯æ•°æ®æå–ï¼Œæ˜¯å…¨éƒ¨çš„é”™è¯¯æ•°æ®
                #         if not verResult:
                #             print(f"å°è¯•ä¿®æ­£å½“å‰é”™è¯¯ï¼Œç¬¬{i}æ¬¡å°è¯•")
                #             err_str = str(errMessage)
                #             print(f"--- Error Snippet ---\n{err_str[:500]}...\n---------------------")
                #             if len(err_str) > 4000:
                #                 err_str = err_str[:2000] + "\n...[TRUNCATED]...\n" + err_str[-2000:]
                #             new_kernel_code_full = correct_cuda_kernel(
                #                 full_pytorch_source_code,
                #                 new_kernel_code_full,
                #                 errMessage
                #             )
                #             if new_kernel_code_full:
                #                 print("Code corrected by LLM. Retrying verification...")
                #             else:
                #                 print("LLM correction failed (did not return valid code). Aborting.")
                #                 break
                #         else:
                #             current_code_is_correct = True
                #             break
                # current_module_name = current_module_name + "_verify"
                # # if not current_code_is_correct:
                # module, stdout_log, err_msg = cuda_utils.load_module(
                #     new_kernel_code_full,
                #     current_module_name,
                #     init_inputs, 
                # )
                # # print("Compilation successful.")
                
                # new_ptxas_metrics = cuda_utils.parse_ptxas_info(stdout_log)# DONE3 é’ˆå¯¹21ç”¨ä¾‹è¿™é‡Œæå–çš„PTXASä¿¡æ¯ä¸å¤ªå¯¹åŠ²
                # if not module:
                #     status, details = "Failed (Compilation)", f"New kernel is COMPILATION INCORRECT.{err_msg}"
                #     print(f"âŒ {status}")
                #     continue 
                # is_correct, err_str = cuda_utils.check_correctness(inputs, ref_outputs, module)
                # if not is_correct:
                #     status, details = "Failed (Correctness)", f"New kernel is OUTPUT RESULT INCORRECT.{err_str}"
                #     print(f"âŒ {status}")
                #     continue 

            except Exception as e:
                status, details = "An exception occurred during compilation or validation!", str(e)
                print(f"âŒ {status}")
                continue 
                
            print("New kernel is CORRECT. Benchmarking...")
            
            new_time_ms = cuda_utils.benchmark_kernel(inputs, module)
            print("Analyzing new kernel with NCU...")
            
            new_ncu_metrics = cuda_utils.get_real_ncu_metrics(
                module.__file__, 
                current_module_name, 
                inputs    
            )
            
            if new_time_ms < best_time_ms:
                status = "Success (New Best)"
                details = f"Performance improved from {best_time_ms:.3f} ms to {new_time_ms:.3f} ms."
                print(f"âœ… {status} {details}")
                
                best_time_ms = new_time_ms
                
                # [!!! å·²æ›´æ–° !!!] æå– CUDA-only ä»£ç ä»¥ä¾›ä¸‹æ¬¡è¿­ä»£
                # if new_kernel_code_full.startswith(cpp_source):
                #     new_kernel_code_cuda_only = new_kernel_code_full[len(cpp_source):].strip()
                # else:
                #     match = re.search(r'__global__\s+void\s+' + kernel_name + r'\(.*?\)\s*\{.*\}', new_kernel_code_full, re.DOTALL)
                #     if match:
                #         new_kernel_code_cuda_only = match.group(0)
                #     else:
                #         new_kernel_code_cuda_only = new_kernel_code_full # æ— æ³•åˆ†ç¦»
                #         print(f"[Warning] Round {i}: Could not auto-extract kernel. Saving full code.")

                best_kernel_code_cuda = new_kernel_code_full  
                best_ptxas_metrics = new_ptxas_metrics
                best_ncu_metrics = new_ncu_metrics

                # [!!! å·²ä¿®æ”¹ !!!] (æ¥è‡ªä½ ä¹‹å‰çš„è¯·æ±‚) å®æ—¶ä¿å­˜æœ€ä½³å†…æ ¸
                try:
                    # ä» history_file_path æ¨å¯¼æœ€ä½³å†…æ ¸è·¯å¾„
                    # (ä¾‹å¦‚: ".../100_HingeLoss_optimization_history.json" 
                    # å˜ä¸º ".../100_HingeLoss_best_kernel.cu")
                    best_kernel_path = history_file_path.replace(
                        "_optimization_history.json", 
                        "_best_kernel.cu"
                    )
                    
                    # ä¿å­˜å®Œæ•´çš„ C++ åŒ…è£…å™¨ + ä¼˜åŒ–çš„ CUDA å†…æ ¸
                    with open(best_kernel_path, "w", encoding='utf-8') as f:
                        f.write(best_kernel_code_cuda)
                    print(f"âœ… Real-time save: New best kernel saved to {best_kernel_path}")
                
                except Exception as e:
                    print(f"[Warning] Round {i}: Failed to real-time save best kernel: {e}")
                # [!!! ç»“æŸä¿®æ”¹ !!!]

                # [!!! æ–°å¢ 3 !!!] å®æ—¶ä¿å­˜æœ€ä¼˜ç»Ÿè®¡æ•°æ® (ms å’Œ speedup)
                if baseline_time_ms != float('inf') and best_time_ms > 0:
                    try:
                        speedup = baseline_time_ms / best_time_ms
                        stats = {
                            "problem_name": problem_name,
                            "baseline_ms": baseline_time_ms,
                            "best_cuda_ms": best_time_ms,
                            "speedup": speedup,
                            "last_updated_round": i
                        }
                        
                        # ä» history_file_path æ¨å¯¼
                        stats_file_path = history_file_path.replace(
                            "_optimization_history.json",
                            "_best_stats.json"
                        )
                        
                        with open(stats_file_path, "w", encoding='utf-8') as f:
                            json.dump(stats, f, indent=2)
                        print(f"âœ… Real-time save: New best stats saved to {stats_file_path} (Speedup: {speedup:.2f}x)")
                            
                    except Exception as e:
                        print(f"[Warning] Round {i}: Failed to real-time save stats: {e}")
                # [!!! æ–°å¢ç»“æŸ !!!]

            else:
                status = "Failed (Performance Regression)"
                details = f"New time {new_time_ms:.3f} ms is not better than best time {best_time_ms:.3f} ms."
                print(f"âŒ {status} {details}")
            
            current_ncu_metrics = best_ncu_metrics# DONEï¼šè¿™é‡Œåº”è¯¥æ˜¯best_ncu_metricså§ï¼Œä¹‹å‰æ˜¯new_ncu_metrics

        except Exception as e:
            status, details = "Failed (Unhandled Exception)", str(e)
            print(f"âŒ {status} {details}")
            
        finally:
            # ================= [!!! é‡ç‚¹ä¿®æ”¹ï¼šå¼ºåŠ›å›æ”¶å½“å‰è½®æ¬¡èµ„æº !!!] =================
            print(f"--- Cleaning up resources for Round {i} ---")
            try:
                # 1. æ¸…é™¤æœ¬è½®ç¼–è¯‘ç”Ÿæˆçš„ JIT æ¨¡å—ç¼“å­˜ (å…³é”®)
                # è¿™ä¸€æ­¥ä¼šé‡Šæ”¾åŠ è½½çš„ .so æ–‡ä»¶å¥æŸ„å’Œç›¸å…³æ˜¾å­˜å¼•ç”¨
                cuda_utils._gemm_module = None
                
                # 2. æ˜¾å¼åˆ é™¤æœ¬è½®äº§ç”Ÿçš„ä¸­é—´å¤§å¯¹è±¡
                # ä½¿ç”¨ locals().get() æˆ–æ£€æŸ¥å˜é‡æ˜¯å¦å­˜åœ¨ï¼Œé˜²æ­¢å› æŠ¥é”™å¯¼è‡´å˜é‡æœªå®šä¹‰
                if 'module' in locals() and module is not None:
                    del module
                if 'new_kernel_code_full' in locals(): 
                    # æ³¨æ„ï¼šå¦‚æœ history_entry éœ€è¦ç”¨åˆ°ä»£ç å­—ç¬¦ä¸²ï¼Œè¯·ç¡®ä¿å…ˆä¿å­˜åˆ° history_entry å† del
                    # æˆ–è€…ä¾èµ– Python çš„å¼•ç”¨è®¡æ•°ï¼ˆåªè¦ history_entry å¼•ç”¨äº†ï¼Œdel å±€éƒ¨å˜é‡ä¹Ÿæ²¡äº‹ï¼‰
                    pass 

                # 3. å¼ºåˆ¶ Python åƒåœ¾å›æ”¶ (å›æ”¶ CPU å†…å­˜å¯¹è±¡)
                gc.collect()
                
                # 4. å¼ºåˆ¶ PyTorch æ¸…ç©º CUDA ç¼“å­˜ (å›æ”¶ GPU æ˜¾å­˜)
                # [æ ¸å¿ƒé€»è¾‘]ï¼šè¿™é‡ŒåŒ…è£¹ try...exceptï¼Œé˜²æ­¢ "misaligned address" ç­‰ä¸¥é‡é”™è¯¯
                # å¯¼è‡´ empty_cache() æŠ›å‡ºå¼‚å¸¸ä»è€Œä¸­æ–­å¾ªç¯ã€‚
                torch.cuda.empty_cache()
                
            except Exception as cleanup_err:
                # åæ‰æ¸…ç†è¿‡ç¨‹ä¸­çš„é”™è¯¯ï¼Œä¿è¯ç»å¯¹èƒ½è¿›å…¥ä¸‹ä¸€è½®
                print(f"[Warning] Cleanup failed in Round {i} (Ignored to continue loop): {cleanup_err}")
            
            # [!!! å·²æ›´æ–° !!!] æå– CUDA-only ä»£ç ä»¥ä¿å­˜åˆ°å†å²
            # code_to_save = ""
            # if new_kernel_code_full: # ä»…å½“ Coder æˆåŠŸæ—¶
            #     if new_kernel_code_cuda_only: # å¦‚æœåœ¨ 'Success' å—ä¸­å·²æå–
            #         code_to_save = new_kernel_code_cuda_only
            #     # å¦åˆ™ï¼Œå†æ¬¡å°è¯•æå–ï¼ˆä»¥é˜²å¤±è´¥ï¼‰
            #     elif new_kernel_code_full.startswith(cpp_source):
            #         code_to_save = new_kernel_code_full[len(cpp_source):].strip()
            #     else:
            #         match = re.search(r'__global__\s+void\s+' + kernel_name + r'\(.*?\)\s*\{.*\}', new_kernel_code_full, re.DOTALL)
            #         if match: 
            #             code_to_save = match.group(0)
            #         else: 
            #             code_to_save = new_kernel_code_full # æ— æ³•åˆ†ç¦»ï¼Œä¿å­˜å…¨éƒ¨
            
            # history_entry = { # ç”±äºTODO 1 åšçš„ä¿®æ”¹ï¼Œæ”¹æˆä¸‹é¢çš„å†å²å­˜å‚¨æ–¹å¼ğŸ‘‡
            #     "round": i,# å½“å‰çš„è½®æ•°
            #     "goal": opt_goal,# ç”Ÿæˆè¯¥é¡¹ä¸­çš„ä»£ç çš„ä¼˜åŒ–ç›®æ ‡ï¼ˆæœ¬é¡¹ä¸­çš„codeå°±æ˜¯åŸºäºè¿™ä¸ªgoalç”Ÿæˆçš„ï¼‰
            #     "status": status,# å½“å‰codeçš„çŠ¶æ€
            #     "time_ms": new_time_ms if new_time_ms != float('inf') else None,# å½“å‰codeçš„æ‰§è¡Œæ—¶é—´
            #     "ptxas_metrics": new_ptxas_metrics,# å½“å‰codeçš„ptxasæŒ‡æ ‡
            #     "all_ncu_metrics": new_ncu_metrics,# å½“å‰codeçš„ncuæŒ‡æ ‡
            #     "selected_ncu_metrics": relevant_metric_names,# åœ¨ç”Ÿæˆå½“å‰codeçš„æ—¶å€™é€‰æ‹©çš„ncuæŒ‡æ ‡
            #     "details": details,
            #     "code": new_kernel_code_full# å½“å‰code
            # }
            history_entry = {
                "round": i,
                "goal": opt_goal,
                # [!!! æ–°å¢ !!!] ä¿å­˜è¯Šæ–­å’Œå…·ä½“çš„å®æ–½è®¡åˆ’ï¼Œè¿™å°±æ˜¯â€œä»£ç æ”¹åŠ¨â€çš„è¯­ä¹‰æ›¿èº«
                "bottleneck_analysis": bottleneck_analysis, 
                "detailed_plan": detailed_plan,
                
                "status": status,
                "time_ms": new_time_ms if new_time_ms != float('inf') else None,
                "ptxas_metrics": new_ptxas_metrics,
                "all_ncu_metrics": new_ncu_metrics,
                "selected_ncu_metrics": relevant_metric_names,
                "details": details,
                "code": new_kernel_code_full
            }

            optimization_history.append(history_entry)

            # [!!! å·²ä¿®æ”¹ !!!] (æ¥è‡ªä½ ä¹‹å‰çš„è¯·æ±‚) å®æ—¶ä¿å­˜å†å²
            try:
                with open(history_file_path, 'w', encoding='utf-8') as f:
                    json.dump(optimization_history, f, indent=2)
            except Exception as e:
                print(f"[Warning] Round {i}: Failed to real-time save history: {e}")
            # [!!! ç»“æŸä¿®æ”¹ !!!]

    # 4. æœ€ç»ˆæŠ¥å‘Š
    print("\n--- Optimization Finished ---")
    if optimization_history:
        print(f"Baseline performance (Round 0): {optimization_history[0].get('time_ms', 0.0):.3f} ms")
    print(f"Best kernel performance: {best_time_ms:.3f} ms")
    
    # [!!! æ³¨æ„ !!!] 
    # æœ€ç»ˆä¿å­˜ .cu å’Œ .json çš„ä»£ç ä¿ç•™åœ¨æ­¤å¤„ï¼Œ
    # ä½œä¸º"æœ€ç»ˆ"çŠ¶æ€çš„ä¿è¯ï¼Œ
    # å³ä½¿å®æ—¶ä¿å­˜å¤±è´¥ã€‚
    
    final_kernel_path = history_file_path.replace(
        "_optimization_history.json", 
        "_best_kernel.cu"
    )
    with open(final_kernel_path, "w", encoding='utf-8') as f:
        f.write(best_kernel_code_cuda)  
    print(f"Best kernel C++/CUDA source saved to {final_kernel_path}")
    
    with open(history_file_path, 'w') as f:  
        json.dump(optimization_history, f, indent=2)
    print(f"Optimization history saved to {history_file_path}")
    
    # 5. [!!! å·²æ›´æ–° !!!] è¿”å›æœ€ä½³èŠ‚ç‚¹
    best_entry = None
    if best_time_ms != float('inf'):
         best_entry = next((h for h in reversed(optimization_history) if h.get('time_ms') == best_time_ms), None)
    
    if not best_entry: # å¦‚æœæ²¡æœ‰æˆåŠŸçš„ï¼Œè¿”å› Baseline
         best_entry = optimization_history[0] if optimization_history else {"error": "No history found."}
         
    return best_entry #è¿”å›çš„æ˜¯æœ€å¥½çš„é‚£ä¸ªå†å²ä¼˜åŒ–é¡¹ï¼Œæ³¨æ„æ˜¯æ•´ä¸ªé¡¹ä¸ä»…ä»…æ˜¯cuda kernel

# [!!! æ–°å¢ !!!] 
# æ·»åŠ æ–°çš„ main() å’Œ if __name__ == "__main__": 
# ä»¥è°ƒç”¨é€šç”¨å¾ªç¯ï¼Œå®ç°åå‘å…¼å®¹
# def main():
#     """
#     ä¸ºåŸå§‹ GEMM é—®é¢˜è®¾ç½®å‚æ•°ï¼Œå¹¶è°ƒç”¨é€šç”¨çš„ä¼˜åŒ–å™¨ã€‚
#     """
#     print("--- Running Original GEMM Problem (Backward-Compatibility) ---")
    
#     # 1. è®¾ç½®åŸå§‹çš„ GEMM é—®é¢˜å‚æ•°
#     N = 8192 # <--- [!!! æ–°å¢ !!!] ä¸º GEMM å®šä¹‰ N
#     # (å¦‚æœ config.py ä¸­å­˜åœ¨ MATRIX_Nï¼Œåˆ™è¦†ç›–)
#     if hasattr(config, 'MATRIX_N'): 
#         N = config.MATRIX_N
            
#     device = torch.device(config.DEVICE)
#     torch.manual_seed(42)
#     A_torch = torch.randn((N, N), dtype=torch.float32, device=device)
#     B_torch = torch.randn((N, N), dtype=torch.float32, device=device)
#     C_ref_torch = torch.matmul(A_torch, B_torch) 
    
#     inputs = [A_torch, B_torch]
#     ref_outputs = [C_ref_torch]
    
#     problem_name = "gemm_N" + str(N)
#     cpp_source = kernels.CPP_SOURCE
#     initial_cuda_code = kernels.NAIVE_CUDA_SOURCE # <--- ä½¿ç”¨ kernels.py ä¸­çš„åŸºçº¿
#     kernel_name = "gemm_kernel" # __global__ name
#     wrapper_function_name = "gemm_cuda" # C++ wrapper name
#     iteration_rounds = config.ITERATION_ROUNDS
#     history_file_path = config.HISTORY_FILE # ä½¿ç”¨ config ä¸­çš„é»˜è®¤å†å²æ–‡ä»¶

#     # 2. è°ƒç”¨é€šç”¨ä¼˜åŒ–å™¨
#     # [!!! æ³¨æ„ !!!] 
#     # æ­¤å¤„æœªä¼ é€’ baseline_time_msï¼Œ
#     # å› ä¸ºæ—§çš„ main() 
#     # åœ¨æœ€åæ‰è®¡ç®—å®ƒã€‚
#     # è¿™æ„å‘³ç€ _best_stats.json 
#     # ä¸ä¼šåœ¨æ­¤æ¨¡å¼ä¸‹ç”Ÿæˆï¼Œè¿™æ˜¯æ­£å¸¸çš„ã€‚
#     best_node = run_optimization_on_problem(
#         problem_name=problem_name,
#         cpp_source=cpp_source,
#         initial_cuda_code=initial_cuda_code,
#         inputs=inputs,
#         ref_outputs=ref_outputs,
#         kernel_name=kernel_name,
#         wrapper_function_name=wrapper_function_name,
#         iteration_rounds=iteration_rounds,
#         history_file_path=history_file_path
#         # baseline_time_ms 
#         # ä½¿ç”¨é»˜è®¤çš„ float('inf')
#     )
    
#     if 'error' in best_node:
#         print(f"GEMM optimization failed: {best_node.get('error', 'Unknown')}")
#         return
    
#     if best_node.get('time_ms') is None or best_node['time_ms'] == float('inf'):
#          print("Optimization finished, but no successful kernel was found.")
#          return

#     # 3. è¿è¡ŒåŸå§‹çš„æœ€ç»ˆåŸºå‡†æµ‹è¯• (æ¥è‡ªæ—§ main())
#     print("\n--- Running Final Benchmark (GEMM vs PyTorch) ---")
    
#     try:
#         pytorch_time_ms = cuda_utils.get_pytorch_performance(A_torch, B_torch)
#         print(f"PyTorch (torch.matmul) performance: {pytorch_time_ms:.3f} ms")
#         print(f"Our best LLM-optimized kernel: {best_node['time_ms']:.3f} ms")
        
#         speedup = pytorch_time_ms / best_node['time_ms']
#         if best_node['time_ms'] < pytorch_time_ms:
#             print(f"SUCCESS: Optimized kernel is {speedup:.2f}x faster than PyTorch!")
#         else:
#             print(f"Result: PyTorch is {1/speedup:.2f}x faster.")
#     except AttributeError:
#          print("Warning: cuda_utils.get_pytorch_performance not found. Skipping final comparison.")
#     except Exception as e:
#          print(f"Error during final benchmark: {e}")


# if __name__ == "__main__":
#     main()