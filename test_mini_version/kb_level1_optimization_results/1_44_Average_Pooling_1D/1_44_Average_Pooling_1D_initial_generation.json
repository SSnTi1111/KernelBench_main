[
  {
    "attempt": 0,
    "type": "generation",
    "response": "```cpp\n#include <torch/extension.h>\n\n// C++ Wrapper 函数声明 (签名)\ntorch::Tensor kb_1_44_Average_Pooling_1D_wrapper(torch::Tensor arg0, int64_t arg1, int64_t arg2, int64_t arg3);\n```\n\n```cu\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <cmath>\n#include <vector>\n#include <ATen/cuda/CUDAContext.h>\n\n// [重要] CUDA 辅助函数在 kernel 之前定义/声明\n__device__ float blockReduceSum(float val, float* shared) {\n    int lane = threadIdx.x % warpSize;\n    int wid = threadIdx.x / warpSize;\n\n    // Warp 内归约\n    for (int offset = warpSize / 2; offset > 0; offset /= 2) {\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    }\n\n    // 每个 warp 的第一个线程写入共享内存\n    if (lane == 0) {\n        shared[wid] = val;\n    }\n    __syncthreads();\n\n    // 第一个 warp 进行最终归约\n    val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0.0f;\n    if (wid == 0) {\n        for (int offset = warpSize / 2; offset > 0; offset /= 2) {\n            val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n        }\n    }\n    return val;\n}\n\n// CUDA 内核实现: 1D Average Pooling (count_include_pad=True)\n__global__ void avg_pool1d_kernel(\n    const float* __restrict__ x,\n    float* __restrict__ y,\n    int N, int C, int L_in,\n    int K, int stride, int pad,\n    int L_out\n) {\n    int c = blockIdx.x;\n    int n = blockIdx.y;\n\n    if (n >= N || c >= C) return;\n\n    int base_in = ((n * C) + c) * L_in;\n    int base_out = ((n * C) + c) * L_out;\n\n    // 每个 block 覆盖一个 (n, c) 切片，线程沿着输出长度遍历\n    for (int out_idx = threadIdx.x; out_idx < L_out; out_idx += blockDim.x) {\n        int start = out_idx * stride - pad;\n        float sum = 0.0f;\n\n        // 对窗口求和（包含 pad，pad 区域视为 0）\n        #pragma unroll\n        for (int k = 0; k < K; ++k) {\n            int idx = start + k;\n            float val = 0.0f;\n            if (idx >= 0 && idx < L_in) {\n                val = x[base_in + idx];\n            }\n            sum += val;\n        }\n\n        y[base_out + out_idx] = sum / static_cast<float>(K);\n    }\n}\n\n// C++ Wrapper 实现\ntorch::Tensor kb_1_44_Average_Pooling_1D_wrapper(torch::Tensor arg0, int64_t arg1, int64_t arg2, int64_t arg3) {\n    TORCH_CHECK(arg0.is_cuda(), \"Input tensor must be on CUDA device\");\n    TORCH_CHECK(arg0.dtype() == torch::kFloat32, \"Input tensor must be float32\");\n    TORCH_CHECK(arg0.dim() == 3, \"Input tensor must be 3D (N, C, L_in)\");\n\n    int64_t N64 = arg0.size(0);\n    int64_t C64 = arg0.size(1);\n    int64_t L_in64 = arg0.size(2);\n\n    TORCH_CHECK(arg1 > 0, \"kernel_size must be > 0\");\n    TORCH_CHECK(arg2 > 0, \"stride must be > 0\");\n    TORCH_CHECK(arg3 >= 0, \"padding must be >= 0\");\n\n    int K = static_cast<int>(arg1);\n    int stride = static_cast<int>(arg2);\n    int pad = static_cast<int>(arg3);\n\n    int N = static_cast<int>(N64);\n    int C = static_cast<int>(C64);\n    int L_in = static_cast<int>(L_in64);\n\n    // PyTorch AvgPool1d 输出长度公式: floor((L_in + 2*pad - K) / stride) + 1\n    TORCH_CHECK((L_in + 2 * pad - K) >= 0, \"Invalid parameters lead to negative output length numerator\");\n    int L_out = (L_in + 2 * pad - K) / stride + 1;\n\n    auto input = arg0.contiguous();\n    auto options = input.options();\n    torch::Tensor output = torch::empty({N64, C64, static_cast<int64_t>(L_out)}, options);\n\n    const float* x = input.data_ptr<float>();\n    float* y = output.data_ptr<float>();\n\n    dim3 block(256);\n    dim3 grid(C, N);\n\n    cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n\n    avg_pool1d_kernel<<<grid, block, 0, stream>>>(\n        x, y,\n        N, C, L_in,\n        K, stride, pad,\n        L_out\n    );\n\n    TORCH_CHECK(cudaGetLastError() == cudaSuccess, \"avg_pool1d_kernel launch failed\");\n\n    return output;\n}\n```",
    "cpp_code_extracted": true,
    "cuda_code_extracted": true,
    "error": ""
  }
]