[
  {
    "attempt": 0,
    "type": "generation",
    "response_cuda_code": "import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\nsource = r'''\n#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n#define THREADS 256\n\n// ----------------------------------\n// CUDA kernel : element-wise sigmoid\n// ----------------------------------\n__global__\nvoid sigmoid_kernel(const float* __restrict__ x,\n                    float* __restrict__ y,\n                    const int64_t numel) {\n    const int64_t idx = static_cast<int64_t>(blockIdx.x) * blockDim.x + threadIdx.x;\n    if (idx < numel) {\n        float val = x[idx];\n        y[idx] = 1.0f / (1.0f + __expf(-val));\n    }\n}\n\n// ----------------------------------\n// Host wrapper\n// ----------------------------------\ntorch::Tensor sigmoid_forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input tensor must be on CUDA device\");\n    TORCH_CHECK(x.scalar_type() == at::ScalarType::Float,\n                \"Only float32 tensors are supported\");\n\n    auto y = torch::empty_like(x);\n\n    const int64_t numel = x.numel();\n    const int blocks = static_cast<int>((numel + THREADS - 1) / THREADS);\n\n    sigmoid_kernel<<<blocks, THREADS>>>(x.data_ptr<float>(),\n                                        y.data_ptr<float>(),\n                                        numel);\n\n    cudaError_t err = cudaGetLastError();\n    TORCH_CHECK(err == cudaSuccess, \"sigmoid kernel launch failed: \",\n                cudaGetErrorString(err));\n    return y;\n}\n\n'''\n\ncpp_src = r'''\ntorch::Tensor sigmoid_forward(torch::Tensor x);\n'''\n\nsigmoid_cuda = load_inline(\n    name='sigmoid_cuda',\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=['sigmoid_forward'],\n    with_cuda=True,\n    verbose=True,\n    extra_cuda_cflags=['-O3', '--ptxas-options=-v'],\n)\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Same interface as the original Model, but runs the Sigmoid activation\n    through a custom CUDA kernel.\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.sigmoid = sigmoid_cuda\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.sigmoid.sigmoid_forward(x)"
  }
]