[
  {
    "attempt": 0,
    "type": "generation",
    "response_cuda_code": "import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\nsource = r'''\n#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n\n// ------------------------------------\n// CUDA kernel : element-wise Sigmoid\n// ------------------------------------\ntemplate<typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ x,\n                               scalar_t* __restrict__ y,\n                               const int64_t numel)\n{\n    int idx     = blockIdx.x * blockDim.x + threadIdx.x;\n    int stride  = blockDim.x * gridDim.x;\n\n    for (int64_t i = idx; i < numel; i += stride)\n    {\n        scalar_t v = x[i];\n        y[i] = scalar_t(1.0) / (scalar_t(1.0) + exp(-v));\n    }\n}\n\n// ------------------------------------\n// Host wrapper\n// ------------------------------------\ntorch::Tensor sigmoid_cuda(torch::Tensor x)\n{\n    TORCH_CHECK(x.is_cuda(),  \"Input must reside on CUDA device\");\n    TORCH_CHECK(x.dtype() == torch::kFloat32 || x.dtype() == torch::kFloat64,\n                \"Supported dtypes are float32 and float64\");\n\n    // Ensure contiguous memory for simple indexing\n    auto x_contig = x.contiguous();\n    auto y        = torch::empty_like(x_contig);\n\n    const int64_t numel = x_contig.numel();\n    const int     threads = 256;\n    const int     blocks  = (numel + threads - 1) / threads;\n\n    AT_DISPATCH_FLOATING_TYPES(x_contig.scalar_type(), \"sigmoid_cuda\", ([&]\n    {\n        sigmoid_kernel<scalar_t>\n            <<<blocks, threads>>>(x_contig.data_ptr<scalar_t>(),\n                                  y.data_ptr<scalar_t>(),\n                                  numel);\n    }));\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess)\n        throw std::runtime_error(std::string(\"sigmoid_cuda failed: \")\n                                 + cudaGetErrorString(err));\n\n    return y;\n}\n'''\n\ncpp_src = r'''\ntorch::Tensor sigmoid_cuda(torch::Tensor x);\n'''\n\nsigmoid_mod = load_inline(\n    name       = 'sigmoid_mod',\n    cpp_sources   = cpp_src,\n    cuda_sources  = source,\n    functions     = ['sigmoid_cuda'],\n    with_cuda     = True,\n    verbose       = True,\n    extra_cuda_cflags=['-O3', '--use_fast_math', '--ptxas-options=-v'],\n)\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    Drop-in replacement of original Model using a custom CUDA sigmoid.\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.sigmoid_cuda = sigmoid_mod.sigmoid_cuda\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.sigmoid_cuda(x)"
  }
]