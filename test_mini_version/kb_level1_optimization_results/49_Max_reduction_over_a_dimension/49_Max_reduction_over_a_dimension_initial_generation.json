[
  {
    "attempt": 0,
    "type": "generation",
    "response": "```cpp\n#include <torch/extension.h>\n\n// C++ Wrapper 函数声明 (签名)\ntorch::Tensor kb_49_Max_reduction_over_a_dimension_wrapper(torch::Tensor arg0, int64_t arg1);\n```\n\n```cu\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <cmath>\n#include <vector>\n#include <cfloat>\n// PyTorch 2.1+ 移除了 c10::cuda::getCurrentCUDAStream\n// 使用 at::cuda::getCurrentCUDAStream() 代替\n#include <ATen/cuda/CUDAContext.h>\n\n// [辅助函数] 可选的设备端最大值函数\n__device__ inline float dmaxf(float a, float b) {\n    return a > b ? a : b;\n}\n\n// CUDA 内核实现：对指定维度进行最大值归约\n// 输入为连续内存的 N 维张量，在 dim 维进行归约\n// 将输出视为二维展平：[outer, inner]，其中\n// outer = 归约维之前各维大小之积\n// inner = 归约维之后各维大小之积\n__global__ void max_reduce_axis_kernel(const float* __restrict__ input,\n                                       float* __restrict__ output,\n                                       long long outer,\n                                       long long inner,\n                                       long long red_len) {\n    long long tid = static_cast<long long>(blockIdx.x) * blockDim.x + threadIdx.x;\n    long long total = outer * inner;\n    if (tid >= total) return;\n\n    long long out_outer = tid / inner;\n    long long out_inner = tid % inner;\n\n    // 连续内存下：基址 = out_outer * red_len * inner + out_inner\n    long long base = out_outer * red_len * inner + out_inner;\n\n    float max_val = -FLT_MAX;\n    for (long long j = 0; j < red_len; ++j) {\n        float v = input[base + j * inner];\n        if (v > max_val) {\n            max_val = v;\n        }\n    }\n    output[tid] = max_val;\n}\n\n// C++ Wrapper 实现\ntorch::Tensor kb_49_Max_reduction_over_a_dimension_wrapper(torch::Tensor arg0, int64_t arg1) {\n    TORCH_CHECK(arg0.is_cuda(), \"Input tensor must be on CUDA device.\");\n    TORCH_CHECK(arg0.scalar_type() == at::ScalarType::Float,\n                \"Input tensor must be float32.\");\n    TORCH_CHECK(arg0.numel() > 0, \"Input tensor must have at least one element.\");\n\n    // 处理维度参数（支持负维度）\n    int64_t nDim = arg0.dim();\n    TORCH_CHECK(nDim >= 1, \"Input tensor must have at least 1 dimension.\");\n    int64_t dim = arg1 >= 0 ? arg1 : (arg1 + nDim);\n    TORCH_CHECK(dim >= 0 && dim < nDim, \"Reduction dim out of range.\");\n\n    // 确保连续\n    auto x = arg0.contiguous();\n\n    // 计算 outer, inner, red_len\n    auto sizes = x.sizes();\n    long long outer = 1;\n    for (int64_t i = 0; i < dim; ++i) {\n        outer *= static_cast<long long>(sizes[i]);\n    }\n    long long red_len = static_cast<long long>(sizes[dim]);\n    long long inner = 1;\n    for (int64_t i = dim + 1; i < nDim; ++i) {\n        inner *= static_cast<long long>(sizes[i]);\n    }\n\n    // 构造输出尺寸（移除归约维）\n    std::vector<int64_t> out_sizes;\n    out_sizes.reserve(static_cast<size_t>(nDim - 1));\n    for (int64_t i = 0; i < nDim; ++i) {\n        if (i == dim) continue;\n        out_sizes.push_back(sizes[i]);\n    }\n\n    auto out = at::empty(out_sizes, x.options());\n\n    // 计算网格/块维度\n    long long total = outer * inner;\n    int threads = 256;\n    int blocks = static_cast<int>((total + threads - 1) / threads);\n    if (blocks == 0) {\n        // 如果 total == 0（理论上不应出现），直接返回零大小的张量\n        return out;\n    }\n\n    // 启动内核\n    auto stream = at::cuda::getCurrentCUDAStream();\n    const float* in_ptr = x.data_ptr<float>();\n    float* out_ptr = out.data_ptr<float>();\n    max_reduce_axis_kernel<<<blocks, threads, 0, stream>>>(\n        in_ptr, out_ptr, outer, inner, red_len\n    );\n\n    // 检查内核错误\n    cudaError_t err = cudaGetLastError();\n    TORCH_CHECK(err == cudaSuccess, \"Kernel launch failed: \", cudaGetErrorString(err));\n\n    return out;\n}\n```",
    "cpp_code_extracted": true,
    "cuda_code_extracted": true,
    "error": ""
  }
]