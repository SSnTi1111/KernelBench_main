[
  {
    "round": 0,
    "goal": "Baseline",
    "status": "Success",
    "time_ms": 5.511065673828125,
    "ptxas_metrics": {
      "registers_used": 26.0,
      "shared_mem_bytes": 0.0,
      "spill_bytes": 0.0
    },
    "all_ncu_metrics": {
      "DRAMFrequency": 1504712746.34,
      "SMFrequency": 1059703044.45,
      "ElapsedCycles": 5847461.0,
      "MemoryThroughput": 84.59,
      "DRAMThroughput": 84.59,
      "Duration": 5517632.0,
      "L1TEXCacheThroughput": 23.4,
      "L2CacheThroughput": 88.32,
      "SMActiveCycles": 5805671.03,
      "ComputeSMThroughput": 26.6,
      "BlockSize": 256.0,
      "GridSize": 2048.0,
      "RegistersPerThread": 26.0,
      "SharedMemoryConfigurationSize": 32768.0,
      "DriverSharedMemoryPerBlock": 1024.0,
      "DynamicSharedMemoryPerBlock": 0.0,
      "StaticSharedMemoryPerBlock": 0.0,
      "Threads": 524288.0,
      "WavesPerSM": 2.37,
      "BlockLimitSM": 32.0,
      "BlockLimitRegisters": 8.0,
      "BlockLimitSharedMem": 32.0,
      "BlockLimitWarps": 8.0,
      "TheoreticalActiveWarpsperSM": 64.0,
      "TheoreticalOccupancy": 100.0,
      "AchievedOccupancy": 88.77,
      "AchievedActiveWarpsPerSM": 56.81
    },
    "selected_ncu_metrics": [],
    "details": "Initial baseline measurement",
    "code": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <cmath>\n#include <vector>\n#include <cfloat>\n// PyTorch 2.1+ \u79fb\u9664\u4e86 c10::cuda::getCurrentCUDAStream\n// \u4f7f\u7528 at::cuda::getCurrentCUDAStream() \u4ee3\u66ff\n#include <ATen/cuda/CUDAContext.h>\n\n// [\u8f85\u52a9\u51fd\u6570] \u53ef\u9009\u7684\u8bbe\u5907\u7aef\u6700\u5927\u503c\u51fd\u6570\n__device__ inline float dmaxf(float a, float b) {\n    return a > b ? a : b;\n}\n\n// CUDA \u5185\u6838\u5b9e\u73b0\uff1a\u5bf9\u6307\u5b9a\u7ef4\u5ea6\u8fdb\u884c\u6700\u5927\u503c\u5f52\u7ea6\n// \u8f93\u5165\u4e3a\u8fde\u7eed\u5185\u5b58\u7684 N \u7ef4\u5f20\u91cf\uff0c\u5728 dim \u7ef4\u8fdb\u884c\u5f52\u7ea6\n// \u5c06\u8f93\u51fa\u89c6\u4e3a\u4e8c\u7ef4\u5c55\u5e73\uff1a[outer, inner]\uff0c\u5176\u4e2d\n// outer = \u5f52\u7ea6\u7ef4\u4e4b\u524d\u5404\u7ef4\u5927\u5c0f\u4e4b\u79ef\n// inner = \u5f52\u7ea6\u7ef4\u4e4b\u540e\u5404\u7ef4\u5927\u5c0f\u4e4b\u79ef\n__global__ void max_reduce_axis_kernel(const float* __restrict__ input,\n                                       float* __restrict__ output,\n                                       long long outer,\n                                       long long inner,\n                                       long long red_len) {\n    long long tid = static_cast<long long>(blockIdx.x) * blockDim.x + threadIdx.x;\n    long long total = outer * inner;\n    if (tid >= total) return;\n\n    long long out_outer = tid / inner;\n    long long out_inner = tid % inner;\n\n    // \u8fde\u7eed\u5185\u5b58\u4e0b\uff1a\u57fa\u5740 = out_outer * red_len * inner + out_inner\n    long long base = out_outer * red_len * inner + out_inner;\n\n    float max_val = -FLT_MAX;\n    for (long long j = 0; j < red_len; ++j) {\n        float v = input[base + j * inner];\n        if (v > max_val) {\n            max_val = v;\n        }\n    }\n    output[tid] = max_val;\n}\n\n// C++ Wrapper \u5b9e\u73b0\ntorch::Tensor kb_49_Max_reduction_over_a_dimension_wrapper(torch::Tensor arg0, int64_t arg1) {\n    TORCH_CHECK(arg0.is_cuda(), \"Input tensor must be on CUDA device.\");\n    TORCH_CHECK(arg0.scalar_type() == at::ScalarType::Float,\n                \"Input tensor must be float32.\");\n    TORCH_CHECK(arg0.numel() > 0, \"Input tensor must have at least one element.\");\n\n    // \u5904\u7406\u7ef4\u5ea6\u53c2\u6570\uff08\u652f\u6301\u8d1f\u7ef4\u5ea6\uff09\n    int64_t nDim = arg0.dim();\n    TORCH_CHECK(nDim >= 1, \"Input tensor must have at least 1 dimension.\");\n    int64_t dim = arg1 >= 0 ? arg1 : (arg1 + nDim);\n    TORCH_CHECK(dim >= 0 && dim < nDim, \"Reduction dim out of range.\");\n\n    // \u786e\u4fdd\u8fde\u7eed\n    auto x = arg0.contiguous();\n\n    // \u8ba1\u7b97 outer, inner, red_len\n    auto sizes = x.sizes();\n    long long outer = 1;\n    for (int64_t i = 0; i < dim; ++i) {\n        outer *= static_cast<long long>(sizes[i]);\n    }\n    long long red_len = static_cast<long long>(sizes[dim]);\n    long long inner = 1;\n    for (int64_t i = dim + 1; i < nDim; ++i) {\n        inner *= static_cast<long long>(sizes[i]);\n    }\n\n    // \u6784\u9020\u8f93\u51fa\u5c3a\u5bf8\uff08\u79fb\u9664\u5f52\u7ea6\u7ef4\uff09\n    std::vector<int64_t> out_sizes;\n    out_sizes.reserve(static_cast<size_t>(nDim - 1));\n    for (int64_t i = 0; i < nDim; ++i) {\n        if (i == dim) continue;\n        out_sizes.push_back(sizes[i]);\n    }\n\n    auto out = at::empty(out_sizes, x.options());\n\n    // \u8ba1\u7b97\u7f51\u683c/\u5757\u7ef4\u5ea6\n    long long total = outer * inner;\n    int threads = 256;\n    int blocks = static_cast<int>((total + threads - 1) / threads);\n    if (blocks == 0) {\n        // \u5982\u679c total == 0\uff08\u7406\u8bba\u4e0a\u4e0d\u5e94\u51fa\u73b0\uff09\uff0c\u76f4\u63a5\u8fd4\u56de\u96f6\u5927\u5c0f\u7684\u5f20\u91cf\n        return out;\n    }\n\n    // \u542f\u52a8\u5185\u6838\n    auto stream = at::cuda::getCurrentCUDAStream();\n    const float* in_ptr = x.data_ptr<float>();\n    float* out_ptr = out.data_ptr<float>();\n    max_reduce_axis_kernel<<<blocks, threads, 0, stream>>>(\n        in_ptr, out_ptr, outer, inner, red_len\n    );\n\n    // \u68c0\u67e5\u5185\u6838\u9519\u8bef\n    cudaError_t err = cudaGetLastError();\n    TORCH_CHECK(err == cudaSuccess, \"Kernel launch failed: \", cudaGetErrorString(err));\n\n    return out;\n}"
  },
  {
    "round": 1,
    "goal": "Refactor the kernel to use shared memory for coalesced data loading to enable contiguous accesses and improve cache hit rates, curing the strided memory access inefficiency.\nOptimization History:\nPrevious Optimization Attempts:\n  Round 0:\n    Goal: Baseline\n    Status: Success\n    Performance: 5.511 ms\n    Registers: 26.0\n    Shared Mem: 0.0 bytes",
    "status": "Failed (Performance Regression)",
    "time_ms": 7.77523193359375,
    "ptxas_metrics": {
      "registers_used": 108.0,
      "shared_mem_bytes": 32768.0,
      "spill_bytes": 0.0
    },
    "all_ncu_metrics": {
      "DRAMFrequency": 1509336820.51,
      "SMFrequency": 1063046669.71,
      "ElapsedCycles": 8272607.0,
      "MemoryThroughput": 57.26,
      "DRAMThroughput": 57.26,
      "Duration": 7781664.0,
      "L1TEXCacheThroughput": 23.81,
      "L2CacheThroughput": 63.34,
      "SMActiveCycles": 8108054.1,
      "ComputeSMThroughput": 36.24,
      "BlockSize": 256.0,
      "GridSize": 2048.0,
      "RegistersPerThread": 108.0,
      "SharedMemoryConfigurationSize": 102400.0,
      "DriverSharedMemoryPerBlock": 1024.0,
      "DynamicSharedMemoryPerBlock": 0.0,
      "StaticSharedMemoryPerBlock": 32768.0,
      "Threads": 524288.0,
      "WavesPerSM": 9.48,
      "BlockLimitSM": 32.0,
      "BlockLimitRegisters": 2.0,
      "BlockLimitSharedMem": 3.0,
      "BlockLimitWarps": 8.0,
      "TheoreticalActiveWarpsperSM": 16.0,
      "TheoreticalOccupancy": 25.0,
      "AchievedOccupancy": 24.3,
      "AchievedActiveWarpsPerSM": 15.55
    },
    "selected_ncu_metrics": [
      "DRAMThroughput",
      "L1TEXCacheThroughput",
      "L2CacheThroughput",
      "StaticSharedMemoryPerBlock",
      "DynamicSharedMemoryPerBlock"
    ],
    "details": "New time 7.775 ms is not better than best time 5.511 ms.",
    "code": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <cmath>\n#include <vector>\n#include <cfloat>\n#include <ATen/cuda/CUDAContext.h>\n\n// [\u8f85\u52a9\u51fd\u6570] \u53ef\u9009\u7684\u8bbe\u5907\u7aef\u6700\u5927\u503c\u51fd\u6570\n__device__ inline float dmaxf(float a, float b) {\n    return a > b ? a : b;\n}\n\n// CUDA \u5185\u6838\u5b9e\u73b0\uff1a\u5bf9\u6307\u5b9a\u7ef4\u5ea6\u8fdb\u884c\u6700\u5927\u503c\u5f52\u7ea6\uff08\u5171\u4eab\u5185\u5b58\u5206\u5757\u7248\uff09\n__global__ void max_reduce_axis_kernel(const float* __restrict__ input,\n                                       float* __restrict__ output,\n                                       long long outer,\n                                       long long inner,\n                                       long long red_len) {\n    // \u53ef\u8c03\u5e38\u91cf\n    constexpr int THREADS_PER_BLOCK = 256;   // \u4fdd\u6301\u4e0e host \u7aef launch \u8bbe\u7f6e\u4e00\u81f4\n    constexpr int RED_TILE          = 32;    // \u6bcf\u6b21\u5728 shared memory \u4e2d\u7f13\u5b58\u7684 reduction \u957f\u5ea6\n\n    // \u5171\u4eab\u5185\u5b58 tile\uff1a\u5f62\u72b6 [RED_TILE, THREADS_PER_BLOCK]\uff0c\u884c\u4e3b\u5e8f\n    __shared__ float s_tile[RED_TILE * THREADS_PER_BLOCK];\n    #define S(row, col) s_tile[(row) * THREADS_PER_BLOCK + (col)]\n\n    // \u5f53\u524d\u7ebf\u7a0b\u8d1f\u8d23\u7684\u8f93\u51fa\u5143\u7d20\u5168\u5c40 index\n    long long tid   = static_cast<long long>(blockIdx.x) * blockDim.x + threadIdx.x;\n    long long total = outer * inner;\n    if (tid >= total) {\n        return;    // \u8d8a\u754c\u7ebf\u7a0b\u76f4\u63a5\u8fd4\u56de\n    }\n\n    // \u53cd\u7b97\u51fa (outer, inner) \u5750\u6807\n    long long out_outer = tid / inner;\n    long long out_inner = tid % inner;\n\n    // \u8be5\u8f93\u51fa\u5143\u7d20\u5728 input \u4e2d\u7684\u8d77\u59cb\u57fa\u5740\n    long long base = out_outer * red_len * inner + out_inner;\n\n    // \u5f52\u7ea6\u8fc7\u7a0b\u4e2d\u7ef4\u62a4\u7684 running max\n    float running_max = -FLT_MAX;\n\n    // \u5206\u5757\u904d\u5386 reduction \u7ef4\n    for (long long red0 = 0; red0 < red_len; red0 += RED_TILE) {\n\n        // 5.1 \u5171\u4eab\u5185\u5b58\u52a0\u8f7d\uff1a\u6240\u6709\u7ebf\u7a0b\u534f\u540c\uff0c\u5c06 [red0, red0+RED_TILE) \u7684\u503c\u642c\u5230 shared memory\n        #pragma unroll\n        for (int jj = 0; jj < RED_TILE; ++jj) {\n            long long r_idx = red0 + jj;\n            if (r_idx < red_len) {\n                // \u6309\u5217\uff08threadIdx.x\uff09\u65b9\u5411\u8fde\u7eed\u8bbf\u95ee\uff0c\u786e\u4fdd\u5168\u5c40\u8bfb\u5408\u5e76\n                float v = input[base + r_idx * inner];\n                S(jj, threadIdx.x) = v;\n            }\n        }\n\n        __syncthreads();  // \u786e\u4fdd tile \u5df2\u5b8c\u5168\u5199\u5165\u540e\u518d\u8fdb\u884c\u8ba1\u7b97\n\n        // 5.2 \u5728 shared memory \u4e0a\u505a per-thread \u5f52\u7ea6\n        #pragma unroll\n        for (int jj = 0; jj < RED_TILE; ++jj) {\n            if (red0 + jj < red_len) {   // \u5c3e\u90e8\u8d8a\u754c\u4fdd\u62a4\n                running_max = dmaxf(running_max, S(jj, threadIdx.x));\n            }\n        }\n\n        __syncthreads();  // \u9632\u6b62\u4e0b\u4e00\u8f6e\u52a0\u8f7d\u8986\u76d6\u4ecd\u5728\u4f7f\u7528\u7684 tile\n    }\n\n    // \u5199\u56de\u7ed3\u679c\n    output[tid] = running_max;\n}\n\n// C++ Wrapper \u5b9e\u73b0\ntorch::Tensor kb_49_Max_reduction_over_a_dimension_wrapper(torch::Tensor arg0, int64_t arg1) {\n    TORCH_CHECK(arg0.is_cuda(), \"Input tensor must be on CUDA device.\");\n    TORCH_CHECK(arg0.scalar_type() == at::ScalarType::Float,\n                \"Input tensor must be float32.\");\n    TORCH_CHECK(arg0.numel() > 0, \"Input tensor must have at least one element.\");\n\n    // \u5904\u7406\u7ef4\u5ea6\u53c2\u6570\uff08\u652f\u6301\u8d1f\u7ef4\u5ea6\uff09\n    int64_t nDim = arg0.dim();\n    TORCH_CHECK(nDim >= 1, \"Input tensor must have at least 1 dimension.\");\n    int64_t dim = arg1 >= 0 ? arg1 : (arg1 + nDim);\n    TORCH_CHECK(dim >= 0 && dim < nDim, \"Reduction dim out of range.\");\n\n    // \u786e\u4fdd\u8fde\u7eed\n    auto x = arg0.contiguous();\n\n    // \u8ba1\u7b97 outer, inner, red_len\n    auto sizes = x.sizes();\n    long long outer = 1;\n    for (int64_t i = 0; i < dim; ++i) {\n        outer *= static_cast<long long>(sizes[i]);\n    }\n    long long red_len = static_cast<long long>(sizes[dim]);\n    long long inner = 1;\n    for (int64_t i = dim + 1; i < nDim; ++i) {\n        inner *= static_cast<long long>(sizes[i]);\n    }\n\n    // \u6784\u9020\u8f93\u51fa\u5c3a\u5bf8\uff08\u79fb\u9664\u5f52\u7ea6\u7ef4\uff09\n    std::vector<int64_t> out_sizes;\n    out_sizes.reserve(static_cast<size_t>(nDim - 1));\n    for (int64_t i = 0; i < nDim; ++i) {\n        if (i == dim) continue;\n        out_sizes.push_back(sizes[i]);\n    }\n\n    auto out = at::empty(out_sizes, x.options());\n\n    // \u8ba1\u7b97\u7f51\u683c/\u5757\u7ef4\u5ea6\n    long long total = outer * inner;\n    int threads = 256;\n    int blocks = static_cast<int>((total + threads - 1) / threads);\n    if (blocks == 0) {\n        // \u5982\u679c total == 0\uff08\u7406\u8bba\u4e0a\u4e0d\u5e94\u51fa\u73b0\uff09\uff0c\u76f4\u63a5\u8fd4\u56de\u96f6\u5927\u5c0f\u7684\u5f20\u91cf\n        return out;\n    }\n\n    // \u542f\u52a8\u5185\u6838\n    auto stream = at::cuda::getCurrentCUDAStream();\n    const float* in_ptr = x.data_ptr<float>();\n    float* out_ptr = out.data_ptr<float>();\n    max_reduce_axis_kernel<<<blocks, threads, 0, stream>>>(\n        in_ptr, out_ptr, outer, inner, red_len\n    );\n\n    // \u68c0\u67e5\u5185\u6838\u9519\u8bef\n    cudaError_t err = cudaGetLastError();\n    TORCH_CHECK(err == cudaSuccess, \"Kernel launch failed: \", cudaGetErrorString(err));\n\n    return out;\n}"
  },
  {
    "round": 2,
    "goal": "Refactor the kernel to use warp-cooperative parallel reduction per output element, loading contiguous data segments into registers for coalesced accesses and parallel max computations to improve L1 hits and hide memory latency.\nOptimization History:\nPrevious Optimization Attempts:\n  Round 0:\n    Goal: Baseline\n    Status: Success\n    Performance: 5.511 ms\n    Registers: 26.0\n    Shared Mem: 0.0 bytes\n  Round 1:\n    Goal: Refactor the kernel to use shared memory for coalesced data loading to enable contiguous accesses and improve cache hit rates, curing the strided memory access inefficiency.\n    Status: Failed (Performance Regression)\n    Performance: 7.775 ms\n    Registers: 108.0\n    Shared Mem: 32768.0 bytes\n    Selected NCU Metrics (for Goal):\n      - DRAMThroughput: 57.26\n      - L1TEXCacheThroughput: 23.81\n      - L2CacheThroughput: 63.34\n      - StaticSharedMemoryPerBlock: 32768.0\n      - DynamicSharedMemoryPerBlock: 0.0",
    "status": "Failed (Performance Regression)",
    "time_ms": 22.6123779296875,
    "ptxas_metrics": {
      "registers_used": 23.0,
      "shared_mem_bytes": 0.0,
      "spill_bytes": 0.0
    },
    "all_ncu_metrics": {
      "DRAMFrequency": 1511631228.18,
      "SMFrequency": 1064740276.71,
      "ElapsedCycles": 24048631.0,
      "MemoryThroughput": 21.96,
      "DRAMThroughput": 21.96,
      "Duration": 22586368.0,
      "L1TEXCacheThroughput": 6.01,
      "L2CacheThroughput": 25.72,
      "SMActiveCycles": 22178343.55,
      "ComputeSMThroughput": 6.46,
      "BlockSize": 128.0,
      "GridSize": 128.0,
      "RegistersPerThread": 23.0,
      "SharedMemoryConfigurationSize": 32768.0,
      "DriverSharedMemoryPerBlock": 1024.0,
      "DynamicSharedMemoryPerBlock": 0.0,
      "StaticSharedMemoryPerBlock": 0.0,
      "Threads": 16384.0,
      "WavesPerSM": 0.07,
      "BlockLimitSM": 32.0,
      "BlockLimitRegisters": 21.0,
      "BlockLimitSharedMem": 32.0,
      "BlockLimitWarps": 16.0,
      "TheoreticalActiveWarpsperSM": 64.0,
      "TheoreticalOccupancy": 100.0,
      "AchievedOccupancy": 7.48,
      "AchievedActiveWarpsPerSM": 4.79
    },
    "selected_ncu_metrics": [
      "DRAMThroughput",
      "L1TEXCacheThroughput",
      "RegistersPerThread",
      "AchievedOccupancy",
      "ComputeSMThroughput"
    ],
    "details": "New time 22.612 ms is not better than best time 5.511 ms.",
    "code": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <cmath>\n#include <vector>\n#include <cfloat>\n// PyTorch 2.1+ \u79fb\u9664\u4e86 c10::cuda::getCurrentCUDAStream\n// \u4f7f\u7528 at::cuda::getCurrentCUDAStream() \u4ee3\u66ff\n#include <ATen/cuda/CUDAContext.h>\n\n// [\u8f85\u52a9\u51fd\u6570] \u8bbe\u5907\u7aef\u6700\u5927\u503c\u51fd\u6570\n__device__ inline float dmaxf(float a, float b) {\n    return a > b ? a : b;\n}\n\n// ====================== Warp\u2013Optimised Kernel ============================ //\ntemplate <int WARPS_PER_BLOCK>\n__global__ void max_reduce_axis_warp_kernel(const float* __restrict__ input,\n                                            float* __restrict__ output,\n                                            long long outer,\n                                            long long inner,\n                                            long long red_len) {\n    // \u6bcf\u4e2a block \u8d1f\u8d23\u4e00\u4e2a out_outer\uff08\u5373\u5f52\u7ea6\u7ef4\u4e4b\u524d\u7684\u884c\uff09\n    const long long out_outer = static_cast<long long>(blockIdx.x);\n    if (out_outer >= outer) return;\n\n    // \u7ebf\u7a0b/warp \u4fe1\u606f\n    const int lane    = threadIdx.x & 31;      // 0\u201331\n    const int warp_id = threadIdx.x >> 5;      // 0 \u2026 WARPS_PER_BLOCK-1\n    const long long warp_stride = static_cast<long long>(WARPS_PER_BLOCK) * 32LL;\n\n    // \u6bcf\u4e2a warp \u4ee5\u6b65\u957f warp_stride \u8fed\u4ee3 inner \u7ef4\n    for (long long inner_idx = static_cast<long long>(warp_id) * 32LL + lane;\n         inner_idx < inner;\n         inner_idx += warp_stride) {\n\n        float local_max = -FLT_MAX;\n\n        // \u8ba1\u7b97\u57fa\u5740\uff1a [outer,row] + inner \u504f\u79fb\n        long long base = out_outer * red_len * inner + inner_idx;\n\n        // \u6cbf red_len \u8fde\u7eed\u8bbf\u95ee\n        for (long long r = 0; r < red_len; ++r) {\n            float v = input[base + r * inner];\n            local_max = dmaxf(local_max, v);\n        }\n\n        // \u5199\u56de\n        output[out_outer * inner + inner_idx] = local_max;\n    }\n}\n\n// ====================== Na\u00efve Fallback Kernel ============================ //\n__global__ void max_reduce_axis_kernel_fallback(const float* __restrict__ input,\n                                                float* __restrict__ output,\n                                                long long outer,\n                                                long long inner,\n                                                long long red_len) {\n    long long tid = static_cast<long long>(blockIdx.x) * blockDim.x + threadIdx.x;\n    long long total = outer * inner;\n    if (tid >= total) return;\n\n    long long out_outer = tid / inner;\n    long long out_inner = tid % inner;\n\n    // \u8fde\u7eed\u5185\u5b58\u4e0b\uff1a\u57fa\u5740 = out_outer * red_len * inner + out_inner\n    long long base = out_outer * red_len * inner + out_inner;\n\n    float max_val = -FLT_MAX;\n    for (long long j = 0; j < red_len; ++j) {\n        float v = input[base + j * inner];\n        if (v > max_val) {\n            max_val = v;\n        }\n    }\n    output[tid] = max_val;\n}\n\n// ============================ C++ Wrapper =============================== //\ntorch::Tensor kb_49_Max_reduction_over_a_dimension_wrapper(torch::Tensor arg0, int64_t arg1) {\n    TORCH_CHECK(arg0.is_cuda(), \"Input tensor must be on CUDA device.\");\n    TORCH_CHECK(arg0.scalar_type() == at::ScalarType::Float,\n                \"Input tensor must be float32.\");\n    TORCH_CHECK(arg0.numel() > 0, \"Input tensor must have at least one element.\");\n\n    // \u5904\u7406\u7ef4\u5ea6\u53c2\u6570\uff08\u652f\u6301\u8d1f\u7ef4\u5ea6\uff09\n    int64_t nDim = arg0.dim();\n    TORCH_CHECK(nDim >= 1, \"Input tensor must have at least 1 dimension.\");\n    int64_t dim = arg1 >= 0 ? arg1 : (arg1 + nDim);\n    TORCH_CHECK(dim >= 0 && dim < nDim, \"Reduction dim out of range.\");\n\n    // \u786e\u4fdd\u8fde\u7eed\n    auto x = arg0.contiguous();\n\n    // \u8ba1\u7b97 outer, inner, red_len\n    auto sizes = x.sizes();\n    long long outer = 1;\n    for (int64_t i = 0; i < dim; ++i) {\n        outer *= static_cast<long long>(sizes[i]);\n    }\n    long long red_len = static_cast<long long>(sizes[dim]);\n    long long inner = 1;\n    for (int64_t i = dim + 1; i < nDim; ++i) {\n        inner *= static_cast<long long>(sizes[i]);\n    }\n\n    // \u6784\u9020\u8f93\u51fa\u5c3a\u5bf8\uff08\u79fb\u9664\u5f52\u7ea6\u7ef4\uff09\n    std::vector<int64_t> out_sizes;\n    out_sizes.reserve(static_cast<size_t>(nDim - 1));\n    for (int64_t i = 0; i < nDim; ++i) {\n        if (i == dim) continue;\n        out_sizes.push_back(sizes[i]);\n    }\n\n    auto out = at::empty(out_sizes, x.options());\n\n    // Decide which kernel to launch\n    constexpr int WARPS_PER_BLOCK = 4;\n    constexpr int WARP_SIZE       = 32;\n    constexpr int OPT_THREADS     = WARPS_PER_BLOCK * WARP_SIZE; // 128\n\n    auto stream = at::cuda::getCurrentCUDAStream();\n    const float* in_ptr = x.data_ptr<float>();\n    float* out_ptr = out.data_ptr<float>();\n\n    bool use_warp_kernel = (inner >= WARP_SIZE) && (outer <= static_cast<long long>(INT_MAX));\n\n    if (use_warp_kernel) {\n        dim3 threads(OPT_THREADS);\n        dim3 blocks(static_cast<int>(outer));\n        if (blocks.x > 0) {\n            max_reduce_axis_warp_kernel<WARPS_PER_BLOCK><<<blocks, threads, 0, stream>>>(\n                in_ptr, out_ptr, outer, inner, red_len\n            );\n        }\n    } else {\n        // Fallback to na\u00efve kernel\n        long long total = outer * inner;\n        int threads = 256;\n        int blocks = static_cast<int>((total + threads - 1) / threads);\n        if (blocks > 0) {\n            max_reduce_axis_kernel_fallback<<<blocks, threads, 0, stream>>>(\n                in_ptr, out_ptr, outer, inner, red_len\n            );\n        }\n    }\n\n    // \u68c0\u67e5\u5185\u6838\u9519\u8bef\n    cudaError_t err = cudaGetLastError();\n    TORCH_CHECK(err == cudaSuccess, \"Kernel launch failed: \", cudaGetErrorString(err));\n\n    return out;\n}"
  },
  {
    "round": 3,
    "goal": "Implement a tiled block-level reduction using shared memory, where each block processes a tile of the outer dimension and collaboratively loads contiguous chunks of the reduction dimension into shared memory for local reduction, thereby enabling coalesced global loads, promoting L1 reuse via shared memory, and reducing overall DRAM traffic without excessive register growth.",
    "status": "Failed (Performance Regression)",
    "time_ms": 6.964530944824219,
    "ptxas_metrics": {
      "registers_used": 87.0,
      "shared_mem_bytes": 0.0,
      "spill_bytes": 0.0
    },
    "all_ncu_metrics": {
      "DRAMFrequency": 1507015561.16,
      "SMFrequency": 1061397496.66,
      "ElapsedCycles": 7423568.0,
      "MemoryThroughput": 67.4,
      "DRAMThroughput": 67.4,
      "Duration": 6993824.0,
      "L1TEXCacheThroughput": 28.29,
      "L2CacheThroughput": 69.76,
      "SMActiveCycles": 6784656.86,
      "ComputeSMThroughput": 25.11,
      "BlockSize": 256.0,
      "GridSize": 128.0,
      "RegistersPerThread": 87.0,
      "SharedMemoryConfigurationSize": 102400.0,
      "DriverSharedMemoryPerBlock": 1024.0,
      "DynamicSharedMemoryPerBlock": 32768.0,
      "StaticSharedMemoryPerBlock": 0.0,
      "Threads": 32768.0,
      "WavesPerSM": 0.59,
      "BlockLimitSM": 32.0,
      "BlockLimitRegisters": 2.0,
      "BlockLimitSharedMem": 3.0,
      "BlockLimitWarps": 8.0,
      "TheoreticalActiveWarpsperSM": 16.0,
      "TheoreticalOccupancy": 25.0,
      "AchievedOccupancy": 14.89,
      "AchievedActiveWarpsPerSM": 9.53
    },
    "selected_ncu_metrics": [
      "DRAMThroughput",
      "L1TEXCacheThroughput",
      "SharedMemoryConfigurationSize",
      "RegistersPerThread",
      "AchievedOccupancy"
    ],
    "details": "New time 6.965 ms is not better than best time 5.511 ms.",
    "code": "#include <cuda.h>\n#include <cuda_runtime.h>\n#include <ATen/cuda/CUDAContext.h>\n\n#include <cfloat>\n#include <cmath>\n#include <vector>\n#include <algorithm>\n\n// [\u8f85\u52a9\u51fd\u6570] \u53ef\u9009\u7684\u8bbe\u5907\u7aef\u6700\u5927\u503c\u51fd\u6570\n__device__ inline float dmaxf(float a, float b) {\n    return a > b ? a : b;\n}\n\n/*\n * \u6539\u8fdb\u7248 CUDA \u5185\u6838\uff1a\u5bf9\u6307\u5b9a\u7ef4\u5ea6\u505a\u6700\u5927\u503c\u5f52\u7ea6\n *\n * gridDim.x = outer\n * blockDim.x = min(inner, 256)                              (\u5fc5\u8981\u65f6\u5411\u4e0a\u53d6 32 \u7684\u6574\u6570\u500d)\n * \u6bcf\u4e2a\u7ebf\u7a0b\u8d1f\u8d23\u4e00\u5217\uff08column\uff0cinner \u7ef4\u4e0a\u7684\u4e00\u4e2a\u7d22\u5f15\uff09\uff0c\u82e5 inner > blockDim.x\uff0c\n * \u540c\u4e00\u7ebf\u7a0b\u4ee5 stride = blockDim.x \u7ee7\u7eed\u5904\u7406\u5269\u4f59\u5217\u3002\uff08\u867d\u7136\u76ee\u524d launch\n * \u65b9\u5f0f\u4fdd\u8bc1 inner \u2264 blockDim.x\uff0c\u8fd9\u6bb5\u903b\u8f91\u4f9d\u7136\u4fdd\u7559\u4ee5\u517c\u5bb9\u8d85\u5927 inner \u7684\u573a\u666f\uff09\n *\n * \u5bf9 reduction \u7ef4(red_len) \u8fdb\u884c\u5206\u5757(tiles)\uff0c\u6bcf\u6b21\u52a0\u8f7d TILE_R (=32) \u4e2a\u5143\u7d20\u5230\u5171\u4eab\u5185\u5b58\uff0c\n * \u7136\u540e\u6240\u6709\u7ebf\u7a0b\u5728\u5171\u4eab\u5185\u5b58\u4e0a\u5b8c\u6210\u8be5 tile \u7684\u5c40\u90e8\u6700\u5927\u503c\u8ba1\u7b97\uff0c\u518d\u7ee7\u7eed\u4e0b\u4e00\u4e2a tile\u3002\n *\n * \u5171\u4eab\u5185\u5b58\u4f7f\u7528\u5916\u90e8\u58f0\u660e\u7684\u53ef\u53d8\u5927\u5c0f\u6570\u7ec4\uff1a\n *     extern __shared__ float s_tile[];\n * \u5b9e\u9645\u5927\u5c0f\u5728 kernel launch \u65f6\u4f20\u5165\uff1a  sharedMem = TILE_R * blockDim.x * sizeof(float)\n */\nconstexpr int TILE_R = 32;            // \u6bcf\u4e2a tile \u5728 reduction \u7ef4\u4e0a\u7684\u957f\u5ea6\n\n__global__ __launch_bounds__(256, 2)\nvoid max_reduce_axis_kernel(\n        const float* __restrict__ input,\n        float*       __restrict__ output,\n        long long outer,\n        long long inner,\n        long long red_len)\n{\n    // \u5f53\u524d block \u5904\u7406\u7684 \u201c\u884c\u201d\n    const long long out_outer = static_cast<long long>(blockIdx.x);\n\n    // \u6bcf\u4e2a\u7ebf\u7a0b\u521d\u59cb\u8d1f\u8d23\u7684 \u201c\u5217\u201d\n    const int lane_col = threadIdx.x;\n    const int stride_inner = blockDim.x;\n\n    // \u5171\u4eab\u5185\u5b58\u6307\u9488\n    extern __shared__ float s_tile[];\n\n    // \u904d\u5386\u6240\u6709\u5217\uff08inner \u7ef4\u4e0a\u7684\u7d22\u5f15\uff09\n    for (long long col = lane_col; col < inner; col += stride_inner) {\n\n        float global_max = -FLT_MAX;\n\n        // \u6309 TILE_R \u5206\u5757\u904d\u5386 reduction \u7ef4\n        for (long long j0 = 0; j0 < red_len; j0 += TILE_R) {\n            const int valid_len = static_cast<int>(\n                (j0 + TILE_R <= red_len) ? TILE_R : (red_len - j0));\n\n            // ------------------------------------------------------------\n            // Step 1. cooperative loading of a tile into shared memory\n            // ------------------------------------------------------------\n            // \u4f9d\u6b21\u628a valid_len \u4e2a\u5143\u7d20\u642c\u5230\u5171\u4eab\u5185\u5b58\uff1as_tile[t * blockDim.x + threadIdx.x]\n            for (int t = 0; t < valid_len; ++t) {\n                long long gindex =\n                    ((out_outer * red_len + (j0 + t)) * inner) + col;\n                float v = input[gindex];\n                s_tile[t * blockDim.x + lane_col] = v;\n            }\n\n            // \u5904\u7406\u8d8a\u754c\u7684\u5171\u4eab\u5185\u5b58\u69fd\uff08\u4ec5\u5728 tile \u4e0d\u6ee1\u65f6\u9700\u8981\u586b\u5145\uff09\n            for (int t = valid_len; t < TILE_R; ++t) {\n                s_tile[t * blockDim.x + lane_col] = -FLT_MAX;\n            }\n\n            __syncthreads();\n\n            // ------------------------------------------------------------\n            // Step 2. \u5f52\u7ea6\uff1a\u5bf9\u8be5 tile \u5185\u7684\u503c\u6c42\u6700\u5927\n            // ------------------------------------------------------------\n            float local_max = -FLT_MAX;\n            #pragma unroll\n            for (int t = 0; t < TILE_R; ++t) {\n                float v = s_tile[t * blockDim.x + lane_col];\n                local_max = dmaxf(local_max, v);\n            }\n\n            // \u7d2f\u52a0\u5230\u8de8 tile \u7684\u5168\u5c40\u6700\u5927\n            global_max = dmaxf(global_max, local_max);\n\n            __syncthreads(); // \u4e3a\u4e0b\u4e00 tile \u6e05\u7406\u540c\u6b65\n        }\n\n        // ------------------------------------------------------------\n        // \u5199\u56de\u7ed3\u679c\n        // ------------------------------------------------------------\n        long long out_index = out_outer * inner + col;\n        output[out_index] = global_max;\n    }\n}\n\n// C++ Wrapper \u5b9e\u73b0\ntorch::Tensor kb_49_Max_reduction_over_a_dimension_wrapper(torch::Tensor arg0, int64_t arg1) {\n    TORCH_CHECK(arg0.is_cuda(), \"Input tensor must be on CUDA device.\");\n    TORCH_CHECK(arg0.scalar_type() == at::ScalarType::Float,\n                \"Input tensor must be float32.\");\n    TORCH_CHECK(arg0.numel() > 0, \"Input tensor must have at least one element.\");\n\n    // \u5904\u7406\u7ef4\u5ea6\u53c2\u6570\uff08\u652f\u6301\u8d1f\u7ef4\u5ea6\uff09\n    int64_t nDim = arg0.dim();\n    TORCH_CHECK(nDim >= 1, \"Input tensor must have at least 1 dimension.\");\n    int64_t dim = arg1 >= 0 ? arg1 : (arg1 + nDim);\n    TORCH_CHECK(dim >= 0 && dim < nDim, \"Reduction dim out of range.\");\n\n    // \u786e\u4fdd\u8fde\u7eed\n    auto x = arg0.contiguous();\n\n    // \u8ba1\u7b97 outer, red_len, inner\n    auto sizes = x.sizes();\n    long long outer = 1;\n    for (int64_t i = 0; i < dim; ++i) {\n        outer *= static_cast<long long>(sizes[i]);\n    }\n    long long red_len = static_cast<long long>(sizes[dim]);\n    long long inner = 1;\n    for (int64_t i = dim + 1; i < nDim; ++i) {\n        inner *= static_cast<long long>(sizes[i]);\n    }\n\n    // \u6784\u9020\u8f93\u51fa\u5c3a\u5bf8\uff08\u79fb\u9664\u5f52\u7ea6\u7ef4\uff09\n    std::vector<int64_t> out_sizes;\n    out_sizes.reserve(static_cast<size_t>(nDim - 1));\n    for (int64_t i = 0; i < nDim; ++i) {\n        if (i == dim) continue;\n        out_sizes.push_back(sizes[i]);\n    }\n\n    auto out = at::empty(out_sizes, x.options());\n\n    // Launch \u53c2\u6570\n    int threads = static_cast<int>(std::min<long long>(256, inner));\n    // \u5411\u4e0a\u5bf9\u9f50\u5230 32 \u7684\u500d\u6570\uff08warp \u5927\u5c0f\uff09\uff0c\u63d0\u5347\u5185\u5b58\u5bf9\u9f50\u6548\u679c\n    threads = ((threads + 31) / 32) * 32;\n    threads = std::min(threads, 256);\n\n    int blocks = static_cast<int>(outer);\n    if (blocks == 0) {\n        // \u7406\u8bba\u4e0a\u4e0d\u4f1a\u51fa\u73b0\uff0c\u56e0\u4e3a outer \u2265 1\uff0c\u505a\u4e2a\u4fdd\u9669\n        return out;\n    }\n\n    // \u5171\u4eab\u5185\u5b58\u5927\u5c0f\uff1aTILE_R * threads * sizeof(float)\n    size_t smem_bytes = static_cast<size_t>(TILE_R) * static_cast<size_t>(threads) * sizeof(float);\n\n    // \u542f\u52a8\u5185\u6838\n    auto stream = at::cuda::getCurrentCUDAStream();\n    const float* in_ptr = x.data_ptr<float>();\n    float* out_ptr = out.data_ptr<float>();\n\n    max_reduce_axis_kernel<<<blocks, threads, smem_bytes, stream>>>(\n        in_ptr, out_ptr, outer, inner, red_len\n    );\n\n    // \u68c0\u67e5\u5185\u6838\u9519\u8bef\n    cudaError_t err = cudaGetLastError();\n    TORCH_CHECK(err == cudaSuccess, \"Kernel launch failed: \", cudaGetErrorString(err));\n\n    return out;\n}"
  },
  {
    "round": 4,
    "goal": "Refactor the kernel to cooperatively load contiguous input tiles into a minimal shared memory buffer per block (sized to fit within register limits), enabling fully coalesced global reads and fast strided reductions from shared memory to boost L1 bypass and effective reuse, while maintaining low register counts through streamlined indexing and no extra locals.",
    "status": "Success (New Best)",
    "time_ms": 1.3029376029968263,
    "ptxas_metrics": {
      "registers_used": 30.0,
      "shared_mem_bytes": 1024.0,
      "spill_bytes": 0.0
    },
    "all_ncu_metrics": {
      "DRAMFrequency": 1505151107.3,
      "SMFrequency": 1058963092.76,
      "ElapsedCycles": 1385366.0,
      "MemoryThroughput": 62.74,
      "DRAMThroughput": 62.74,
      "Duration": 1307680.0,
      "L1TEXCacheThroughput": 61.88,
      "L2CacheThroughput": 70.76,
      "SMActiveCycles": 1371570.89,
      "ComputeSMThroughput": 67.5,
      "BlockSize": 256.0,
      "GridSize": 2048.0,
      "RegistersPerThread": 30.0,
      "SharedMemoryConfigurationSize": 32768.0,
      "DriverSharedMemoryPerBlock": 1024.0,
      "DynamicSharedMemoryPerBlock": 0.0,
      "StaticSharedMemoryPerBlock": 1024.0,
      "Threads": 524288.0,
      "WavesPerSM": 2.37,
      "BlockLimitSM": 32.0,
      "BlockLimitRegisters": 8.0,
      "BlockLimitSharedMem": 16.0,
      "BlockLimitWarps": 8.0,
      "TheoreticalActiveWarpsperSM": 64.0,
      "TheoreticalOccupancy": 100.0,
      "AchievedOccupancy": 85.55,
      "AchievedActiveWarpsPerSM": 54.75
    },
    "selected_ncu_metrics": [
      "DRAMThroughput",
      "L1TEXCacheThroughput",
      "L2CacheThroughput",
      "StaticSharedMemoryPerBlock",
      "RegistersPerThread"
    ],
    "details": "Performance improved from 5.511 ms to 1.303 ms.",
    "code": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <cmath>\n#include <vector>\n#include <cfloat>\n// PyTorch 2.1+ \u79fb\u9664\u4e86 c10::cuda::getCurrentCUDAStream\n// \u4f7f\u7528 at::cuda::getCurrentCUDAStream() \u4ee3\u66ff\n#include <ATen/cuda/CUDAContext.h>\n\n// [\u8f85\u52a9\u51fd\u6570] \u8bbe\u5907\u7aef\u6700\u5927\u503c\u51fd\u6570\n__device__ inline float dmaxf(float a, float b) {\n    return a > b ? a : b;\n}\n\n/*\n * \u4f18\u5316\u7248 CUDA \u5185\u6838\uff1a\n *  1. \u91c7\u7528 Tx = 32, Ty = 8 \u7684\u7ebf\u7a0b\u5e03\u5c40\uff08256 \u7ebf\u7a0b/\u5757\uff09\n *  2. \u4f7f\u7528 1 KB \u7684\u9759\u6001\u5171\u4eab\u5185\u5b58\u4f5c\u4e3a tile \u7f13\u51b2\uff0c\u534f\u540c\u52a0\u8f7d\u5e76\u5728\u5171\u4eab\u5185\u5b58\u4e2d\u505a\u5217\u5f52\u7ea6\n *  3. \u7ef4\u6301\u73b0\u6709 1-D block \u914d\u7f6e\uff0c\u5185\u90e8\u5c06\u5176\u91cd\u6620\u5c04\u5230 2-D (Tx \u00d7 Ty) \u5750\u6807\uff0c\n *     \u8fd9\u6837\u65e0\u9700\u6539\u52a8 C++ wrapper \u91cc\u7684 launch \u903b\u8f91\n *  4. \u6240\u6709\u7ebf\u7a0b\uff08\u5305\u62ec\u201c\u8d8a\u754c\u7ebf\u7a0b\u201d\uff09\u90fd\u4f1a\u53c2\u4e0e __syncthreads()\uff0c\n *     \u901a\u8fc7 valid_thread \u6807\u8bb0\u907f\u514d\u975e\u6cd5\u5168\u5c40\u5185\u5b58\u8bbf\u95ee\n */\n__global__ void max_reduce_axis_kernel(const float* __restrict__ input,\n                                       float* __restrict__ output,\n                                       long long outer,\n                                       long long inner,\n                                       long long red_len) {\n    constexpr int Tx = 32;  // \u6bcf warp \u8d1f\u8d23 inner \u7ef4\u7684 32 \u5217\n    constexpr int Ty = 8;   // 8 \u884c\u534f\u540c\u904d\u5386 reduction \u7ef4\n    __shared__ float sTile[Tx * Ty];   // 256 \u00d7 4B = 1 KB\n\n    const long long total = outer * inner;\n    const long long global_tid = static_cast<long long>(blockIdx.x) * blockDim.x + threadIdx.x;\n\n    // \u7ebf\u7a0b\u5728 block \u5185\u7684 2-D \u5750\u6807\n    const int lane = threadIdx.x & (Tx - 1);  // 0 \u2026 31\n    const int row  = threadIdx.x >> 5;        // 0 \u2026 7\n\n    // \u5224\u65ad\u8be5\u7ebf\u7a0b\u662f\u5426\u5bf9\u5e94\u5230\u4e00\u4e2a\u6709\u6548\u7684 (outer_idx, inner_idx)\n    const bool valid_thread = (global_tid < total);\n\n    // \u4ec5\u5f53 valid_thread \u65f6\u624d\u8ba1\u7b97\u771f\u5b9e\u7d22\u5f15\uff0c\u907f\u514d 64-bit \u9664\u6cd5\u5f00\u9500\u7ed9\u65e0\u6548\u7ebf\u7a0b\n    const long long outer_idx = valid_thread ? (global_tid / inner) : 0;\n    const long long inner_idx = valid_thread ? (global_tid % inner) : 0;\n\n    // \u57fa\u5740\uff1a\u56fa\u5b9a\u5728\u8be5\u7ebf\u7a0b\u8d1f\u8d23\u7684 (outer_idx, inner_idx) \u4e0a\uff0c\u4e0b\u6807 j \u6cbf reduction \u7ef4\u79fb\u52a8\n    const long long base_offset = outer_idx * red_len * inner + inner_idx;\n\n    float local_max = -FLT_MAX;\n\n    // tile \u6570 = ceil(red_len / Ty)\n    const long long num_tiles = (red_len + Ty - 1) / Ty;\n\n    for (long long tile_idx = 0; tile_idx < num_tiles; ++tile_idx) {\n        // \u5f53\u524d\u7ebf\u7a0b\u8fd9\u4e00\u884c\u8d1f\u8d23\u7684 reduction \u7ef4\u7d22\u5f15\n        const long long j = tile_idx * Ty + row;\n\n        // \u534f\u540c\u5168\u5c40\u52a0\u8f7d\n        float v = -FLT_MAX;\n        if (valid_thread && j < red_len) {\n            v = input[base_offset + j * inner];\n        }\n\n        // \u5b58\u5165\u5171\u4eab\u5185\u5b58\uff0c\u6392\u5e03\u4e3a column-major\uff1a\u884c(row) * Tx + lane\n        sTile[row * Tx + lane] = v;\n\n        __syncthreads();\n\n        // \u6bcf\u4e2a\u7ebf\u7a0b\u7eb5\u5411\u626b\u63cf\u8be5\u5217\u5b8c\u6210\u672c tile \u7684\u5f52\u7ea6\n#pragma unroll\n        for (int k = 0; k < Ty; ++k) {\n            float t = sTile[k * Tx + lane];\n            local_max = dmaxf(local_max, t);\n        }\n\n        __syncthreads();  // \u4fdd\u62a4\u4e0b\u4e00 tile \u7684\u5199\u5165\n    }\n\n    if (valid_thread) {\n        output[global_tid] = local_max;\n    }\n}\n\n// C++ Wrapper \u5b9e\u73b0\ntorch::Tensor kb_49_Max_reduction_over_a_dimension_wrapper(torch::Tensor arg0, int64_t arg1) {\n    TORCH_CHECK(arg0.is_cuda(), \"Input tensor must be on CUDA device.\");\n    TORCH_CHECK(arg0.scalar_type() == at::ScalarType::Float,\n                \"Input tensor must be float32.\");\n    TORCH_CHECK(arg0.numel() > 0, \"Input tensor must have at least one element.\");\n\n    // \u5904\u7406\u7ef4\u5ea6\u53c2\u6570\uff08\u652f\u6301\u8d1f\u7ef4\u5ea6\uff09\n    int64_t nDim = arg0.dim();\n    TORCH_CHECK(nDim >= 1, \"Input tensor must have at least 1 dimension.\");\n    int64_t dim = arg1 >= 0 ? arg1 : (arg1 + nDim);\n    TORCH_CHECK(dim >= 0 && dim < nDim, \"Reduction dim out of range.\");\n\n    // \u786e\u4fdd\u8fde\u7eed\n    auto x = arg0.contiguous();\n\n    // \u8ba1\u7b97 outer, inner, red_len\n    auto sizes = x.sizes();\n    long long outer = 1;\n    for (int64_t i = 0; i < dim; ++i) {\n        outer *= static_cast<long long>(sizes[i]);\n    }\n    long long red_len = static_cast<long long>(sizes[dim]);\n    long long inner = 1;\n    for (int64_t i = dim + 1; i < nDim; ++i) {\n        inner *= static_cast<long long>(sizes[i]);\n    }\n\n    // \u6784\u9020\u8f93\u51fa\u5c3a\u5bf8\uff08\u79fb\u9664\u5f52\u7ea6\u7ef4\uff09\n    std::vector<int64_t> out_sizes;\n    out_sizes.reserve(static_cast<size_t>(nDim - 1));\n    for (int64_t i = 0; i < nDim; ++i) {\n        if (i == dim) continue;\n        out_sizes.push_back(sizes[i]);\n    }\n\n    auto out = at::empty(out_sizes, x.options());\n\n    // \u8ba1\u7b97\u7f51\u683c/\u5757\u7ef4\u5ea6\n    long long total = outer * inner;\n    int threads = 256;  // Tx * Ty\n    int blocks = static_cast<int>((total + threads - 1) / threads);\n    if (blocks == 0) {\n        // \u5982\u679c total == 0\uff08\u7406\u8bba\u4e0a\u4e0d\u5e94\u51fa\u73b0\uff09\uff0c\u76f4\u63a5\u8fd4\u56de\u96f6\u5927\u5c0f\u7684\u5f20\u91cf\n        return out;\n    }\n\n    // \u542f\u52a8\u5185\u6838\n    auto stream = at::cuda::getCurrentCUDAStream();\n    const float* in_ptr = x.data_ptr<float>();\n    float* out_ptr = out.data_ptr<float>();\n    max_reduce_axis_kernel<<<blocks, threads, 0, stream>>>(\n        in_ptr, out_ptr, outer, inner, red_len\n    );\n\n    // \u68c0\u67e5\u5185\u6838\u9519\u8bef\n    cudaError_t err = cudaGetLastError();\n    TORCH_CHECK(err == cudaSuccess, \"Kernel launch failed: \", cudaGetErrorString(err));\n\n    return out;\n}"
  },
  {
    "round": 5,
    "goal": "Refactor the kernel to use a larger shared memory tile (Ty=16 rows, Tx=32 columns) with a 512-thread block and 2048 bytes of static shared memory, halving the number of reduction tiles and __syncthreads() calls to streamline loop execution and boost memory throughput efficiency while keeping registers low via shared indexing logic.",
    "status": "Success (New Best)",
    "time_ms": 1.157529640197754,
    "ptxas_metrics": {
      "registers_used": 27.0,
      "shared_mem_bytes": 2048.0,
      "spill_bytes": 0.0
    },
    "all_ncu_metrics": {
      "DRAMFrequency": 1495692940.37,
      "SMFrequency": 1052182022.91,
      "ElapsedCycles": 1228652.0,
      "MemoryThroughput": 70.74,
      "DRAMThroughput": 70.74,
      "Duration": 1167200.0,
      "L1TEXCacheThroughput": 67.22,
      "L2CacheThroughput": 78.13,
      "SMActiveCycles": 1215496.73,
      "ComputeSMThroughput": 69.76,
      "BlockSize": 256.0,
      "GridSize": 2048.0,
      "RegistersPerThread": 27.0,
      "SharedMemoryConfigurationSize": 65536.0,
      "DriverSharedMemoryPerBlock": 1024.0,
      "DynamicSharedMemoryPerBlock": 0.0,
      "StaticSharedMemoryPerBlock": 2048.0,
      "Threads": 524288.0,
      "WavesPerSM": 2.37,
      "BlockLimitSM": 32.0,
      "BlockLimitRegisters": 8.0,
      "BlockLimitSharedMem": 21.0,
      "BlockLimitWarps": 8.0,
      "TheoreticalActiveWarpsperSM": 64.0,
      "TheoreticalOccupancy": 100.0,
      "AchievedOccupancy": 86.2,
      "AchievedActiveWarpsPerSM": 55.17
    },
    "selected_ncu_metrics": [
      "StaticSharedMemoryPerBlock",
      "RegistersPerThread",
      "AchievedOccupancy",
      "MemoryThroughput",
      "DRAMThroughput"
    ],
    "details": "Performance improved from 1.303 ms to 1.158 ms.",
    "code": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <cmath>\n#include <vector>\n#include <cfloat>\n// PyTorch 2.1+ \u79fb\u9664\u4e86 c10::cuda::getCurrentCUDAStream\n// \u4f7f\u7528 at::cuda::getCurrentCUDAStream() \u4ee3\u66ff\n#include <ATen/cuda/CUDAContext.h>\n\n// [\u8f85\u52a9\u51fd\u6570] \u8bbe\u5907\u7aef\u6700\u5927\u503c\u51fd\u6570\n__device__ inline float dmaxf(float a, float b) {\n    return a > b ? a : b;\n}\n\n/*\n * \u4f18\u5316\u7248 CUDA \u5185\u6838\uff1a\n *  1. Tx = 32, Ty = 16 \u21d2 tile \u8986\u76d6 512 \u4e2a\u5143\u7d20\uff082 KB shared memory\uff09\n *  2. \u4ecd\u4fdd\u6301\u6bcf block 256 \u7ebf\u7a0b\u7684 launch \u914d\u7f6e\uff1a\n *     \u6bcf\u4e2a\u7ebf\u7a0b\u8d1f\u8d23\u52a0\u8f7d\u4e24\u884c\uff08row \u4e0e row+8\uff09\u6570\u636e\u6765\u586b\u6ee1 16 \u884c tile\n *  3. \u5176\u5b83\u903b\u8f91\uff08\u7d22\u5f15\u6620\u5c04\u3001\u5f52\u7ea6\u3001\u540c\u6b65\uff09\u4fdd\u6301\u4e00\u81f4\n */\n__global__ void max_reduce_axis_kernel(const float* __restrict__ input,\n                                       float* __restrict__ output,\n                                       long long outer,\n                                       long long inner,\n                                       long long red_len) {\n    constexpr int Tx = 32;  // \u5217\u5bbd\uff08threads per warp\uff09\n    constexpr int Ty = 16;  // \u884c\u6570\uff08\u6bcf tile \u7684 reduction \u811a\u672c\u6b65\u957f\uff09\n    __shared__ float sTile[Tx * Ty];   // 32 \u00d7 16 = 512 \u00d7 4B = 2 KB\n\n    const long long total = outer * inner;\n    const long long global_tid = static_cast<long long>(blockIdx.x) * blockDim.x + threadIdx.x;\n\n    // \u7ebf\u7a0b\u5728 block \u5185\u7684 2-D \u5750\u6807\n    const int lane = threadIdx.x & (Tx - 1);  // 0 \u2026 31\n    const int row_base = threadIdx.x >> 5;    // 0 \u2026 7 \uff08\u771f\u5b9e\u884c\uff0c\u8d1f\u8d23\u5199\u4e24\u884c\uff1arow_base \u53ca row_base+8\uff09\n\n    // \u5224\u65ad\u8be5\u7ebf\u7a0b\u662f\u5426\u5bf9\u5e94\u5230\u4e00\u4e2a\u6709\u6548\u7684 (outer_idx, inner_idx)\n    const bool valid_thread = (global_tid < total);\n\n    // \u4ec5\u5f53 valid_thread \u65f6\u624d\u8ba1\u7b97\u771f\u5b9e\u7d22\u5f15\n    const long long outer_idx = valid_thread ? (global_tid / inner) : 0;\n    const long long inner_idx = valid_thread ? (global_tid % inner) : 0;\n\n    // \u57fa\u5740\uff1a\u56fa\u5b9a\u5728\u8be5\u7ebf\u7a0b\u8d1f\u8d23\u7684 (outer_idx, inner_idx) \u4e0a\uff0c\u4e0b\u6807 j \u6cbf reduction \u7ef4\u79fb\u52a8\n    const long long base_offset = outer_idx * red_len * inner + inner_idx;\n\n    float local_max = -FLT_MAX;\n\n    // tile \u6570 = ceil(red_len / Ty)\n    const long long num_tiles = (red_len + Ty - 1) / Ty;\n\n    for (long long tile_idx = 0; tile_idx < num_tiles; ++tile_idx) {\n        // \u5f53\u524d\u7ebf\u7a0b\u8d1f\u8d23\u7684\u4e24\u4e2a reduction \u7d22\u5f15\n        const long long j0 = tile_idx * Ty + row_base;       // row_base \u884c\n        const long long j1 = j0 + 8;                         // row_base + 8 \u884c\n\n        // \u534f\u540c\u5168\u5c40\u52a0\u8f7d\n        float v0 = -FLT_MAX;\n        float v1 = -FLT_MAX;\n        if (valid_thread && j0 < red_len) {\n            v0 = input[base_offset + j0 * inner];\n        }\n        if (valid_thread && j1 < red_len) {\n            v1 = input[base_offset + j1 * inner];\n        }\n\n        // \u5199\u5165\u5171\u4eab\u5185\u5b58\uff08column-major\uff09\uff1a\u884c * Tx + lane\n        sTile[row_base * Tx + lane]       = v0;\n        sTile[(row_base + 8) * Tx + lane] = v1;\n\n        __syncthreads();\n\n        // \u6bcf\u4e2a\u7ebf\u7a0b\u7eb5\u5411\u626b\u63cf\u8be5\u5217\u5b8c\u6210\u672c tile \u7684\u5f52\u7ea6\n#pragma unroll\n        for (int k = 0; k < Ty; ++k) {\n            float t = sTile[k * Tx + lane];\n            local_max = dmaxf(local_max, t);\n        }\n\n        __syncthreads();  // \u4fdd\u62a4\u4e0b\u4e00 tile \u7684\u5199\u5165\n    }\n\n    if (valid_thread) {\n        output[global_tid] = local_max;\n    }\n}\n\n// C++ Wrapper \u5b9e\u73b0\ntorch::Tensor kb_49_Max_reduction_over_a_dimension_wrapper(torch::Tensor arg0, int64_t arg1) {\n    TORCH_CHECK(arg0.is_cuda(), \"Input tensor must be on CUDA device.\");\n    TORCH_CHECK(arg0.scalar_type() == at::ScalarType::Float,\n                \"Input tensor must be float32.\");\n    TORCH_CHECK(arg0.numel() > 0, \"Input tensor must have at least one element.\");\n\n    // \u5904\u7406\u7ef4\u5ea6\u53c2\u6570\uff08\u652f\u6301\u8d1f\u7ef4\u5ea6\uff09\n    int64_t nDim = arg0.dim();\n    TORCH_CHECK(nDim >= 1, \"Input tensor must have at least 1 dimension.\");\n    int64_t dim = arg1 >= 0 ? arg1 : (arg1 + nDim);\n    TORCH_CHECK(dim >= 0 && dim < nDim, \"Reduction dim out of range.\");\n\n    // \u786e\u4fdd\u8fde\u7eed\n    auto x = arg0.contiguous();\n\n    // \u8ba1\u7b97 outer, inner, red_len\n    auto sizes = x.sizes();\n    long long outer = 1;\n    for (int64_t i = 0; i < dim; ++i) {\n        outer *= static_cast<long long>(sizes[i]);\n    }\n    long long red_len = static_cast<long long>(sizes[dim]);\n    long long inner = 1;\n    for (int64_t i = dim + 1; i < nDim; ++i) {\n        inner *= static_cast<long long>(sizes[i]);\n    }\n\n    // \u6784\u9020\u8f93\u51fa\u5c3a\u5bf8\uff08\u79fb\u9664\u5f52\u7ea6\u7ef4\uff09\n    std::vector<int64_t> out_sizes;\n    out_sizes.reserve(static_cast<size_t>(nDim - 1));\n    for (int64_t i = 0; i < nDim; ++i) {\n        if (i == dim) continue;\n        out_sizes.push_back(sizes[i]);\n    }\n\n    auto out = at::empty(out_sizes, x.options());\n\n    // \u8ba1\u7b97\u7f51\u683c/\u5757\u7ef4\u5ea6\n    long long total = outer * inner;\n    int threads = 256;  // \u4fdd\u6301 256 \u7ebf\u7a0b/\u5757\uff0c\u5185\u6838\u5185\u90e8\u6bcf\u7ebf\u7a0b\u52a0\u8f7d\u4e24\u884c\n    int blocks = static_cast<int>((total + threads - 1) / threads);\n    if (blocks == 0) {\n        // \u5982\u679c total == 0\uff08\u7406\u8bba\u4e0a\u4e0d\u5e94\u51fa\u73b0\uff09\uff0c\u76f4\u63a5\u8fd4\u56de\u96f6\u5927\u5c0f\u7684\u5f20\u91cf\n        return out;\n    }\n\n    // \u542f\u52a8\u5185\u6838\n    auto stream = at::cuda::getCurrentCUDAStream();\n    const float* in_ptr = x.data_ptr<float>();\n    float* out_ptr = out.data_ptr<float>();\n    max_reduce_axis_kernel<<<blocks, threads, 0, stream>>>(\n        in_ptr, out_ptr, outer, inner, red_len\n    );\n\n    // \u68c0\u67e5\u5185\u6838\u9519\u8bef\n    cudaError_t err = cudaGetLastError();\n    TORCH_CHECK(err == cudaSuccess, \"Kernel launch failed: \", cudaGetErrorString(err));\n\n    return out;\n}"
  }
]