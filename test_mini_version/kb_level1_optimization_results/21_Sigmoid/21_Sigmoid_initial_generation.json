[
  {
    "attempt": 0,
    "type": "generation",
    "response_cuda_code": "import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\nsource = r'''\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n\n////////////////////////////////////////////////////////////////////////////////\n// CUDA kernel : element-wise Sigmoid\n////////////////////////////////////////////////////////////////////////////////\ntemplate<typename scalar_t>\n__global__ void sigmoid_kernel(const scalar_t* __restrict__ inp,\n                               scalar_t* __restrict__ out,\n                               const int64_t numel) {\n    // grid-stride loop so arbitrary tensor sizes are handled\n    for (int64_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n         idx < numel;\n         idx += gridDim.x * blockDim.x) {\n\n        scalar_t x = inp[idx];\n        out[idx] = static_cast<scalar_t>(1.0) /\n                   (static_cast<scalar_t>(1.0) + exp(-x));\n    }\n}\n\n////////////////////////////////////////////////////////////////////////////////\n// Host wrapper\n////////////////////////////////////////////////////////////////////////////////\ntorch::Tensor sigmoid_forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must reside on CUDA device.\");\n    TORCH_CHECK(x.scalar_type() == torch::kFloat32,\n                \"Only float32 tensors are supported.\");\n\n    auto y = torch::empty_like(x);\n    const int64_t numel = x.numel();\n\n    const int threads = 256;\n    const int blocks  = (numel + threads - 1) / threads;\n\n    AT_DISPATCH_FLOATING_TYPES(x.scalar_type(), \"sigmoid_cuda\", ([&](){\n        sigmoid_kernel<scalar_t><<<blocks, threads>>>(x.data_ptr<scalar_t>(),\n                                                      y.data_ptr<scalar_t>(),\n                                                      numel);\n    }));\n\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess) {\n        throw std::runtime_error(std::string(\"CUDA kernel failed with error: \") +\n                                 cudaGetErrorString(err));\n    }\n    return y;\n}\n'''\n\ncpp_src = r'''\ntorch::Tensor sigmoid_forward(torch::Tensor x);\n'''\n\nsigmoid_cuda = load_inline(\n    name='sigmoid_cuda',\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=['sigmoid_forward'],\n    with_cuda=True,\n    extra_cuda_cflags=['-O2', '--ptxas-options=-v'],\n    verbose=True\n)\n\nclass ModelNew(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.sigmoid = sigmoid_cuda\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.sigmoid.sigmoid_forward(x)"
  }
]