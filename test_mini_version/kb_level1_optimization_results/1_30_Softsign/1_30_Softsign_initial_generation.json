[
  {
    "attempt": 0,
    "type": "generation",
    "response_cuda_code": "import torch\nimport torch.nn as nn\nfrom torch.utils.cpp_extension import load_inline\n\nsource = r'''\n#include <torch/extension.h>\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <math.h>\n\n////////////////////////////////////////////////////////////////\n// Element-wise Softsign: y = x / (1 + |x|)\n////////////////////////////////////////////////////////////////\ntemplate <typename scalar_t>\n__global__ void softsign_kernel(const scalar_t* __restrict__ x,\n                                scalar_t* __restrict__ y,\n                                const int64_t N)\n{\n    // flat thread id\n    int64_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int64_t stride = blockDim.x * gridDim.x;\n\n    for (; idx < N; idx += stride) {\n        scalar_t val = x[idx];\n        scalar_t denom = scalar_t(1.0) + abs(val);\n        y[idx] = val / denom;\n    }\n}\n\ntorch::Tensor softsign_forward(torch::Tensor x) {\n    TORCH_CHECK(x.is_cuda(), \"Input must be a CUDA tensor\");\n    TORCH_CHECK(x.scalar_type() == at::kFloat, \"Only float32 tensors are supported\");\n    \n    auto x_contig = x.contiguous();\n    auto y = torch::empty_like(x_contig);\n\n    const int64_t N = x_contig.numel();\n    const int threads = 256;\n    const int blocks = (N + threads - 1) / threads;\n\n    AT_DISPATCH_FLOATING_TYPES_AND_HALF(x_contig.scalar_type(), \"softsign_kernel_launch\", ([&] {\n        softsign_kernel<scalar_t><<<blocks, threads>>>(\n            x_contig.data_ptr<scalar_t>(),\n            y.data_ptr<scalar_t>(),\n            N);\n    }));\n    \n    cudaError_t err = cudaGetLastError();\n    TORCH_CHECK(err == cudaSuccess, \"CUDA kernel failed : \", cudaGetErrorString(err));\n\n    return y;\n}\n'''\n\ncpp_src = r'''\ntorch::Tensor softsign_forward(torch::Tensor x);\n'''\n\nsoftsign_module = load_inline(\n    name='softsign_module',\n    cpp_sources=cpp_src,\n    cuda_sources=source,\n    functions=['softsign_forward'],\n    with_cuda=True,\n    verbose=True,\n    extra_cuda_cflags=['-O3', '--ptxas-options=-v']\n)\n\nclass ModelNew(nn.Module):\n    \"\"\"\n    CUDA-accelerated Softsign model.\n    Mirrors the original Model interface.\n    \"\"\"\n    def __init__(self):\n        super(ModelNew, self).__init__()\n        # no parameters to initialize\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return softsign_module.softsign_forward(x)"
  }
]