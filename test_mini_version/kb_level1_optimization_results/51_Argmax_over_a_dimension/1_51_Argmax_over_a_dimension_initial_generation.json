[
  {
    "attempt": 0,
    "type": "generation",
    "response": "--- C++ 签名 (`cpp`) ---\n```cpp\n#include <torch/extension.h>\n\n// C++ Wrapper 函数声明 (签名)\ntorch::Tensor kb_1_51_Argmax_over_a_dimension_wrapper(torch::Tensor arg0, int64_t arg1);\n```\n\n--- C++/CUDA 组合实现 (`cu`) ---\n```cu\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <ATen/cuda/CUDAContext.h>\n#include <vector>\n#include <cfloat>     // FLT_MAX\n\n//=======================================================================\n// CUDA Kernel : 3-D tensor argmax along an arbitrary dimension (0/1/2)\n// Each thread produces exactly one output element (i.e. one argmax index)\n//=======================================================================\n__global__ void argmax_dim3_kernel(const float* __restrict__ input,\n                                   int64_t* __restrict__ output,\n                                   int64_t size0, int64_t size1, int64_t size2,\n                                   int      reduce_dim)\n{\n    /*  Shape convention\n          dim-0 : size0\n          dim-1 : size1\n          dim-2 : size2\n    */\n    size_t out_elements = 0;\n    if (reduce_dim == 0)\n        out_elements = size1 * size2;\n    else if (reduce_dim == 1)\n        out_elements = size0 * size2;\n    else /* reduce_dim == 2 */\n        out_elements = size0 * size1;\n\n    size_t out_idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (out_idx >= out_elements) return;\n\n    float  max_val  = -FLT_MAX;\n    int64_t max_idx = 0;\n\n    if (reduce_dim == 0) {\n        // --- map flat output index -> (i1, i2)\n        int64_t i1 = out_idx / size2;\n        int64_t i2 = out_idx % size2;\n\n        // --- scan along dim-0\n        for (int64_t r = 0; r < size0; ++r) {\n            float val = input[r * size1 * size2 + i1 * size2 + i2];\n            if (val > max_val) {\n                max_val  = val;\n                max_idx  = r;\n            }\n        }\n    }\n    else if (reduce_dim == 1) {   //  ***   default path for the provided example   ***\n        // --- map flat output index -> (i0, i2)\n        int64_t i0 = out_idx / size2;\n        int64_t i2 = out_idx % size2;\n\n        // --- scan along dim-1\n        for (int64_t r = 0; r < size1; ++r) {\n            float val = input[i0 * size1 * size2 + r * size2 + i2];\n            if (val > max_val) {\n                max_val  = val;\n                max_idx  = r;\n            }\n        }\n    }\n    else { // reduce_dim == 2\n        // --- map flat output index -> (i0, i1)\n        int64_t i0 = out_idx / size1;\n        int64_t i1 = out_idx % size1;\n\n        // --- scan along dim-2\n        for (int64_t r = 0; r < size2; ++r) {\n            float val = input[i0 * size1 * size2 + i1 * size2 + r];\n            if (val > max_val) {\n                max_val  = val;\n                max_idx  = r;\n            }\n        }\n    }\n\n    // write back\n    output[out_idx] = max_idx;\n}\n\n//=======================================================================\n// C++ Wrapper\n//=======================================================================\ntorch::Tensor kb_1_51_Argmax_over_a_dimension_wrapper(torch::Tensor arg0, int64_t arg1) {\n    // -------------------- sanity checks --------------------\n    TORCH_CHECK(arg0.is_cuda(),   \"Input tensor must reside on CUDA device\");\n    TORCH_CHECK(arg0.scalar_type() == at::kFloat,\n                \"Input tensor must be of type float32\");\n    TORCH_CHECK(arg0.dim() == 3,\n                \"This implementation supports 3-D tensors only (got \",\n                arg0.dim(), \")\");\n\n    int64_t reduce_dim = arg1;\n    TORCH_CHECK(reduce_dim >= 0 && reduce_dim < 3,\n                \"reduce_dim must be 0, 1 or 2 (got \", reduce_dim, \")\");\n\n    // Make sure the tensor is contiguous for simple stride math\n    auto input = arg0.contiguous();\n\n    const auto sizes = input.sizes();\n    int64_t size0 = sizes[0];\n    int64_t size1 = sizes[1];\n    int64_t size2 = sizes[2];\n\n    // -------------------- create output tensor --------------------\n    std::vector<int64_t> out_sizes;\n    if (reduce_dim == 0)\n        out_sizes = {size1, size2};\n    else if (reduce_dim == 1)\n        out_sizes = {size0, size2};\n    else // reduce_dim == 2\n        out_sizes = {size0, size1};\n\n    auto output = torch::empty(out_sizes,\n                               torch::dtype(torch::kInt64)\n                               .device(arg0.device()));\n\n    // -------------------- launch kernel --------------------\n    const int threads = 256;\n    int64_t num_out_elements = output.numel();\n    const int blocks = static_cast<int>((num_out_elements + threads - 1) / threads);\n\n    // current CUDA stream\n    cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n\n    argmax_dim3_kernel<<<blocks, threads, 0, stream>>>(\n        input.data_ptr<float>(),\n        output.data_ptr<int64_t>(),\n        size0, size1, size2,\n        static_cast<int>(reduce_dim)\n    );\n\n    // optional: check for launch errors (debug builds)\n#ifndef NDEBUG\n    cudaError_t err = cudaGetLastError();\n    TORCH_CHECK(err == cudaSuccess, \"CUDA kernel failed : \", cudaGetErrorString(err));\n#endif\n\n    return output;\n}\n```\n",
    "cpp_code_extracted": true,
    "cuda_code_extracted": true,
    "error": ""
  }
]