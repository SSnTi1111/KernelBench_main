[
  {
    "round": 0,
    "goal": "Baseline",
    "status": "Success",
    "time_ms": 5.573734283447266,
    "ptxas_metrics": {
      "registers_used": 30.0,
      "shared_mem_bytes": 0.0,
      "spill_bytes": 0.0
    },
    "all_ncu_metrics": {
      "DRAMFrequency": 1511938173.22,
      "SMFrequency": 1064771463.35,
      "ElapsedCycles": 5934659.0,
      "MemoryThroughput": 83.58,
      "DRAMThroughput": 83.58,
      "Duration": 5573248.0,
      "L1TEXCacheThroughput": 23.38,
      "L2CacheThroughput": 87.57,
      "SMActiveCycles": 5863893.56,
      "ComputeSMThroughput": 35.38,
      "BlockSize": 256.0,
      "GridSize": 2048.0,
      "RegistersPerThread": 30.0,
      "SharedMemoryConfigurationSize": 32768.0,
      "DriverSharedMemoryPerBlock": 1024.0,
      "DynamicSharedMemoryPerBlock": 0.0,
      "StaticSharedMemoryPerBlock": 0.0,
      "Threads": 524288.0,
      "WavesPerSM": 2.37,
      "BlockLimitSM": 32.0,
      "BlockLimitRegisters": 8.0,
      "BlockLimitSharedMem": 32.0,
      "BlockLimitWarps": 8.0,
      "TheoreticalActiveWarpsperSM": 64.0,
      "TheoreticalOccupancy": 100.0,
      "AchievedOccupancy": 88.8,
      "AchievedActiveWarpsPerSM": 56.83
    },
    "selected_ncu_metrics": [],
    "details": "Initial baseline measurement",
    "code": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <cmath>\n#include <vector>\n#include <cstdint>\n\n// PyTorch \u22652.1 \u9700\u4f7f\u7528 at::cuda::getCurrentCUDAStream()\n#include <ATen/cuda/CUDAContext.h>\n\n/*\n * ---------------------------------------------------------------------------\n * \uff08\u53ef\u9009\uff09CUDA \u8f85\u52a9\u51fd\u6570\u793a\u4f8b\uff08\u6b64\u793a\u4f8b\u672a\u5728\u5f53\u524d kernel \u4e2d\u4f7f\u7528\uff0c\u4fdd\u7559\u4f5c\u6a21\u677f\uff09\n * ---------------------------------------------------------------------------\n */\n__device__ float blockReduceSum(float val, float* shared)\n{\n    const unsigned int lane = threadIdx.x & 0x1f;      // 0\u201331\n    const unsigned int wid  = threadIdx.x >> 5;        // warp ID\n\n    // \u2500\u2500 warp \u5185\u6c42\u548c \u2500\u2500\n    for (int offset = warpSize >> 1; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n\n    // \u6bcf\u4e2a warp \u7684\u7b2c\u4e00\u4e2a\u7ebf\u7a0b\u628a\u7ed3\u679c\u5199\u5230 shared memory\n    if (lane == 0) shared[wid] = val;\n    __syncthreads();\n\n    // \u2500\u2500 \u7b2c\u4e00\u4e2a warp \u7ee7\u7eed\u5bf9\u6240\u6709 warp \u7684\u90e8\u5206\u548c\u505a\u5f52\u7ea6 \u2500\u2500\n    val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0.f;\n    if (wid == 0) {\n        for (int offset = warpSize >> 1; offset > 0; offset >>= 1)\n            val += __shfl_down_sync(0xffffffff, val, offset);\n    }\n    return val;\n}\n\n/*\n * ---------------------------------------------------------------------------\n * Argmin kernel : \u5bf9\u8f93\u5165\u5f20\u91cf\u5728\u6307\u5b9a\u7ef4\u5ea6\u6267\u884c argmin\n * \u4ec5\u652f\u6301\u8fde\u7eed (contiguous) Tensor\u3002\n * \u7ef4\u5ea6\u62c6\u5206:\n *    input shape  : [outer_size, reduce_size, inner_size]\n *    output shape : [outer_size, inner_size]\n * \u6bcf\u4e2a\u7ebf\u7a0b\u5904\u7406\u4e00\u4e2a (outer_idx, inner_idx) \u5bf9\uff0c\u5e94\u5728 reduce_size \u4e0a\u987a\u5e8f\u626b\u63cf\u3002\n * ---------------------------------------------------------------------------\n */\ntemplate <typename scalar_t>\n__global__ void argmin_dim_kernel(const scalar_t* __restrict__ input,\n                                  int64_t* __restrict__ output,\n                                  int64_t outer_size,\n                                  int64_t reduce_size,\n                                  int64_t inner_size,\n                                  int64_t total_threads)\n{\n    const int64_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid >= total_threads) return;\n\n    // \u53cd\u7b97\u5750\u6807\n    const int64_t outer_idx = tid / inner_size;\n    const int64_t inner_idx = tid % inner_size;\n\n    // \u8be5 (outer_idx, inner_idx) \u5bf9\u5e94\u5230 input \u4e2d\u7684\u8d77\u59cb\u4f4d\u7f6e\n    int64_t base_offset = (outer_idx * reduce_size * inner_size) + inner_idx;\n\n    scalar_t min_val  = input[base_offset]; // r == 0\n    int64_t  min_idx  = 0;\n\n    // \u987a\u5e8f\u626b\u63cf reduce \u7ef4\n    for (int64_t r = 1; r < reduce_size; ++r) {\n        scalar_t val = input[base_offset + r * inner_size];\n        if (val < min_val) {\n            min_val = val;\n            min_idx = r;\n        }\n    }\n\n    output[tid] = min_idx; // \u5199\u5165 argmin \u7ed3\u679c\n}\n\n/*\n * ---------------------------------------------------------------------------\n * C++ \u5305\u88c5\u51fd\u6570\n * ---------------------------------------------------------------------------\n */\ntorch::Tensor kb_1_52_Argmin_over_a_dimension_wrapper(torch::Tensor arg0,\n                                                      int64_t      dim_in)\n{\n    TORCH_CHECK(arg0.is_cuda(), \"Input tensor must be on CUDA\");\n    TORCH_CHECK(arg0.is_contiguous(),\n                \"Only contiguous tensors are supported for this op\");\n\n    // \u5904\u7406\u8d1f dim\n    int64_t dim = dim_in;\n    if (dim < 0) dim += arg0.dim();\n    TORCH_CHECK(dim >= 0 && dim < arg0.dim(),\n                \"Reduction dim is out of bounds\");\n\n    // \u8ba1\u7b97 outer / reduce / inner size\n    const auto sizes = arg0.sizes();\n    int64_t outer_size  = 1;\n    int64_t inner_size  = 1;\n    int64_t reduce_size = sizes[dim];\n\n    for (int64_t i = 0; i < dim; ++i) outer_size *= sizes[i];\n    for (int64_t i = dim + 1; i < arg0.dim(); ++i) inner_size *= sizes[i];\n\n    // \u8f93\u51fa\u5c3a\u5bf8 = \u8f93\u5165\u5c3a\u5bf8\u79fb\u9664 reduce \u7ef4\n    std::vector<int64_t> out_sizes;\n    out_sizes.reserve(arg0.dim() - 1);\n    for (int64_t i = 0; i < arg0.dim(); ++i)\n        if (i != dim) out_sizes.push_back(sizes[i]);\n\n    torch::Tensor output = torch::empty(\n        out_sizes,\n        torch::TensorOptions()\n            .dtype(torch::kInt64)\n            .device(arg0.device()));\n\n    // \u8c03\u5ea6 kernel\n    const int64_t total_threads = outer_size * inner_size;\n    const int threads = 256;\n    const int blocks  = (total_threads + threads - 1) / threads;\n\n    AT_DISPATCH_FLOATING_TYPES(arg0.scalar_type(), \"argmin_dim_kernel\", ([&] {\n        argmin_dim_kernel<scalar_t>\n            <<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(\n                arg0.data_ptr<scalar_t>(),\n                output.data_ptr<int64_t>(),\n                outer_size,\n                reduce_size,\n                inner_size,\n                total_threads);\n    }));\n\n    // CUDA \u5185\u6838\u9519\u8bef\u68c0\u67e5\uff08\u53ef\u9009\uff09\n    cudaError_t err = cudaGetLastError();\n    TORCH_CHECK(err == cudaSuccess, \"CUDA kernel failed : \",\n                cudaGetErrorString(err));\n\n    return output;\n}"
  },
  {
    "round": 1,
    "goal": "Implement shared memory tiling to enable coalesced global loads and contiguous access patterns for the argmin reduction.\nOptimization History:\nPrevious Optimization Attempts:\n  Round 0:\n    Goal: Baseline\n    Status: Success\n    Performance: 5.574 ms\n    Registers: 30.0\n    Shared Mem: 0.0 bytes",
    "status": "Failed (Performance Regression)",
    "time_ms": 95.08311157226562,
    "ptxas_metrics": {
      "registers_used": 32.0,
      "shared_mem_bytes": 3072.0,
      "spill_bytes": 0.0
    },
    "all_ncu_metrics": {
      "DRAMFrequency": 1507214939.5,
      "SMFrequency": 1061620948.17,
      "ElapsedCycles": 99618187.0,
      "MemoryThroughput": 60.26,
      "DRAMThroughput": 60.26,
      "Duration": 93835520.0,
      "L1TEXCacheThroughput": 20.52,
      "L2CacheThroughput": 80.02,
      "SMActiveCycles": 96816255.96,
      "ComputeSMThroughput": 3.44,
      "BlockSize": 256.0,
      "GridSize": 2048.0,
      "RegistersPerThread": 32.0,
      "SharedMemoryConfigurationSize": 65536.0,
      "DriverSharedMemoryPerBlock": 1024.0,
      "DynamicSharedMemoryPerBlock": 0.0,
      "StaticSharedMemoryPerBlock": 3072.0,
      "Threads": 524288.0,
      "WavesPerSM": 2.37,
      "BlockLimitSM": 32.0,
      "BlockLimitRegisters": 8.0,
      "BlockLimitSharedMem": 16.0,
      "BlockLimitWarps": 8.0,
      "TheoreticalActiveWarpsperSM": 64.0,
      "TheoreticalOccupancy": 100.0,
      "AchievedOccupancy": 97.58,
      "AchievedActiveWarpsPerSM": 62.45
    },
    "selected_ncu_metrics": [
      "SharedMemoryConfigurationSize",
      "StaticSharedMemoryPerBlock",
      "DynamicSharedMemoryPerBlock",
      "DRAMThroughput",
      "L1TEXCacheThroughput"
    ],
    "details": "New time 95.083 ms is not better than best time 5.574 ms.",
    "code": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <cmath>\n#include <vector>\n#include <cstdint>\n#include <limits>\n\n// PyTorch \u22652.1 \u9700\u4f7f\u7528 at::cuda::getCurrentCUDAStream()\n#include <ATen/cuda/CUDAContext.h>\n\n/*\n * ---------------------------------------------------------------------------\n * \uff08\u53ef\u9009\uff09CUDA \u8f85\u52a9\u51fd\u6570\u793a\u4f8b\uff08\u6b64\u793a\u4f8b\u672a\u5728\u5f53\u524d kernel \u4e2d\u4f7f\u7528\uff0c\u4fdd\u7559\u4f5c\u6a21\u677f\uff09\n * ---------------------------------------------------------------------------\n */\n__device__ float blockReduceSum(float val, float* shared)\n{\n    const unsigned int lane = threadIdx.x & 0x1f;      // 0\u201331\n    const unsigned int wid  = threadIdx.x >> 5;        // warp ID\n\n    // \u2500\u2500 warp \u5185\u6c42\u548c \u2500\u2500\n    for (int offset = warpSize >> 1; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n\n    // \u6bcf\u4e2a warp \u7684\u7b2c\u4e00\u4e2a\u7ebf\u7a0b\u628a\u7ed3\u679c\u5199\u5230 shared memory\n    if (lane == 0) shared[wid] = val;\n    __syncthreads();\n\n    // \u2500\u2500 \u7b2c\u4e00\u4e2a warp \u7ee7\u7eed\u5bf9\u6240\u6709 warp \u7684\u90e8\u5206\u548c\u505a\u5f52\u7ea6 \u2500\u2500\n    val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0.f;\n    if (wid == 0) {\n        for (int offset = warpSize >> 1; offset > 0; offset >>= 1)\n            val += __shfl_down_sync(0xffffffff, val, offset);\n    }\n    return val;\n}\n\n/*\n * ---------------------------------------------------------------------------\n * Argmin kernel : \u5bf9\u8f93\u5165\u5f20\u91cf\u5728\u6307\u5b9a\u7ef4\u5ea6\u6267\u884c argmin\n * \u65b0\u7248\u672c\uff1a\u540c\u4e00 thread-block \u5408\u4f5c\u5b8c\u6210\u4e00\u6b21 argmin \u8ba1\u7b97\u3002\n * \u7ef4\u5ea6\u62c6\u5206:\n *    input shape  : [outer_size, reduce_size, inner_size]\n *    output shape : [outer_size, inner_size]\n *\n * \u6bcf\u4e2a block \u5904\u7406\u82e5\u5e72\u4e2a (outer_idx, inner_idx) \u5bf9\uff08grid-stride loop\uff09\u3002\n * \u6bcf\u6b21\u53ea\u9488\u5bf9\u4e00\u4e2a\u8f93\u51fa\u5143\u7d20\uff0cblock \u5185\u6240\u6709\u7ebf\u7a0b\u534f\u540c\u5b8c\u6210 reduce \u7ef4\u5ea6\u626b\u63cf\u3002\n * ---------------------------------------------------------------------------\n */\ntemplate <typename scalar_t>\n__global__ void argmin_dim_kernel(const scalar_t* __restrict__ input,\n                                  int64_t* __restrict__ output,\n                                  int64_t outer_size,\n                                  int64_t reduce_size,\n                                  int64_t inner_size,\n                                  int64_t total_outputs)\n{\n    // block-level shared memory\uff08\u9759\u6001\u5927\u5c0f\uff0c\u8db3\u591f\u5bb9\u7eb3 256 \u7ebf\u7a0b\u5757\uff09\n    __shared__ scalar_t s_val[256];\n    __shared__ int64_t  s_idx[256];\n\n    const int tid = threadIdx.x;\n\n    // grid-stride \u5904\u7406\u591a\u4e2a\u8f93\u51fa\u5143\u7d20\uff0c\u4fdd\u8bc1\u6240\u6709\u8f93\u51fa\u90fd\u88ab\u8986\u76d6\n    for (int64_t out_idx = blockIdx.x; out_idx < total_outputs; out_idx += gridDim.x) {\n        // \u53cd\u7b97 (outer, inner) \u5750\u6807\n        const int64_t outer_idx = out_idx / inner_size;\n        const int64_t inner_idx = out_idx % inner_size;\n\n        // \u8d77\u59cb\u504f\u79fb\n        const int64_t base_offset = (outer_idx * reduce_size * inner_size) + inner_idx;\n\n        // \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 1. \u7ebf\u7a0b\u79c1\u6709\u5c40\u90e8\u626b\u63cf \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        scalar_t local_min = std::numeric_limits<scalar_t>::max();\n        int64_t  local_idx = -1;\n\n        for (int64_t r = tid; r < reduce_size; r += blockDim.x) {\n            scalar_t val = input[base_offset + r * inner_size];\n            if (val < local_min) {\n                local_min = val;\n                local_idx = r;\n            }\n        }\n\n        // \u5199\u5165 shared memory\uff0c\u51c6\u5907 block \u7ea7\u5f52\u7ea6\n        s_val[tid] = local_min;\n        s_idx[tid] = local_idx;\n        __syncthreads();\n\n        // \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 2. block \u5185\u5e76\u884c\u5f52\u7ea6\uff08argmin\uff09 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        for (int stride = blockDim.x >> 1; stride > 0; stride >>= 1) {\n            if (tid < stride) {\n                if (s_val[tid + stride] < s_val[tid]) {\n                    s_val[tid] = s_val[tid + stride];\n                    s_idx[tid] = s_idx[tid + stride];\n                }\n            }\n            __syncthreads();\n        }\n\n        // \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 3. \u5199\u56de\u7ed3\u679c \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        if (tid == 0) {\n            output[out_idx] = s_idx[0];\n        }\n        __syncthreads();  // \u786e\u4fdd\u6240\u6709\u7ebf\u7a0b\u5728\u8fdb\u5165\u4e0b\u4e00\u6b21\u5faa\u73af\u524d\u5b8c\u6210\n    }\n}\n\n/*\n * ---------------------------------------------------------------------------\n * C++ \u5305\u88c5\u51fd\u6570\n * ---------------------------------------------------------------------------\n */\ntorch::Tensor kb_1_52_Argmin_over_a_dimension_wrapper(torch::Tensor arg0,\n                                                      int64_t      dim_in)\n{\n    TORCH_CHECK(arg0.is_cuda(), \"Input tensor must be on CUDA\");\n    TORCH_CHECK(arg0.is_contiguous(),\n                \"Only contiguous tensors are supported for this op\");\n\n    // \u5904\u7406\u8d1f dim\n    int64_t dim = dim_in;\n    if (dim < 0) dim += arg0.dim();\n    TORCH_CHECK(dim >= 0 && dim < arg0.dim(),\n                \"Reduction dim is out of bounds\");\n\n    // \u8ba1\u7b97 outer / reduce / inner size\n    const auto sizes = arg0.sizes();\n    int64_t outer_size  = 1;\n    int64_t inner_size  = 1;\n    int64_t reduce_size = sizes[dim];\n\n    for (int64_t i = 0; i < dim; ++i) outer_size *= sizes[i];\n    for (int64_t i = dim + 1; i < arg0.dim(); ++i) inner_size *= sizes[i];\n\n    // \u8f93\u51fa\u5c3a\u5bf8 = \u8f93\u5165\u5c3a\u5bf8\u79fb\u9664 reduce \u7ef4\n    std::vector<int64_t> out_sizes;\n    out_sizes.reserve(arg0.dim() - 1);\n    for (int64_t i = 0; i < arg0.dim(); ++i)\n        if (i != dim) out_sizes.push_back(sizes[i]);\n\n    torch::Tensor output = torch::empty(\n        out_sizes,\n        torch::TensorOptions()\n            .dtype(torch::kInt64)\n            .device(arg0.device()));\n\n    // \u8c03\u5ea6 kernel\n    const int64_t total_threads = outer_size * inner_size;\n    const int threads = 256;\n    const int blocks  = (total_threads + threads - 1) / threads;\n\n    AT_DISPATCH_FLOATING_TYPES(arg0.scalar_type(), \"argmin_dim_kernel\", ([&] {\n        argmin_dim_kernel<scalar_t>\n            <<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(\n                arg0.data_ptr<scalar_t>(),\n                output.data_ptr<int64_t>(),\n                outer_size,\n                reduce_size,\n                inner_size,\n                total_threads);\n    }));\n\n    // CUDA \u5185\u6838\u9519\u8bef\u68c0\u67e5\uff08\u53ef\u9009\uff09\n    cudaError_t err = cudaGetLastError();\n    TORCH_CHECK(err == cudaSuccess, \"CUDA kernel failed : \",\n                cudaGetErrorString(err));\n\n    return output;\n}"
  },
  {
    "round": 2,
    "goal": "Implement a shared memory transpose stage in the kernel where threads in a block cooperatively load contiguous chunks of the input data into shared memory, rearranging it into a transposed layout to convert the strided argmin accesses into contiguous shared memory reads.\nOptimization History:\nPrevious Optimization Attempts:\n  Round 0:\n    Goal: Baseline\n    Status: Success\n    Performance: 5.574 ms\n    Registers: 30.0\n    Shared Mem: 0.0 bytes\n  Round 1:\n    Goal: Implement shared memory tiling to enable coalesced global loads and contiguous access patterns for the argmin reduction.\n    Status: Failed (Performance Regression)\n    Performance: 95.083 ms\n    Registers: 32.0\n    Shared Mem: 3072.0 bytes\n    Selected NCU Metrics (for Goal):\n      - SharedMemoryConfigurationSize: 65536.0\n      - StaticSharedMemoryPerBlock: 3072.0\n      - DynamicSharedMemoryPerBlock: 0.0\n      - DRAMThroughput: 60.26\n      - L1TEXCacheThroughput: 20.52",
    "status": "Failed (Performance Regression)",
    "time_ms": 11.466751861572266,
    "ptxas_metrics": {
      "registers_used": 20.0,
      "shared_mem_bytes": 4224.0,
      "spill_bytes": 0.0
    },
    "all_ncu_metrics": {
      "DRAMFrequency": 1511456821.28,
      "SMFrequency": 1064564329.62,
      "ElapsedCycles": 12203914.0,
      "MemoryThroughput": 38.88,
      "DRAMThroughput": 38.88,
      "Duration": 11463424.0,
      "L1TEXCacheThroughput": 16.73,
      "L2CacheThroughput": 53.83,
      "SMActiveCycles": 12151174.85,
      "ComputeSMThroughput": 46.6,
      "BlockSize": 256.0,
      "GridSize": 16384.0,
      "RegistersPerThread": 20.0,
      "SharedMemoryConfigurationSize": 102400.0,
      "DriverSharedMemoryPerBlock": 1024.0,
      "DynamicSharedMemoryPerBlock": 0.0,
      "StaticSharedMemoryPerBlock": 4224.0,
      "Threads": 4194304.0,
      "WavesPerSM": 18.96,
      "BlockLimitSM": 32.0,
      "BlockLimitRegisters": 10.0,
      "BlockLimitSharedMem": 19.0,
      "BlockLimitWarps": 8.0,
      "TheoreticalActiveWarpsperSM": 64.0,
      "TheoreticalOccupancy": 100.0,
      "AchievedOccupancy": 97.99,
      "AchievedActiveWarpsPerSM": 62.71
    },
    "selected_ncu_metrics": [
      "SharedMemoryConfigurationSize",
      "StaticSharedMemoryPerBlock",
      "DynamicSharedMemoryPerBlock",
      "DRAMThroughput",
      "L1TEXCacheThroughput"
    ],
    "details": "New time 11.467 ms is not better than best time 5.574 ms.",
    "code": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <cmath>\n#include <vector>\n#include <cstdint>\n#include <limits>\n\n// PyTorch \u22652.1 \u9700\u4f7f\u7528 at::cuda::getCurrentCUDAStream()\n#include <ATen/cuda/CUDAContext.h>\n\n/*\n * ---------------------------------------------------------------------------\n * Argmin kernel : \u5bf9\u8f93\u5165\u5f20\u91cf\u5728\u6307\u5b9a\u7ef4\u5ea6\u6267\u884c argmin\n * \u65b0\u7248\u5b9e\u73b0:\n *   \u2013 blockDim : (TILE_C  , TILE_RLD) = (32,8)  \u2192 256 threads / block\n *   \u2013 gridDim  : (ceil(inner/TILE_C), outer)\n *   \u2013 \u6bcf\u4e2a\u7ebf\u7a0b\u5757\u4e00\u6b21\u5904\u7406 (TILE_C) \u4e2a inner columns\uff0c\u7ebf\u7a0b\u7ec4\u5728 reduce \u7ef4\n *     \u4ee5 TILE_R (=32) \u4e3a\u6b65\u957f\u5bf9\u8f93\u5165\u505a\u5206\u5757\u52a0\u8f7d\u5e76\u5728 shared memory \u4e2d\u505a\u5f52\u7ea6\u3002\n * ---------------------------------------------------------------------------\n */\nconstexpr int TILE_C   = 32; // \u6bcf\u5757\u5904\u7406\u7684 inner \u65b9\u5411\u5217\u6570\nconstexpr int TILE_R   = 32; // \u6bcf\u6b21\u52a0\u8f7d\uff0f\u5f52\u7ea6\u7684 reduce \u7ef4\u6df1\u5ea6\nconstexpr int TILE_RLD =  8; // blockDim.y   (32*8 = 256 threads)\n\ntemplate <typename scalar_t>\n__global__ void argmin_dim_kernel(const scalar_t* __restrict__ input,\n                                  int64_t*       __restrict__ output,\n                                  int64_t outer_size,\n                                  int64_t reduce_size,\n                                  int64_t inner_size)\n{\n    // \u5c06 1-D \u7ebf\u7a0b\u7d22\u5f15\u62c6\u5206\u6210 (tx, ty)\n    const int tx = threadIdx.x;          // 0 \u2026 TILE_C-1  \u2192 column owner\n    const int ty = threadIdx.y;          // 0 \u2026 TILE_RLD-1 \u2192 cooperative loader\n\n    const int64_t outer_idx        = blockIdx.y;\n    const int64_t inner_tile_base  = static_cast<int64_t>(blockIdx.x) * TILE_C + tx;\n\n    if (inner_tile_base >= inner_size) return; // \u8fb9\u754c\u68c0\u67e5\n\n    // block-scope shared memory : \u505a\u8f6c\u7f6e\u907f\u514d bank conflict (+1 \u884c padding)\n    __shared__ scalar_t s_tile[TILE_C][TILE_R + 1];\n\n    scalar_t min_val = std::numeric_limits<scalar_t>::infinity();\n    int64_t  min_idx = 0;\n\n    // \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    // \u9010\u5757\u5904\u7406 reduce \u7ef4\n    // \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    for (int64_t r_base = 0; r_base < reduce_size; r_base += TILE_R)\n    {\n        // \u2014\u2014 cooperative global \u2192 shared load (\u5e26\u8f6c\u7f6e) \u2014\u2014\n        for (int r_local = ty; r_local < TILE_R && (r_base + r_local) < reduce_size; r_local += TILE_RLD)\n        {\n            int64_t r_global  = r_base + r_local;\n            int64_t g_offset  = ((outer_idx * reduce_size + r_global) * inner_size) + inner_tile_base;\n            s_tile[tx][r_local] = input[g_offset];  // \u5168\u5c40\u5185\u5b58\u662f\u5217\u5411\u91cf\uff0c\u8bfb\u53d6\u65f6\u5929\u7136\u5408\u5e76\n        }\n        __syncthreads();   // \u8ba9\u6240\u6709 TILE_R \u884c\u90fd\u5199\u5b8c\n\n        // \u2014\u2014 \u4ec5 ty==0 \u7684\u7ebf\u7a0b\u505a\u672c\u5217 (tx) \u7684\u5f52\u7ea6 \u2014\u2014\n        if (ty == 0)\n        {\n            #pragma unroll\n            for (int r = 0; r < TILE_R && (r_base + r) < reduce_size; ++r)\n            {\n                scalar_t v = s_tile[tx][r];\n                if (v < min_val)\n                {\n                    min_val = v;\n                    min_idx = r_base + r;\n                }\n            }\n        }\n        __syncthreads();   // \u4e0b\u4e00\u8f6e\u52a0\u8f7d\u524d\u786e\u4fdd shared memory \u4e0d\u88ab\u5360\u7528\n    }\n\n    // \u2014\u2014 \u5199\u56de\u7ed3\u679c (\u6bcf\u5217\u53ea\u5199\u4e00\u6b21) \u2014\u2014\n    if (ty == 0)\n    {\n        int64_t out_offset = outer_idx * inner_size + inner_tile_base;\n        output[out_offset] = min_idx;\n    }\n}\n\n/*\n * ---------------------------------------------------------------------------\n * C++ \u5305\u88c5\u51fd\u6570 (\u672a\u6539\u52a8\u63a5\u53e3\uff0c\u4ec5\u5185\u90e8\u8c03\u5ea6\u53c2\u6570\u66f4\u65b0)\n * ---------------------------------------------------------------------------\n */\ntorch::Tensor kb_1_52_Argmin_over_a_dimension_wrapper(torch::Tensor arg0,\n                                                      int64_t      dim_in)\n{\n    TORCH_CHECK(arg0.is_cuda(), \"Input tensor must be on CUDA\");\n    TORCH_CHECK(arg0.is_contiguous(),\n                \"Only contiguous tensors are supported for this op\");\n\n    // \u5904\u7406\u8d1f dim\n    int64_t dim = dim_in;\n    if (dim < 0) dim += arg0.dim();\n    TORCH_CHECK(dim >= 0 && dim < arg0.dim(),\n                \"Reduction dim is out of bounds\");\n\n    // \u8ba1\u7b97 outer / reduce / inner size\n    const auto sizes = arg0.sizes();\n    int64_t outer_size  = 1;\n    int64_t inner_size  = 1;\n    int64_t reduce_size = sizes[dim];\n\n    for (int64_t i = 0; i < dim; ++i) outer_size *= sizes[i];\n    for (int64_t i = dim + 1; i < arg0.dim(); ++i) inner_size *= sizes[i];\n\n    // \u8f93\u51fa\u5c3a\u5bf8 = \u8f93\u5165\u5c3a\u5bf8\u79fb\u9664 reduce \u7ef4\n    std::vector<int64_t> out_sizes;\n    out_sizes.reserve(arg0.dim() - 1);\n    for (int64_t i = 0; i < arg0.dim(); ++i)\n        if (i != dim) out_sizes.push_back(sizes[i]);\n\n    torch::Tensor output = torch::empty(\n        out_sizes,\n        torch::TensorOptions()\n            .dtype(torch::kInt64)\n            .device(arg0.device()));\n\n    // \u2500\u2500 \u8c03\u5ea6 kernel \u2500\u2500\n    const dim3 threads(TILE_C, TILE_RLD);\n    const int  grid_x = static_cast<int>((inner_size + TILE_C - 1) / TILE_C);\n    const dim3 blocks(grid_x, static_cast<int>(outer_size));\n\n    AT_DISPATCH_FLOATING_TYPES(arg0.scalar_type(), \"argmin_dim_kernel\", ([&] {\n        argmin_dim_kernel<scalar_t>\n            <<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(\n                arg0.data_ptr<scalar_t>(),\n                output.data_ptr<int64_t>(),\n                outer_size,\n                reduce_size,\n                inner_size);\n    }));\n\n    // CUDA \u5185\u6838\u9519\u8bef\u68c0\u67e5\uff08\u53ef\u9009\uff09\n    cudaError_t err = cudaGetLastError();\n    TORCH_CHECK(err == cudaSuccess, \"CUDA kernel failed : \",\n                cudaGetErrorString(err));\n\n    return output;\n}"
  }
]