[
  {
    "attempt": 0,
    "type": "generation",
    "response": "```cpp\n#include <torch/extension.h>\n#include <vector> // 如果返回多个张量\n\n// C++ Wrapper 函数声明 (签名)\ntorch::Tensor kb_43_Max_Pooling_3D_wrapper(torch::Tensor arg0, int64_t arg1, int64_t arg2, int64_t arg3, int64_t arg4);\n```\n\n```cu\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <cmath>\n#include <vector>\n#include <cfloat>\n// [!!! 关键 !!!] \n// PyTorch 2.1+ 移除了 c10::cuda::getCurrentCUDAStream\n// 使用 at::cuda::getCurrentCUDAStream() 代替\n#include <ATen/cuda/CUDAContext.h>\n\n// [重要] 在此放置所有 CUDA 辅助函数 (例如 blockReduceSum)\n// (确保它们在使用它们的 kernel 之前被定义)\n__device__ float blockReduceSum(float val, float* shared) {\n    int lane = threadIdx.x % warpSize;\n    int wid = threadIdx.x / warpSize;\n\n    for (int offset = warpSize / 2; offset > 0; offset /= 2) {\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    }\n\n    if (lane == 0) {\n        shared[wid] = val;\n    }\n    __syncthreads();\n\n    val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0.0f;\n    if (wid == 0) {\n        for (int offset = warpSize / 2; offset > 0; offset /= 2) {\n            val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n        }\n    }\n    return val;\n}\n\n// CUDA 内核实现: 3D Max Pooling（支持 stride、padding、dilation，ceil_mode=false）\n__global__ void maxpool3d_forward_kernel(\n    const float* __restrict__ in,\n    float* __restrict__ out,\n    long long N, long long C,\n    long long D, long long H, long long W,\n    int k, int stride, int pad, int dilation,\n    long long D_out, long long H_out, long long W_out\n) {\n    long long total = N * C * D_out * H_out * W_out;\n    long long idx = (long long)blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx >= total) return;\n\n    // 反解线性索引 -> (n, c, od, oh, ow)\n    long long ow = idx % W_out;\n    long long tmp = idx / W_out;\n    long long oh = tmp % H_out; tmp /= H_out;\n    long long od = tmp % D_out; tmp /= D_out;\n    long long c  = tmp % C;     tmp /= C;\n    long long n  = tmp;\n\n    // 计算池化窗口起始位置（考虑padding与stride）\n    long long start_d = od * stride - pad;\n    long long start_h = oh * stride - pad;\n    long long start_w = ow * stride - pad;\n\n    float maxval = -FLT_MAX;\n\n    // 遍历核窗口（含dilation）\n    for (int kd = 0; kd < k; ++kd) {\n        long long id = start_d + (long long)kd * dilation;\n        if (id < 0 || id >= D) continue;\n        for (int kh = 0; kh < k; ++kh) {\n            long long ih = start_h + (long long)kh * dilation;\n            if (ih < 0 || ih >= H) continue;\n            for (int kw = 0; kw < k; ++kw) {\n                long long iw = start_w + (long long)kw * dilation;\n                if (iw < 0 || iw >= W) continue;\n\n                long long in_index = (((n * C + c) * D + id) * H + ih) * W + iw;\n                float val = in[in_index];\n                if (val > maxval) {\n                    maxval = val;\n                }\n            }\n        }\n    }\n\n    long long out_index = (((n * C + c) * D_out + od) * H_out + oh) * W_out + ow;\n    out[out_index] = maxval;\n}\n\n// C++ Wrapper 实现\ntorch::Tensor kb_43_Max_Pooling_3D_wrapper(torch::Tensor arg0, int64_t arg1, int64_t arg2, int64_t arg3, int64_t arg4) {\n    TORCH_CHECK(arg0.is_cuda(), \"Input tensor must be on CUDA device\");\n    TORCH_CHECK(arg0.dtype() == torch::kFloat32, \"Only float32 dtype is supported\");\n    TORCH_CHECK(arg0.dim() == 5, \"Input must be a 5D tensor [N, C, D, H, W]\");\n\n    // 参数\n    int64_t k = arg1;        // kernel_size\n    int64_t stride = (arg2 > 0) ? arg2 : arg1; // 若为None，stride=kernel_size\n    int64_t pad = arg3;      // padding\n    int64_t dilation = arg4; // dilation\n    TORCH_CHECK(k > 0, \"kernel_size must be > 0\");\n    TORCH_CHECK(stride > 0, \"stride must be > 0\");\n    TORCH_CHECK(pad >= 0, \"padding must be >= 0\");\n    TORCH_CHECK(dilation > 0, \"dilation must be > 0\");\n\n    // 保证连续\n    auto x = arg0.contiguous();\n\n    // 尺寸\n    long long N = x.size(0);\n    long long C = x.size(1);\n    long long D = x.size(2);\n    long long H = x.size(3);\n    long long W = x.size(4);\n\n    // 输出尺寸 (ceil_mode = False)\n    long long eff_k = dilation * (k - 1) + 1; // effective kernel\n    long long D_out = (D + 2 * pad - eff_k) >= 0 ? ((D + 2 * pad - eff_k) / stride + 1) : 0;\n    long long H_out = (H + 2 * pad - eff_k) >= 0 ? ((H + 2 * pad - eff_k) / stride + 1) : 0;\n    long long W_out = (W + 2 * pad - eff_k) >= 0 ? ((W + 2 * pad - eff_k) / stride + 1) : 0;\n\n    TORCH_CHECK(D_out > 0 && H_out > 0 && W_out > 0, \"Calculated output size is non-positive. Check your parameters.\");\n\n    // 分配输出\n    auto out = torch::empty({N, C, D_out, H_out, W_out}, x.options());\n\n    // 指针\n    const float* in_ptr = x.data_ptr<float>();\n    float* out_ptr = out.data_ptr<float>();\n\n    // 启动配置\n    long long total = N * C * D_out * H_out * W_out;\n    int threads = 256;\n    int blocks = static_cast<int>((total + threads - 1) / threads);\n\n    // 调用内核\n    cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n    maxpool3d_forward_kernel<<<blocks, threads, 0, stream>>>(\n        in_ptr, out_ptr,\n        N, C, D, H, W,\n        static_cast<int>(k),\n        static_cast<int>(stride),\n        static_cast<int>(pad),\n        static_cast<int>(dilation),\n        D_out, H_out, W_out\n    );\n    auto err = cudaGetLastError();\n    TORCH_CHECK(err == cudaSuccess, \"maxpool3d_forward_kernel launch failed: \", cudaGetErrorString(err));\n\n    return out;\n}\n```",
    "cpp_code_extracted": true,
    "cuda_code_extracted": true,
    "error": ""
  }
]