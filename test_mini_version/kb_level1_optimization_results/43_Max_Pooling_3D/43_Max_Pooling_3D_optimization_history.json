[
  {
    "round": 0,
    "goal": "Baseline",
    "status": "Success",
    "time_ms": 15.8202880859375,
    "ptxas_metrics": {
      "registers_used": 56.0,
      "shared_mem_bytes": 0.0,
      "spill_bytes": 0.0
    },
    "all_ncu_metrics": {
      "DRAMFrequency": 1512063316.68,
      "SMFrequency": 1065009164.71,
      "ElapsedCycles": 16841138.0,
      "MemoryThroughput": 15.19,
      "DRAMThroughput": 15.19,
      "Duration": 15812832.0,
      "L1TEXCacheThroughput": 14.34,
      "L2CacheThroughput": 22.28,
      "SMActiveCycles": 16817333.97,
      "ComputeSMThroughput": 66.88,
      "BlockSize": 256.0,
      "GridSize": 65535.0,
      "RegistersPerThread": 56.0,
      "SharedMemoryConfigurationSize": 32768.0,
      "DriverSharedMemoryPerBlock": 1024.0,
      "DynamicSharedMemoryPerBlock": 0.0,
      "StaticSharedMemoryPerBlock": 0.0,
      "Threads": 16776960.0,
      "WavesPerSM": 151.7,
      "BlockLimitSM": 32.0,
      "BlockLimitRegisters": 4.0,
      "BlockLimitSharedMem": 32.0,
      "BlockLimitWarps": 8.0,
      "TheoreticalActiveWarpsperSM": 32.0,
      "TheoreticalOccupancy": 50.0,
      "AchievedOccupancy": 48.46,
      "AchievedActiveWarpsPerSM": 31.01
    },
    "selected_ncu_metrics": [],
    "details": "Initial baseline measurement",
    "code": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <cmath>\n#include <cfloat>\n#include <vector>\n#include <ATen/cuda/CUDAContext.h>\n\n// CUDA \u8f85\u52a9\u51fd\u6570\u793a\u4f8b\uff08\u672a\u5728\u672c\u5185\u6838\u4e2d\u4f7f\u7528\uff0c\u4f46\u6309\u8981\u6c42\u63d0\u4f9b\uff09\n__device__ float blockReduceSum(float val, float* shared) {\n    int lane = threadIdx.x % warpSize;\n    int wid = threadIdx.x / warpSize;\n\n    // Warp\u5185\u5f52\u7ea6\n    for (int offset = warpSize / 2; offset > 0; offset /= 2) {\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    }\n\n    // \u6bcf\u4e2awarp\u7684\u7b2c\u4e00\u4e2a\u7ebf\u7a0b\u5c06\u7ed3\u679c\u5199\u5165\u5171\u4eab\u5185\u5b58\n    if (lane == 0) {\n        shared[wid] = val;\n    }\n    __syncthreads();\n\n    // \u7b2c\u4e00\u4e2awarp\u8fdb\u884c\u6700\u7ec8\u5f52\u7ea6\n    val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0.0f;\n    if (wid == 0) {\n        for (int offset = warpSize / 2; offset > 0; offset /= 2) {\n            val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n        }\n    }\n    return val;\n}\n\n// 3D MaxPooling CUDA \u5185\u6838\u5b9e\u73b0 (NCDHW)\n__global__ void MaxPool3dKernel(\n    const float* __restrict__ input,\n    float* __restrict__ output,\n    int64_t N, int64_t C,\n    int64_t D, int64_t H, int64_t W,\n    int64_t kernel_size,\n    int64_t stride,\n    int64_t padding,\n    int64_t dilation,\n    int64_t OD, int64_t OH, int64_t OW\n) {\n    int64_t total = N * C * OD * OH * OW;\n    int64_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    int64_t stride_grid = (int64_t)blockDim.x * gridDim.x;\n\n    for (int64_t linear = tid; linear < total; linear += stride_grid) {\n        // \u53cd\u89e3\u7ebf\u6027\u7d22\u5f15\u4e3a (n, c, od, oh, ow)\n        int64_t tmp = linear;\n        int64_t ow = tmp % OW; tmp /= OW;\n        int64_t oh = tmp % OH; tmp /= OH;\n        int64_t od = tmp % OD; tmp /= OD;\n        int64_t c  = tmp % C;  tmp /= C;\n        int64_t n  = tmp;\n\n        float maxVal = -FLT_MAX;\n\n        // \u8ba1\u7b97\u8f93\u5165\u8d77\u59cb\u4f4d\u7f6e\uff08\u5e26\u586b\u5145\uff09\n        int64_t dstart = od * stride - padding;\n        int64_t hstart = oh * stride - padding;\n        int64_t wstart = ow * stride - padding;\n\n        // \u904d\u5386\u5e26\u81a8\u80c0\u7684\u6838\n        for (int64_t kd = 0; kd < kernel_size; ++kd) {\n            int64_t id = dstart + kd * dilation;\n            if (id < 0 || id >= D) continue;\n\n            for (int64_t kh = 0; kh < kernel_size; ++kh) {\n                int64_t ih = hstart + kh * dilation;\n                if (ih < 0 || ih >= H) continue;\n\n                for (int64_t kw = 0; kw < kernel_size; ++kw) {\n                    int64_t iw = wstart + kw * dilation;\n                    if (iw < 0 || iw >= W) continue;\n\n                    int64_t in_index = (((n * C + c) * D + id) * H + ih) * W + iw;\n                    float val = input[in_index];\n                    if (val > maxVal) {\n                        maxVal = val;\n                    }\n                }\n            }\n        }\n\n        output[linear] = maxVal;\n    }\n}\n\n// C++ Wrapper \u5b9e\u73b0\ntorch::Tensor kb_43_Max_Pooling_3D_wrapper(torch::Tensor arg0, int64_t arg1, int64_t arg2, int64_t arg3, int64_t arg4) {\n    // \u9a8c\u8bc1\u8f93\u5165\n    TORCH_CHECK(arg0.is_cuda(), \"Input tensor must be on CUDA device\");\n    TORCH_CHECK(arg0.dtype() == torch::kFloat32, \"Input tensor must be float32\");\n    TORCH_CHECK(arg0.dim() == 5, \"Input must be a 5D tensor in NCDHW format\");\n    TORCH_CHECK(arg1 > 0, \"kernel_size must be > 0\");\n    TORCH_CHECK(arg2 > 0, \"stride must be > 0\");\n    TORCH_CHECK(arg3 >= 0, \"padding must be >= 0\");\n    TORCH_CHECK(arg4 > 0, \"dilation must be > 0\");\n\n    auto x = arg0.contiguous();\n\n    int64_t N = x.size(0);\n    int64_t C = x.size(1);\n    int64_t D = x.size(2);\n    int64_t H = x.size(3);\n    int64_t W = x.size(4);\n\n    int64_t kernel_size = arg1;\n    int64_t stride = arg2;\n    int64_t padding = arg3;\n    int64_t dilation = arg4;\n\n    // \u8ba1\u7b97\u8f93\u51fa\u5c3a\u5bf8 (ceil_mode=false)\n    int64_t k_eff = 1 + (kernel_size - 1) * dilation;\n\n    auto compute_out = [&](int64_t in_size) -> int64_t {\n        int64_t out = (in_size + 2 * padding - k_eff) / stride + 1;\n        return std::max<int64_t>(out, 0);\n    };\n\n    int64_t OD = compute_out(D);\n    int64_t OH = compute_out(H);\n    int64_t OW = compute_out(W);\n\n    TORCH_CHECK(OD > 0 && OH > 0 && OW > 0, \"Computed output dimensions must be > 0\");\n\n    // \u5206\u914d\u8f93\u51fa\u5f20\u91cf\n    auto out = torch::empty({N, C, OD, OH, OW}, x.options());\n\n    // \u8ba1\u7b97\u7f51\u683c/\u5757\u7ef4\u5ea6\n    int threads = 256;\n    int64_t total = N * C * OD * OH * OW;\n    int64_t blocks64 = (total + threads - 1) / threads;\n    // \u9650\u5236block\u6570\u91cf\u4ee5\u4f7f\u7528grid-stride loop\n    int blocks = static_cast<int>(std::min<int64_t>(blocks64, 65535));\n\n    // \u8c03\u7528\u5185\u6838\n    auto stream = at::cuda::getCurrentCUDAStream();\n    MaxPool3dKernel<<<blocks, threads, 0, stream.stream()>>>(\n        x.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, D, H, W,\n        kernel_size, stride, padding, dilation,\n        OD, OH, OW\n    );\n\n    // \u9519\u8bef\u68c0\u67e5\n    cudaError_t err = cudaGetLastError();\n    TORCH_CHECK(err == cudaSuccess, \"MaxPool3dKernel launch failed: \", cudaGetErrorString(err));\n\n    return out;\n}"
  },
  {
    "round": 1,
    "goal": "Refactor index calculations to reuse local variables and compute deltas incrementally, reducing registers_used to enable higher occupancy (>75%) and more blocks/SM.\nOptimization History:\nPrevious Optimization Attempts:\n  Round 0:\n    Goal: Baseline\n    Status: Success\n    Performance: 15.820 ms\n    Registers: 56.0\n    Shared Mem: 0.0 bytes",
    "status": "Success (New Best)",
    "time_ms": 8.535756683349609,
    "ptxas_metrics": {
      "registers_used": 38.0,
      "shared_mem_bytes": 0.0,
      "spill_bytes": 0.0
    },
    "all_ncu_metrics": {
      "DRAMFrequency": 1511745837.62,
      "SMFrequency": 1064718978.81,
      "ElapsedCycles": 9082080.0,
      "MemoryThroughput": 28.18,
      "DRAMThroughput": 28.18,
      "Duration": 8529728.0,
      "L1TEXCacheThroughput": 26.66,
      "L2CacheThroughput": 41.83,
      "SMActiveCycles": 9064363.81,
      "ComputeSMThroughput": 64.17,
      "BlockSize": 256.0,
      "GridSize": 65535.0,
      "RegistersPerThread": 38.0,
      "SharedMemoryConfigurationSize": 32768.0,
      "DriverSharedMemoryPerBlock": 1024.0,
      "DynamicSharedMemoryPerBlock": 0.0,
      "StaticSharedMemoryPerBlock": 0.0,
      "Threads": 16776960.0,
      "WavesPerSM": 101.13,
      "BlockLimitSM": 32.0,
      "BlockLimitRegisters": 6.0,
      "BlockLimitSharedMem": 32.0,
      "BlockLimitWarps": 8.0,
      "TheoreticalActiveWarpsperSM": 48.0,
      "TheoreticalOccupancy": 75.0,
      "AchievedOccupancy": 70.92,
      "AchievedActiveWarpsPerSM": 45.39
    },
    "selected_ncu_metrics": [
      "RegistersPerThread",
      "AchievedOccupancy",
      "TheoreticalOccupancy",
      "BlockLimitRegisters",
      "AchievedActiveWarpsPerSM"
    ],
    "details": "Performance improved from 15.820 ms to 8.536 ms.",
    "code": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <cmath>\n#include <cfloat>\n#include <vector>\n#include <ATen/cuda/CUDAContext.h>\n#include <climits>\n\n// CUDA \u8f85\u52a9\u51fd\u6570\u793a\u4f8b\uff08\u672a\u5728\u672c\u5185\u6838\u4e2d\u4f7f\u7528\uff0c\u4f46\u6309\u8981\u6c42\u63d0\u4f9b\uff09\n__device__ float blockReduceSum(float val, float* shared) {\n    int lane = threadIdx.x % warpSize;\n    int wid  = threadIdx.x / warpSize;\n\n    // Warp\u5185\u5f52\u7ea6\n    for (int offset = warpSize / 2; offset > 0; offset /= 2) {\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    }\n\n    // \u6bcf\u4e2awarp\u7684\u7b2c\u4e00\u4e2a\u7ebf\u7a0b\u5c06\u7ed3\u679c\u5199\u5165\u5171\u4eab\u5185\u5b58\n    if (lane == 0) {\n        shared[wid] = val;\n    }\n    __syncthreads();\n\n    // \u7b2c\u4e00\u4e2awarp\u8fdb\u884c\u6700\u7ec8\u5f52\u7ea6\n    val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0.0f;\n    if (wid == 0) {\n        for (int offset = warpSize / 2; offset > 0; offset /= 2) {\n            val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n        }\n    }\n    return val;\n}\n\n// 3D MaxPooling CUDA \u5185\u6838\u5b9e\u73b0 (NCDHW)\n// \u4f7f\u7528 32-bit \u7d22\u5f15\uff0c\u5e76\u9650\u5236\u5bc4\u5b58\u5668\u6570\u4ee5\u63d0\u9ad8\u5360\u7528\u7387\n__global__ __launch_bounds__(256, 4)\nvoid MaxPool3dKernel(\n    const float* __restrict__ input,\n    float* __restrict__ output,\n    int64_t N64, int64_t C64,\n    int64_t D64, int64_t H64, int64_t W64,\n    int64_t kernel_size64,\n    int64_t stride64,\n    int64_t padding64,\n    int64_t dilation64,\n    int64_t OD64, int64_t OH64, int64_t OW64\n) {\n    // \u5c06\u7ef4\u5ea6\u53c2\u6570\u8f6c\u6362\u4e3a 32-bit\uff0c\u672c\u5185\u6838\u5047\u8bbe numel() < INT_MAX\n    const int N  = static_cast<int>(N64);\n    const int C  = static_cast<int>(C64);\n    const int D  = static_cast<int>(D64);\n    const int H  = static_cast<int>(H64);\n    const int W  = static_cast<int>(W64);\n    const int OD = static_cast<int>(OD64);\n    const int OH = static_cast<int>(OH64);\n    const int OW = static_cast<int>(OW64);\n\n    const int kernel_size = static_cast<int>(kernel_size64);\n    const int stride      = static_cast<int>(stride64);\n    const int padding     = static_cast<int>(padding64);\n    const int dilation    = static_cast<int>(dilation64);\n\n    // \u603b\u5143\u7d20\u6570\u91cf\u4fdd\u6301 64-bit \u4ee5\u9a71\u52a8 grid-stride loop\n    const int64_t total = static_cast<int64_t>(N) * C * OD * OH * OW;\n\n    int64_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n    for (int64_t linear64 = tid; linear64 < total; linear64 += blockDim.x * gridDim.x) {\n        // \u7ebf\u6027\u7d22\u5f15\u5728 INT_MAX \u8303\u56f4\u5185\uff0c\u5b89\u5168\u8f6c\u6362\u4e3a 32-bit\n        int linear = static_cast<int>(linear64);\n        int tmp = linear;\n\n        int ow = tmp % OW; tmp /= OW;\n        int oh = tmp % OH; tmp /= OH;\n        int od = tmp % OD; tmp /= OD;\n        int c  = tmp % C;  tmp /= C;\n        int n  = tmp;\n\n        float maxVal = -FLT_MAX;\n\n        const int dstart = od * stride - padding;\n        const int hstart = oh * stride - padding;\n        const int wstart = ow * stride - padding;\n\n        // \u9884\u8ba1\u7b97 (n,c) \u7684\u57fa\u51c6\u504f\u79fb\n        const int base_nc = (n * C + c) * D;\n\n        for (int kd = 0; kd < kernel_size; ++kd) {\n            int id = dstart + kd * dilation;\n            if (id < 0 || id >= D) continue;\n\n            int d_off = (base_nc + id) * H;  // ((n*C+c)*D + id) * H\n\n            for (int kh = 0; kh < kernel_size; ++kh) {\n                int ih = hstart + kh * dilation;\n                if (ih < 0 || ih >= H) continue;\n\n                int h_off = (d_off + ih) * W; // ... + ih) * W\n\n                for (int kw = 0; kw < kernel_size; ++kw) {\n                    int iw = wstart + kw * dilation;\n                    if (iw < 0 || iw >= W) continue;\n\n                    int idx = h_off + iw; // \u5b8c\u6574\u7ebf\u6027\u7d22\u5f15\n                    float val = input[idx];\n                    if (val > maxVal) {\n                        maxVal = val;\n                    }\n                }\n            }\n        }\n\n        output[linear] = maxVal;\n    }\n}\n\n// C++ Wrapper \u5b9e\u73b0\ntorch::Tensor kb_43_Max_Pooling_3D_wrapper(torch::Tensor arg0, int64_t arg1, int64_t arg2, int64_t arg3, int64_t arg4) {\n    // \u9a8c\u8bc1\u8f93\u5165\n    TORCH_CHECK(arg0.is_cuda(), \"Input tensor must be on CUDA device\");\n    TORCH_CHECK(arg0.dtype() == torch::kFloat32, \"Input tensor must be float32\");\n    TORCH_CHECK(arg0.dim() == 5, \"Input must be a 5D tensor in NCDHW format\");\n    TORCH_CHECK(arg1 > 0, \"kernel_size must be > 0\");\n    TORCH_CHECK(arg2 > 0, \"stride must be > 0\");\n    TORCH_CHECK(arg3 >= 0, \"padding must be >= 0\");\n    TORCH_CHECK(arg4 > 0, \"dilation must be > 0\");\n\n    auto x = arg0.contiguous();\n\n    // 32-bit \u7d22\u5f15\u5b89\u5168\u68c0\u67e5\n    TORCH_CHECK(x.numel() < INT_MAX, \"Tensor is too large for 32-bit indexing\");\n\n    int64_t N = x.size(0);\n    int64_t C = x.size(1);\n    int64_t D = x.size(2);\n    int64_t H = x.size(3);\n    int64_t W = x.size(4);\n\n    int64_t kernel_size = arg1;\n    int64_t stride = arg2;\n    int64_t padding = arg3;\n    int64_t dilation = arg4;\n\n    // \u8ba1\u7b97\u8f93\u51fa\u5c3a\u5bf8 (ceil_mode=false)\n    int64_t k_eff = 1 + (kernel_size - 1) * dilation;\n\n    auto compute_out = [&](int64_t in_size) -> int64_t {\n        int64_t out = (in_size + 2 * padding - k_eff) / stride + 1;\n        return std::max<int64_t>(out, 0);\n    };\n\n    int64_t OD = compute_out(D);\n    int64_t OH = compute_out(H);\n    int64_t OW = compute_out(W);\n\n    TORCH_CHECK(OD > 0 && OH > 0 && OW > 0, \"Computed output dimensions must be > 0\");\n\n    // \u5206\u914d\u8f93\u51fa\u5f20\u91cf\n    auto out = torch::empty({N, C, OD, OH, OW}, x.options());\n\n    // \u8ba1\u7b97\u7f51\u683c/\u5757\u7ef4\u5ea6\n    int threads = 256;\n    int64_t total = N * C * OD * OH * OW;\n    int64_t blocks64 = (total + threads - 1) / threads;\n    // \u9650\u5236block\u6570\u91cf\u4ee5\u4f7f\u7528grid-stride loop\n    int blocks = static_cast<int>(std::min<int64_t>(blocks64, 65535));\n\n    // \u8c03\u7528\u5185\u6838\n    auto stream = at::cuda::getCurrentCUDAStream();\n    MaxPool3dKernel<<<blocks, threads, 0, stream.stream()>>>(\n        x.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, D, H, W,\n        kernel_size, stride, padding, dilation,\n        OD, OH, OW\n    );\n\n    // \u9519\u8bef\u68c0\u67e5\n    cudaError_t err = cudaGetLastError();\n    TORCH_CHECK(err == cudaSuccess, \"MaxPool3dKernel launch failed: \", cudaGetErrorString(err));\n\n    return out;\n}"
  },
  {
    "round": 2,
    "goal": "Implement shared memory tiling to cooperatively load input data tiles into shared memory for reuse across multiple output positions, reducing redundant global reads and improving coalesced access efficiency.\nOptimization History:\nPrevious Optimization Attempts:\n  Round 0:\n    Goal: Baseline\n    Status: Success\n    Performance: 15.820 ms\n    Registers: 56.0\n    Shared Mem: 0.0 bytes\n  Round 1:\n    Goal: Refactor index calculations to reuse local variables and compute deltas incrementally, reducing registers_used to enable higher occupancy (>75%) and more blocks/SM.\n    Status: Success (New Best)\n    Performance: 8.536 ms\n    Registers: 38.0\n    Shared Mem: 0.0 bytes\n    Selected NCU Metrics (for Goal):\n      - RegistersPerThread: 38.0\n      - AchievedOccupancy: 70.92\n      - TheoreticalOccupancy: 75.0\n      - BlockLimitRegisters: 6.0\n      - AchievedActiveWarpsPerSM: 45.39",
    "status": "Failed (Performance Regression)",
    "time_ms": 34.69649963378906,
    "ptxas_metrics": {
      "registers_used": 40.0,
      "shared_mem_bytes": 16384.0,
      "spill_bytes": 0.0
    },
    "all_ncu_metrics": {
      "DRAMFrequency": 1512017071.91,
      "SMFrequency": 1065001452.42,
      "ElapsedCycles": 36923127.0,
      "MemoryThroughput": 23.82,
      "DRAMThroughput": 7.31,
      "Duration": 34669344.0,
      "L1TEXCacheThroughput": 23.85,
      "L2CacheThroughput": 11.85,
      "SMActiveCycles": 36885046.89,
      "ComputeSMThroughput": 67.0,
      "BlockSize": 256.0,
      "GridSize": 65535.0,
      "RegistersPerThread": 40.0,
      "SharedMemoryConfigurationSize": 135168.0,
      "DriverSharedMemoryPerBlock": 1024.0,
      "DynamicSharedMemoryPerBlock": 0.0,
      "StaticSharedMemoryPerBlock": 16384.0,
      "Threads": 16776960.0,
      "WavesPerSM": 101.13,
      "BlockLimitSM": 32.0,
      "BlockLimitRegisters": 6.0,
      "BlockLimitSharedMem": 7.0,
      "BlockLimitWarps": 8.0,
      "TheoreticalActiveWarpsperSM": 48.0,
      "TheoreticalOccupancy": 75.0,
      "AchievedOccupancy": 74.68,
      "AchievedActiveWarpsPerSM": 47.8
    },
    "selected_ncu_metrics": [
      "DRAMThroughput",
      "L1TEXCacheThroughput",
      "L2CacheThroughput",
      "StaticSharedMemoryPerBlock",
      "BlockLimitSharedMem"
    ],
    "details": "New time 34.696 ms is not better than best time 8.536 ms.",
    "code": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <cmath>\n#include <cfloat>\n#include <vector>\n#include <ATen/cuda/CUDAContext.h>\n#include <climits>\n\n// CUDA \u8f85\u52a9\u51fd\u6570\u793a\u4f8b\uff08\u672a\u5728\u672c\u5185\u6838\u4e2d\u4f7f\u7528\uff0c\u4f46\u6309\u8981\u6c42\u63d0\u4f9b\uff09\n__device__ float blockReduceSum(float val, float* shared) {\n    int lane = threadIdx.x % warpSize;\n    int wid  = threadIdx.x / warpSize;\n\n    // Warp\u5185\u5f52\u7ea6\n    for (int offset = warpSize / 2; offset > 0; offset /= 2) {\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    }\n\n    // \u6bcf\u4e2awarp\u7684\u7b2c\u4e00\u4e2a\u7ebf\u7a0b\u5c06\u7ed3\u679c\u5199\u5165\u5171\u4eab\u5185\u5b58\n    if (lane == 0) {\n        shared[wid] = val;\n    }\n    __syncthreads();\n\n    // \u7b2c\u4e00\u4e2awarp\u8fdb\u884c\u6700\u7ec8\u5f52\u7ea6\n    val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0.0f;\n    if (wid == 0) {\n        for (int offset = warpSize / 2; offset > 0; offset /= 2) {\n            val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n        }\n    }\n    return val;\n}\n\n// 3D MaxPooling CUDA \u5185\u6838\u5b9e\u73b0 (NCDHW) - \u5e26\u5bbd\u4f18\u5316\u7248\u672c\n//  \u5173\u952e\u601d\u60f3\uff1a\u8ba9\u4e00\u4e2a thread-block \u8d1f\u8d23\u4e00\u6574\u6761 (n,c,od,oh,*) \u7684\u8f93\u51fa\u5bbd\u5ea6\uff0c\n//           \u5229\u7528\u5171\u4eab\u5185\u5b58\u5bf9\u540c\u4e00\u6761\u8f93\u5165\u884c\u91cd\u590d\u8bbf\u95ee\u505a\u7f13\u5b58\u3002\n//\n__global__ __launch_bounds__(256, 4)\nvoid MaxPool3dKernel(\n    const float* __restrict__ input,\n    float* __restrict__ output,\n    int64_t N64, int64_t C64,\n    int64_t D64, int64_t H64, int64_t W64,\n    int64_t kernel_size64,\n    int64_t stride64,\n    int64_t padding64,\n    int64_t dilation64,\n    int64_t OD64, int64_t OH64, int64_t OW64\n) {\n    // \u5c06\u7ef4\u5ea6\u53c2\u6570\u8f6c\u6362\u4e3a 32-bit\uff0c\u672c\u5185\u6838\u5047\u8bbe numel() < INT_MAX\n    const int N  = static_cast<int>(N64);\n    const int C  = static_cast<int>(C64);\n    const int D  = static_cast<int>(D64);\n    const int H  = static_cast<int>(H64);\n    const int W  = static_cast<int>(W64);\n    const int OD = static_cast<int>(OD64);\n    const int OH = static_cast<int>(OH64);\n    const int OW = static_cast<int>(OW64);\n\n    const int kernel_size = static_cast<int>(kernel_size64);\n    const int stride      = static_cast<int>(stride64);\n    const int padding     = static_cast<int>(padding64);\n    const int dilation    = static_cast<int>(dilation64);\n\n    // \u8ba1\u7b97\u4e00\u6b21\u6027\u9700\u8981\u7f13\u5b58\u7684\u8f93\u5165\u5bbd\u5ea6\n    const int kEffW = 1 + (kernel_size - 1) * dilation;                 // \u6709\u6548\u611f\u53d7\u91ce\n    const int tileW = blockDim.x * stride + kEffW - dilation;           // \u7f13\u5b58\u5bbd\u5ea6\n\n    // \u8bbe\u5b9a\u4e00\u4e2a\u53ef\u5bb9\u7eb3\u5e38\u89c1\u573a\u666f\u7684\u5171\u4eab\u5185\u5b58\u4e0a\u9650\uff08\u5b57\u8282\u5360\u7528 < 48KB\uff09\n    constexpr int kMaxTileW = 4096;\n    __shared__ float sLine[kMaxTileW];\n\n    // \u5982\u679c tileW \u8d85\u8fc7\u4e0a\u9650\uff0c\u5219\u9000\u5316\u4e3a\u539f\u59cb\u5168\u5c40\u5185\u5b58\u5b9e\u73b0\n    const bool useShared = (tileW <= kMaxTileW);\n\n    // \u6240\u6709 (n,c,od,oh) \u884c\u7684\u603b\u6570\n    const int64_t rowCount64 = static_cast<int64_t>(N) * C * OD * OH;\n    const int rowCount = static_cast<int>(rowCount64);  // \u5df2\u4fdd\u8bc1 < INT_MAX\n\n    // grid-stride \u904d\u5386\u884c\uff0c\u517c\u5bb9 wrapper \u4e2d\u53ef\u80fd\u88c1\u526a\u7684 blocks \u6570\n    for (int rowIdx = blockIdx.x; rowIdx < rowCount; rowIdx += gridDim.x) {\n\n        // \u53cd\u89e3\u51fa (n,c,od,oh)\n        int tmp = rowIdx;\n        int oh = tmp % OH; tmp /= OH;\n        int od = tmp % OD; tmp /= OD;\n        int  c = tmp %  C; tmp /= C;\n        int  n = tmp;\n\n        // \u8fd9\u6761\u884c\u5728\u8f93\u5165\u5f20\u91cf\u4e2d\u7684\u8d77\u59cb\u5750\u6807\n        const int dstart = od * stride - padding;\n        const int hstart = oh * stride - padding;\n\n        // \u9884\u8ba1\u7b97 (n,c) \u7684\u57fa\u51c6\u504f\u79fb\n        const int base_nc = (n * C + c) * D;\n\n        // \u6309 blockDim.x \u6b65\u957f\u5bf9 OW \u505a\u5207\u7247\uff0c\u786e\u4fdd\u517c\u5bb9 OW > blockDim.x \u7684\u60c5\u51b5\n        for (int ow_block = 0; ow_block < OW; ow_block += blockDim.x) {\n\n            const int ow = ow_block + threadIdx.x;    // \u6bcf\u4e2a\u7ebf\u7a0b\u8d1f\u8d23\u7684\u5177\u4f53 ow\n            const bool validOw = (ow < OW);\n\n            // \u8be5\u5207\u7247\u5bf9\u5e94\u7684\u8f93\u5165\u5bbd\u5ea6\u7a97\u53e3\n            const int wTileStart = ow_block * stride - padding;\n\n            float localMax = -FLT_MAX;\n\n            // \u4f9d\u6b21\u904d\u5386 (kd,kh) \u5e76\u5229\u7528\u5171\u4eab\u5185\u5b58\u7f13\u5b58\u540c\u4e00\u884c\u5bbd\u5ea6\n            for (int kd = 0; kd < kernel_size; ++kd) {\n                int id = dstart + kd * dilation;\n                if (id < 0 || id >= D) continue;\n\n                int dOff = (base_nc + id) * H;  // ((n*C+c)*D + id)*H\n\n                for (int kh = 0; kh < kernel_size; ++kh) {\n                    int ih = hstart + kh * dilation;\n                    if (ih < 0 || ih >= H) continue;\n\n                    int hOff = (dOff + ih) * W; // ... + ih)*W\n\n                    // ---------- \u5171\u4eab\u5185\u5b58\u534f\u4f5c\u52a0\u8f7d ----------\n                    if (useShared) {\n                        for (int t = threadIdx.x; t < tileW; t += blockDim.x) {\n                            int iw = wTileStart + t;\n                            float v = (iw >= 0 && iw < W) ? input[hOff + iw] : -FLT_MAX;\n                            sLine[t] = v;\n                        }\n                        __syncthreads();\n                    }\n\n                    // ---------- \u6bcf\u7ebf\u7a0b\u5728\u7f13\u5b58\u4e0a\u505a\u6700\u5927\u503c ----------\n                    if (validOw) {\n                        const int wstart = ow * stride - padding;\n                        const int localW = wstart - wTileStart; // \u5728 tile \u4e2d\u7684\u504f\u79fb\n\n                        float maxVal = -FLT_MAX;\n#pragma unroll\n                        for (int kw = 0; kw < kernel_size; ++kw) {\n                            int sIdx = localW + kw * dilation;\n                            float v;\n                            if (useShared) {\n                                v = sLine[sIdx];\n                            } else {\n                                int iw = wstart + kw * dilation;\n                                v = (iw >= 0 && iw < W) ? input[hOff + iw] : -FLT_MAX;\n                            }\n                            maxVal = v > maxVal ? v : maxVal;\n                        }\n                        localMax = maxVal > localMax ? maxVal : localMax;\n                    }\n\n                    if (useShared) __syncthreads(); // \u4fdd\u62a4\u4e0b\u4e00\u8f6e\u52a0\u8f7d\n                } // kh\n            } // kd\n\n            // \u5199\u56de\u6700\u7ec8\u7ed3\u679c\n            if (validOw) {\n                // \u8ba1\u7b97\u8f93\u51fa\u5f20\u91cf\u7684\u7ebf\u6027\u7d22\u5f15\n                int outIdx = (((n * C + c) * OD + od) * OH + oh) * OW + ow;\n                output[outIdx] = localMax;\n            }\n        } // ow_block\n    } // rowIdx\n}\n\n// C++ Wrapper \u5b9e\u73b0\ntorch::Tensor kb_43_Max_Pooling_3D_wrapper(torch::Tensor arg0, int64_t arg1, int64_t arg2, int64_t arg3, int64_t arg4) {\n    // \u9a8c\u8bc1\u8f93\u5165\n    TORCH_CHECK(arg0.is_cuda(), \"Input tensor must be on CUDA device\");\n    TORCH_CHECK(arg0.dtype() == torch::kFloat32, \"Input tensor must be float32\");\n    TORCH_CHECK(arg0.dim() == 5, \"Input must be a 5D tensor in NCDHW format\");\n    TORCH_CHECK(arg1 > 0, \"kernel_size must be > 0\");\n    TORCH_CHECK(arg2 > 0, \"stride must be > 0\");\n    TORCH_CHECK(arg3 >= 0, \"padding must be >= 0\");\n    TORCH_CHECK(arg4 > 0, \"dilation must be > 0\");\n\n    auto x = arg0.contiguous();\n\n    // 32-bit \u7d22\u5f15\u5b89\u5168\u68c0\u67e5\n    TORCH_CHECK(x.numel() < INT_MAX, \"Tensor is too large for 32-bit indexing\");\n\n    int64_t N = x.size(0);\n    int64_t C = x.size(1);\n    int64_t D = x.size(2);\n    int64_t H = x.size(3);\n    int64_t W = x.size(4);\n\n    int64_t kernel_size = arg1;\n    int64_t stride = arg2;\n    int64_t padding = arg3;\n    int64_t dilation = arg4;\n\n    // \u8ba1\u7b97\u8f93\u51fa\u5c3a\u5bf8 (ceil_mode=false)\n    int64_t k_eff = 1 + (kernel_size - 1) * dilation;\n\n    auto compute_out = [&](int64_t in_size) -> int64_t {\n        int64_t out = (in_size + 2 * padding - k_eff) / stride + 1;\n        return std::max<int64_t>(out, 0);\n    };\n\n    int64_t OD = compute_out(D);\n    int64_t OH = compute_out(H);\n    int64_t OW = compute_out(W);\n\n    TORCH_CHECK(OD > 0 && OH > 0 && OW > 0, \"Computed output dimensions must be > 0\");\n\n    // \u5206\u914d\u8f93\u51fa\u5f20\u91cf\n    auto out = torch::empty({N, C, OD, OH, OW}, x.options());\n\n    // \u8ba1\u7b97\u7f51\u683c/\u5757\u7ef4\u5ea6\n    int threads = 256;\n    int64_t totalRows = N * C * OD * OH;\n    int64_t blocks64 = totalRows;            // \u6bcf\u4e2a block \u5bf9\u5e94\u4e00\u6761 (n,c,od,oh)\n    int blocks = static_cast<int>(std::min<int64_t>(blocks64, 65535));\n\n    // \u8c03\u7528\u5185\u6838\n    auto stream = at::cuda::getCurrentCUDAStream();\n    MaxPool3dKernel<<<blocks, threads, 0, stream.stream()>>>(\n        x.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, D, H, W,\n        kernel_size, stride, padding, dilation,\n        OD, OH, OW\n    );\n\n    // \u9519\u8bef\u68c0\u67e5\n    cudaError_t err = cudaGetLastError();\n    TORCH_CHECK(err == cudaSuccess, \"MaxPool3dKernel launch failed: \", cudaGetErrorString(err));\n\n    return out;\n}"
  },
  {
    "round": 3,
    "goal": "Further reduce registers_used to <=32 per thread by reusing temporary variables for offsets and simplifying nested loop computations, enabling 8 blocks per SM for 100% theoretical occupancy and better memory latency hiding.",
    "status": "Success (New Best)",
    "time_ms": 6.908211517333984,
    "ptxas_metrics": {
      "registers_used": 32.0,
      "shared_mem_bytes": 0.0,
      "spill_bytes": 0.0
    },
    "all_ncu_metrics": {
      "DRAMFrequency": 1511893381.84,
      "SMFrequency": 1064752776.13,
      "ElapsedCycles": 7342926.0,
      "MemoryThroughput": 34.84,
      "DRAMThroughput": 34.84,
      "Duration": 6895936.0,
      "L1TEXCacheThroughput": 33.33,
      "L2CacheThroughput": 50.87,
      "SMActiveCycles": 7328095.4,
      "ComputeSMThroughput": 72.75,
      "BlockSize": 256.0,
      "GridSize": 65535.0,
      "RegistersPerThread": 32.0,
      "SharedMemoryConfigurationSize": 32768.0,
      "DriverSharedMemoryPerBlock": 1024.0,
      "DynamicSharedMemoryPerBlock": 0.0,
      "StaticSharedMemoryPerBlock": 0.0,
      "Threads": 16776960.0,
      "WavesPerSM": 75.85,
      "BlockLimitSM": 32.0,
      "BlockLimitRegisters": 8.0,
      "BlockLimitSharedMem": 32.0,
      "BlockLimitWarps": 8.0,
      "TheoreticalActiveWarpsperSM": 64.0,
      "TheoreticalOccupancy": 100.0,
      "AchievedOccupancy": 93.62,
      "AchievedActiveWarpsPerSM": 59.92
    },
    "selected_ncu_metrics": [
      "RegistersPerThread",
      "AchievedOccupancy",
      "TheoreticalOccupancy",
      "BlockLimitRegisters",
      "AchievedActiveWarpsPerSM"
    ],
    "details": "Performance improved from 8.536 ms to 6.908 ms.",
    "code": "#include <torch/extension.h>\n\n// C++ Wrapper \u51fd\u6570\u58f0\u660e (\u7b7e\u540d)\ntorch::Tensor kb_43_Max_Pooling_3D_wrapper(torch::Tensor arg0,\n                                           int64_t       arg1,\n                                           int64_t       arg2,\n                                           int64_t       arg3,\n                                           int64_t       arg4);\n\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <ATen/cuda/CUDAContext.h>\n#include <cfloat>\n#include <climits>\n\n// ------------------------------------------------------------\n// Constant-memory block that keeps all invariant hyper-parameters\n// ------------------------------------------------------------\nstruct Pool3DConst {\n    int N, C, D, H, W;      // input tensor sizes\n    int OD, OH, OW;         // output tensor sizes\n    int k, stride, pad, dil;\n    int total;              // total number of output elements (\u2264 INT_MAX)\n};\n__constant__ Pool3DConst gP;\n\n// ------------------------------------------------------------\n// (Optional) helper \u2013 unused in max-pool kernel but kept from\n// the original file to satisfy build dependencies.\n// ------------------------------------------------------------\n__device__ float blockReduceSum(float val, float* shared) {\n    int lane = threadIdx.x % warpSize;\n    int wid  = threadIdx.x / warpSize;\n\n    // Warp \u5185\u5f52\u7ea6\n    for (int offset = warpSize / 2; offset > 0; offset >>= 1) {\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    }\n\n    // \u6bcf\u4e2a warp \u7684\u7b2c\u4e00\u4e2a\u7ebf\u7a0b\u5199\u5165 shared\n    if (lane == 0) shared[wid] = val;\n    __syncthreads();\n\n    // \u7b2c\u4e00\u4e2a warp \u5b8c\u6210\u6700\u7ec8\u5f52\u7ea6\n    val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0.0f;\n    if (wid == 0) {\n        for (int offset = warpSize / 2; offset > 0; offset >>= 1) {\n            val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n        }\n    }\n    return val;\n}\n\n// ------------------------------------------------------------\n// Optimised 3-D MaxPooling kernel (NCDHW, 32-bit indexing)\n// ------------------------------------------------------------\n__global__ __launch_bounds__(256, 4) __attribute__((maxrregcount(32)))\nvoid MaxPool3dKernel(const float* __restrict__ input,\n                     float*       __restrict__ output) {\n    const int total = gP.total;                      // constant-mem fetch\n    const int strideGrid = blockDim.x * gridDim.x;   // 32-bit\n    int tid = blockIdx.x * blockDim.x + threadIdx.x; // 32-bit\n\n    for (int linear = tid; linear < total; linear += strideGrid) {\n        int tmp = linear;\n\n        // De-linearise (OW,OH,OD,C,N)\n        const int ow = tmp % gP.OW; tmp /= gP.OW;\n        const int oh = tmp % gP.OH; tmp /= gP.OH;\n        const int od = tmp % gP.OD; tmp /= gP.OD;\n        const int  c = tmp % gP.C;  tmp /= gP.C;\n        const int  n = tmp;                    // remaining\n\n        const int nc = n * gP.C + c;           // reused composite\n        const int hStart = oh * gP.stride - gP.pad;\n        const int wStart = ow * gP.stride - gP.pad;\n        const int dStart = od * gP.stride - gP.pad;\n\n        float maxVal = -FLT_MAX;\n\n        // depth loop\n        for (int kd = 0, id = dStart; kd < gP.k; ++kd, id += gP.dil) {\n            if (static_cast<unsigned>(id) >= static_cast<unsigned>(gP.D)) continue;\n\n            int dOffset = (nc * gP.D + id) * gP.H; // ((n*C+c)*D + id) * H\n\n            // height loop\n            for (int kh = 0, ih = hStart; kh < gP.k; ++kh, ih += gP.dil) {\n                if (static_cast<unsigned>(ih) >= static_cast<unsigned>(gP.H)) continue;\n\n                int hOffset = (dOffset + ih) * gP.W; // ... + ih) * W\n\n                // width loop\n                for (int kw = 0, iw = wStart; kw < gP.k; ++kw, iw += gP.dil) {\n                    if (static_cast<unsigned>(iw) >= static_cast<unsigned>(gP.W)) continue;\n\n                    float v = input[hOffset + iw];\n                    if (v > maxVal) maxVal = v;\n                }\n            }\n        }\n\n        output[linear] = maxVal;\n    }\n}\n\n// ------------------------------------------------------------\n// C++ Wrapper \u2013 interface remains unchanged\n// ------------------------------------------------------------\ntorch::Tensor kb_43_Max_Pooling_3D_wrapper(torch::Tensor arg0,\n                                           int64_t       arg1,\n                                           int64_t       arg2,\n                                           int64_t       arg3,\n                                           int64_t       arg4) {\n    // ---- Sanity checks ----------------------------------------------------\n    TORCH_CHECK(arg0.is_cuda(),     \"Input tensor must be on CUDA device\");\n    TORCH_CHECK(arg0.dtype() == torch::kFloat32, \"Input tensor must be float32\");\n    TORCH_CHECK(arg0.dim()  == 5,   \"Input must be a 5D tensor in NCDHW format\");\n    TORCH_CHECK(arg1 > 0,           \"kernel_size must be > 0\");\n    TORCH_CHECK(arg2 > 0,           \"stride must be > 0\");\n    TORCH_CHECK(arg3 >= 0,          \"padding must be >= 0\");\n    TORCH_CHECK(arg4 > 0,           \"dilation must be > 0\");\n\n    auto x = arg0.contiguous();\n    TORCH_CHECK(x.numel() < INT_MAX, \"Tensor is too large for 32-bit indexing\");\n\n    // ---- Gather dimensions ------------------------------------------------\n    const int64_t N = x.size(0);\n    const int64_t C = x.size(1);\n    const int64_t D = x.size(2);\n    const int64_t H = x.size(3);\n    const int64_t W = x.size(4);\n\n    const int64_t kernel_size = arg1;\n    const int64_t stride      = arg2;\n    const int64_t padding     = arg3;\n    const int64_t dilation    = arg4;\n\n    // ---- Compute output size (ceil_mode = false) --------------------------\n    const int64_t k_eff = 1 + (kernel_size - 1) * dilation;\n    auto out_size = [&](int64_t in) {\n        int64_t out = (in + 2 * padding - k_eff) / stride + 1;\n        return std::max<int64_t>(out, 0);\n    };\n    const int64_t OD = out_size(D);\n    const int64_t OH = out_size(H);\n    const int64_t OW = out_size(W);\n    TORCH_CHECK(OD > 0 && OH > 0 && OW > 0,\n                \"Computed output dimensions must be > 0\");\n\n    // ---- Allocate output --------------------------------------------------\n    auto out = torch::empty({N, C, OD, OH, OW}, x.options());\n\n    // ---- Prepare constant-memory payload ----------------------------------\n    Pool3DConst hostP;\n    hostP.N  = static_cast<int>(N);\n    hostP.C  = static_cast<int>(C);\n    hostP.D  = static_cast<int>(D);\n    hostP.H  = static_cast<int>(H);\n    hostP.W  = static_cast<int>(W);\n\n    hostP.OD = static_cast<int>(OD);\n    hostP.OH = static_cast<int>(OH);\n    hostP.OW = static_cast<int>(OW);\n\n    hostP.k      = static_cast<int>(kernel_size);\n    hostP.stride = static_cast<int>(stride);\n    hostP.pad    = static_cast<int>(padding);\n    hostP.dil    = static_cast<int>(dilation);\n\n    int64_t total64 = N * C * OD * OH * OW;\n    TORCH_CHECK(total64 < INT_MAX,\n                \"Total number of output elements exceeds 32-bit range\");\n    hostP.total = static_cast<int>(total64);\n\n    // copy to device constant memory\n    cudaMemcpyToSymbolAsync(gP, &hostP, sizeof(Pool3DConst), 0,\n                            cudaMemcpyHostToDevice,\n                            at::cuda::getCurrentCUDAStream());\n\n    // ---- Launch kernel ----------------------------------------------------\n    const int threads = 256;\n    int blocks = static_cast<int>((hostP.total + threads - 1) / threads);\n    blocks = std::min(blocks, 65535); // stay within grid-size limit\n\n    auto stream = at::cuda::getCurrentCUDAStream();\n    MaxPool3dKernel<<<blocks, threads, 0, stream.stream()>>>(\n        x.data_ptr<float>(), out.data_ptr<float>());\n\n    // ---- Error check ------------------------------------------------------\n    cudaError_t err = cudaGetLastError();\n    TORCH_CHECK(err == cudaSuccess,\n                \"MaxPool3dKernel launch failed: \",\n                cudaGetErrorString(err));\n\n    return out;\n}"
  },
  {
    "round": 4,
    "goal": "Introduce minimal shared memory (\u22644KB/block) for cooperative coalesced loading of input width slices per height/depth position, enabling unit-stride accesses in shared memory during the width max computation to boost memory throughput without reducing block limit below 8/SM.\nOptimization History:\nPrevious Optimization Attempts:\n  Round 0:\n    Goal: Baseline\n    Status: Success\n    Performance: 15.820 ms\n    Registers: 56.0\n    Shared Mem: 0.0 bytes\n  Round 1:\n    Goal: Refactor index calculations to reuse local variables and compute deltas incrementally, reducing registers_used to enable higher occupancy (>75%) and more blocks/SM.\n    Status: Success (New Best)\n    Performance: 8.536 ms\n    Registers: 38.0\n    Shared Mem: 0.0 bytes\n    Selected NCU Metrics (for Goal):\n      - RegistersPerThread: 38.0\n      - AchievedOccupancy: 70.92\n      - TheoreticalOccupancy: 75.0\n      - BlockLimitRegisters: 6.0\n      - AchievedActiveWarpsPerSM: 45.39\n  Round 2:\n    Goal: Implement shared memory tiling to cooperatively load input data tiles into shared memory for reuse across multiple output positions, reducing redundant global reads and improving coalesced access efficiency.\n    Status: Failed (Performance Regression)\n    Performance: 34.696 ms\n    Registers: 40.0\n    Shared Mem: 16384.0 bytes\n    Selected NCU Metrics (for Goal):\n      - DRAMThroughput: 7.31\n      - L1TEXCacheThroughput: 23.85\n      - L2CacheThroughput: 11.85\n      - StaticSharedMemoryPerBlock: 16384.0\n      - BlockLimitSharedMem: 7.0\n  Round 3:\n    Goal: Further reduce registers_used to <=32 per thread by reusing temporary variables for offsets and simplifying nested loop computations, enabling 8 blocks per SM for 100% theoretical occupancy and better memory latency hiding.\n    Status: Success (New Best)\n    Performance: 6.908 ms\n    Registers: 32.0\n    Shared Mem: 0.0 bytes\n    Selected NCU Metrics (for Goal):\n      - RegistersPerThread: 32.0\n      - AchievedOccupancy: 93.62\n      - TheoreticalOccupancy: 100.0\n      - BlockLimitRegisters: 8.0\n      - AchievedActiveWarpsPerSM: 59.92",
    "status": "Failed (Performance Regression)",
    "time_ms": 32.07587890625,
    "ptxas_metrics": {
      "registers_used": 32.0,
      "shared_mem_bytes": 8192.0,
      "spill_bytes": 0.0
    },
    "all_ncu_metrics": {
      "DRAMFrequency": 1511781882.78,
      "SMFrequency": 1064828616.66,
      "ElapsedCycles": 34144016.0,
      "MemoryThroughput": 19.95,
      "DRAMThroughput": 7.78,
      "Duration": 32064960.0,
      "L1TEXCacheThroughput": 19.97,
      "L2CacheThroughput": 12.85,
      "SMActiveCycles": 34112788.34,
      "ComputeSMThroughput": 75.13,
      "BlockSize": 256.0,
      "GridSize": 65535.0,
      "RegistersPerThread": 32.0,
      "SharedMemoryConfigurationSize": 102400.0,
      "DriverSharedMemoryPerBlock": 1024.0,
      "DynamicSharedMemoryPerBlock": 0.0,
      "StaticSharedMemoryPerBlock": 8192.0,
      "Threads": 16776960.0,
      "WavesPerSM": 75.85,
      "BlockLimitSM": 32.0,
      "BlockLimitRegisters": 8.0,
      "BlockLimitSharedMem": 11.0,
      "BlockLimitWarps": 8.0,
      "TheoreticalActiveWarpsperSM": 64.0,
      "TheoreticalOccupancy": 100.0,
      "AchievedOccupancy": 99.45,
      "AchievedActiveWarpsPerSM": 63.65
    },
    "selected_ncu_metrics": [
      "DRAMThroughput",
      "L1TEXCacheThroughput",
      "L2CacheThroughput",
      "StaticSharedMemoryPerBlock",
      "BlockLimitSharedMem"
    ],
    "details": "New time 32.076 ms is not better than best time 6.908 ms.",
    "code": "#include <torch/extension.h>\n\n// C++ Wrapper \u51fd\u6570\u58f0\u660e (\u7b7e\u540d)\ntorch::Tensor kb_43_Max_Pooling_3D_wrapper(torch::Tensor arg0,\n                                           int64_t       arg1,\n                                           int64_t       arg2,\n                                           int64_t       arg3,\n                                           int64_t       arg4);\n\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <ATen/cuda/CUDAContext.h>\n#include <cfloat>\n#include <climits>\n\n// ------------------------------------------------------------\n// Constant-memory block that keeps all invariant hyper-parameters\n// ------------------------------------------------------------\nstruct Pool3DConst {\n    int N, C, D, H, W;      // input tensor sizes\n    int OD, OH, OW;         // output tensor sizes\n    int k, stride, pad, dil;\n    int total;              // total number of output elements (\u2264 INT_MAX)\n};\n__constant__ Pool3DConst gP;\n\n// ------------------------------------------------------------\n// (Optional) helper \u2013 unused in max-pool kernel but kept from\n// the original file to satisfy build dependencies.\n// ------------------------------------------------------------\n__device__ float blockReduceSum(float val, float* shared) {\n    int lane = threadIdx.x % warpSize;\n    int wid  = threadIdx.x / warpSize;\n\n    // Warp \u5185\u5f52\u7ea6\n    for (int offset = warpSize / 2; offset > 0; offset >>= 1) {\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    }\n\n    // \u6bcf\u4e2a warp \u7684\u7b2c\u4e00\u4e2a\u7ebf\u7a0b\u5199\u5165 shared\n    if (lane == 0) shared[wid] = val;\n    __syncthreads();\n\n    // \u7b2c\u4e00\u4e2a warp \u5b8c\u6210\u6700\u7ec8\u5f52\u7ea6\n    val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0.0f;\n    if (wid == 0) {\n        for (int offset = warpSize / 2; offset > 0; offset >>= 1) {\n            val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n        }\n    }\n    return val;\n}\n\n// ------------------------------------------------------------\n// Optimised 3-D MaxPooling kernel (NCDHW, 32-bit indexing)\n// ------------------------------------------------------------\n#define SHARED_MAX 2048  // 8 KB of static shared memory (\u226448 KB per SM)\n\n__global__ __launch_bounds__(256, 4) __attribute__((maxrregcount(32)))\nvoid MaxPool3dKernel(const float* __restrict__ input,\n                     float*       __restrict__ output) {\n    // ---- Derived constants ----------------------------------------------\n    const int rows        = gP.N * gP.C * gP.OD * gP.OH;   // (n,c,od,oh)\n    const int blocksPerRow =\n        (gP.OW + blockDim.x - 1) / blockDim.x;             // ceil(OW / blockDim.x)\n    const long long totalSegs = static_cast<long long>(rows) *\n                                static_cast<long long>(blocksPerRow);\n\n    __shared__ float s_line[SHARED_MAX];\n\n    // ---- Iterate over (row, segment) assignments -------------------------\n    for (long long segLinear = blockIdx.x;\n         segLinear < totalSegs;\n         segLinear += gridDim.x) {\n\n        const int rowIdx    = static_cast<int>(segLinear / blocksPerRow);\n        const int segIdx    = static_cast<int>(segLinear % blocksPerRow);\n\n        // De-linearise rowIdx \u2192 (n, c, od, oh)\n        int tmp  = rowIdx;\n        const int oh = tmp % gP.OH;  tmp /= gP.OH;\n        const int od = tmp % gP.OD;  tmp /= gP.OD;\n        const int  c = tmp % gP.C;   tmp /= gP.C;\n        const int  n = tmp;\n\n        // Thread-local output width index\n        const int ow  = segIdx * blockDim.x + threadIdx.x;\n        const bool active = (ow < gP.OW);\n\n        // Common indices/offsets inside this row\n        const int nc        = n * gP.C + c;\n        const int dStart    = od * gP.stride - gP.pad;\n        const int hStart    = oh * gP.stride - gP.pad;\n\n        // Pre-compute constants for shared memory usage\n        const int requiredWidth =\n            blockDim.x * gP.stride + (gP.k - 1) * gP.dil;\n        const bool useShared = (requiredWidth <= SHARED_MAX);\n\n        // Local max accumulator\n        float maxVal = -FLT_MAX;\n\n        // --------------------------------------------------------------\n        // Depth / Height loops\n        // --------------------------------------------------------------\n        for (int kd = 0, id = dStart; kd < gP.k; ++kd, id += gP.dil) {\n            if (id < 0 || id >= gP.D) continue;\n\n            const int dOffset = (nc * gP.D + id) * gP.H;\n\n            for (int kh = 0, ih = hStart; kh < gP.k; ++kh, ih += gP.dil) {\n                if (ih < 0 || ih >= gP.H) continue;\n\n                // --------------------------------------------------\n                // Cooperative load into shared memory  (if enabled)\n                // --------------------------------------------------\n                if (useShared) {\n                    const int wStartBase = segIdx * blockDim.x * gP.stride - gP.pad;\n                    const int hOffset    = (dOffset + ih) * gP.W;\n                    const float* ptrBase = input + hOffset + wStartBase;\n\n                    // block-wise load\n                    for (int off = threadIdx.x; off < requiredWidth;\n                         off += blockDim.x) {\n                        int iw = wStartBase + off;\n                        float v = -FLT_MAX;\n                        if (iw >= 0 && iw < gP.W) {\n                            v = ptrBase[off];\n                        }\n                        s_line[off] = v;\n                    }\n                    __syncthreads();\n\n                    if (active) {\n                        const int localStart = threadIdx.x * gP.stride;\n                        float localMax = -FLT_MAX;\n                        #pragma unroll\n                        for (int kw = 0; kw < 7; ++kw) {  // k \u22647 (by problem statement)\n                            if (kw >= gP.k) break;\n                            int idx = localStart + kw * gP.dil;\n                            if (idx >= requiredWidth) break;\n                            float v = s_line[idx];\n                            localMax = (v > localMax) ? v : localMax;\n                        }\n                        maxVal = (localMax > maxVal) ? localMax : maxVal;\n                    }\n                    __syncthreads(); // ensure buffer not in use before reload\n                }\n                else {\n                    // ----------------------------------------------\n                    // Fallback path \u2013 direct global memory accesses\n                    // ----------------------------------------------\n                    if (!active) continue;\n                    int hOffset = ((dOffset + ih) * gP.W);\n                    const int wStart = ow * gP.stride - gP.pad;\n                    float localMax = -FLT_MAX;\n\n                    for (int kw = 0, iw = wStart;\n                         kw < gP.k;\n                         ++kw, iw += gP.dil) {\n                        if (iw < 0 || iw >= gP.W) continue;\n                        float v = input[hOffset + iw];\n                        localMax = (v > localMax) ? v : localMax;\n                    }\n                    maxVal = (localMax > maxVal) ? localMax : maxVal;\n                }\n            }\n        }\n\n        // --------------------------------------------------------------\n        // Write-back\n        // --------------------------------------------------------------\n        if (active) {\n            int linearOut =\n                (((((n * gP.C + c) * gP.OD + od) * gP.OH + oh) * gP.OW) + ow);\n            output[linearOut] = maxVal;\n        }\n    }\n}\n\n// ------------------------------------------------------------\n// C++ Wrapper \u2013 interface remains unchanged\n// ------------------------------------------------------------\ntorch::Tensor kb_43_Max_Pooling_3D_wrapper(torch::Tensor arg0,\n                                           int64_t       arg1,\n                                           int64_t       arg2,\n                                           int64_t       arg3,\n                                           int64_t       arg4) {\n    // ---- Sanity checks ----------------------------------------------------\n    TORCH_CHECK(arg0.is_cuda(),     \"Input tensor must be on CUDA device\");\n    TORCH_CHECK(arg0.dtype() == torch::kFloat32, \"Input tensor must be float32\");\n    TORCH_CHECK(arg0.dim()  == 5,   \"Input must be a 5D tensor in NCDHW format\");\n    TORCH_CHECK(arg1 > 0,           \"kernel_size must be > 0\");\n    TORCH_CHECK(arg2 > 0,           \"stride must be > 0\");\n    TORCH_CHECK(arg3 >= 0,          \"padding must be >= 0\");\n    TORCH_CHECK(arg4 > 0,           \"dilation must be > 0\");\n\n    auto x = arg0.contiguous();\n    TORCH_CHECK(x.numel() < INT_MAX, \"Tensor is too large for 32-bit indexing\");\n\n    // ---- Gather dimensions ------------------------------------------------\n    const int64_t N = x.size(0);\n    const int64_t C = x.size(1);\n    const int64_t D = x.size(2);\n    const int64_t H = x.size(3);\n    const int64_t W = x.size(4);\n\n    const int64_t kernel_size = arg1;\n    const int64_t stride      = arg2;\n    const int64_t padding     = arg3;\n    const int64_t dilation    = arg4;\n\n    // ---- Compute output size (ceil_mode = false) --------------------------\n    const int64_t k_eff = 1 + (kernel_size - 1) * dilation;\n    auto out_size = [&](int64_t in) {\n        int64_t out = (in + 2 * padding - k_eff) / stride + 1;\n        return std::max<int64_t>(out, 0);\n    };\n    const int64_t OD = out_size(D);\n    const int64_t OH = out_size(H);\n    const int64_t OW = out_size(W);\n    TORCH_CHECK(OD > 0 && OH > 0 && OW > 0,\n                \"Computed output dimensions must be > 0\");\n\n    // ---- Allocate output --------------------------------------------------\n    auto out = torch::empty({N, C, OD, OH, OW}, x.options());\n\n    // ---- Prepare constant-memory payload ----------------------------------\n    Pool3DConst hostP;\n    hostP.N  = static_cast<int>(N);\n    hostP.C  = static_cast<int>(C);\n    hostP.D  = static_cast<int>(D);\n    hostP.H  = static_cast<int>(H);\n    hostP.W  = static_cast<int>(W);\n\n    hostP.OD = static_cast<int>(OD);\n    hostP.OH = static_cast<int>(OH);\n    hostP.OW = static_cast<int>(OW);\n\n    hostP.k      = static_cast<int>(kernel_size);\n    hostP.stride = static_cast<int>(stride);\n    hostP.pad    = static_cast<int>(padding);\n    hostP.dil    = static_cast<int>(dilation);\n\n    int64_t total64 = N * C * OD * OH * OW;\n    TORCH_CHECK(total64 < INT_MAX,\n                \"Total number of output elements exceeds 32-bit range\");\n    hostP.total = static_cast<int>(total64);\n\n    // copy to device constant memory\n    cudaMemcpyToSymbolAsync(gP, &hostP, sizeof(Pool3DConst), 0,\n                            cudaMemcpyHostToDevice,\n                            at::cuda::getCurrentCUDAStream());\n\n    // ---- Launch kernel ----------------------------------------------------\n    const int threads = 256;\n    int blocks = static_cast<int>((hostP.total + threads - 1) / threads);\n    blocks = std::min(blocks, 65535); // stay within grid-size limit\n\n    auto stream = at::cuda::getCurrentCUDAStream();\n    MaxPool3dKernel<<<blocks, threads, 0, stream.stream()>>>(\n        x.data_ptr<float>(), out.data_ptr<float>());\n\n    // ---- Error check ------------------------------------------------------\n    cudaError_t err = cudaGetLastError();\n    TORCH_CHECK(err == cudaSuccess,\n                \"MaxPool3dKernel launch failed: \",\n                cudaGetErrorString(err));\n\n    return out;\n}"
  },
  {
    "round": 5,
    "goal": "Implement compact shared memory tiling for small adjacent output position groups (e.g., 2x2x1 tile), with cooperative loading of minimal overlapping input patches (\u22642KB/block) for reuse in max computations, to slash redundant global accesses while preserving register-limited occupancy (8 blocks/SM).",
    "status": "Success (New Best)",
    "time_ms": 6.905958557128907,
    "ptxas_metrics": {
      "registers_used": 32.0,
      "shared_mem_bytes": 0.0,
      "spill_bytes": 0.0
    },
    "all_ncu_metrics": {
      "DRAMFrequency": 1512340437.38,
      "SMFrequency": 1065068770.11,
      "ElapsedCycles": 7345601.0,
      "MemoryThroughput": 34.83,
      "DRAMThroughput": 34.83,
      "Duration": 6896352.0,
      "L1TEXCacheThroughput": 33.33,
      "L2CacheThroughput": 50.82,
      "SMActiveCycles": 7327896.37,
      "ComputeSMThroughput": 72.72,
      "BlockSize": 256.0,
      "GridSize": 65535.0,
      "RegistersPerThread": 32.0,
      "SharedMemoryConfigurationSize": 32768.0,
      "DriverSharedMemoryPerBlock": 1024.0,
      "DynamicSharedMemoryPerBlock": 0.0,
      "StaticSharedMemoryPerBlock": 0.0,
      "Threads": 16776960.0,
      "WavesPerSM": 75.85,
      "BlockLimitSM": 32.0,
      "BlockLimitRegisters": 8.0,
      "BlockLimitSharedMem": 32.0,
      "BlockLimitWarps": 8.0,
      "TheoreticalActiveWarpsperSM": 64.0,
      "TheoreticalOccupancy": 100.0,
      "AchievedOccupancy": 93.63,
      "AchievedActiveWarpsPerSM": 59.92
    },
    "selected_ncu_metrics": [
      "DRAMThroughput",
      "L1TEXCacheThroughput",
      "StaticSharedMemoryPerBlock",
      "DynamicSharedMemoryPerBlock",
      "AchievedOccupancy"
    ],
    "details": "Performance improved from 6.908 ms to 6.906 ms.",
    "code": "#include <torch/extension.h>\n\n// C++ Wrapper \u51fd\u6570\u58f0\u660e (\u7b7e\u540d)\ntorch::Tensor kb_43_Max_Pooling_3D_wrapper(torch::Tensor arg0,\n                                           int64_t       arg1,\n                                           int64_t       arg2,\n                                           int64_t       arg3,\n                                           int64_t       arg4);\n\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <ATen/cuda/CUDAContext.h>\n#include <cfloat>\n#include <climits>\n\n// ------------------------------------------------------------\n// Tiled-kernel compile-time knobs  (\u2264 2 KB/block guarantee)\n// ------------------------------------------------------------\n#define TILE_OH            2\n#define TILE_OW            2\n#define MAX_SHARED_BYTES 2048\n\n// ------------------------------------------------------------\n// Constant-memory block that keeps all invariant hyper-parameters\n// ------------------------------------------------------------\nstruct Pool3DConst {\n    int N, C, D, H, W;      // input tensor sizes\n    int OD, OH, OW;         // output tensor sizes\n    int k, stride, pad, dil;\n    int total;              // total number of output elements (\u2264 INT_MAX)\n};\n__constant__ Pool3DConst gP;\n\n// ------------------------------------------------------------\n// (Optional) helper \u2013 unused in max-pool kernel but kept from\n// the original file to satisfy build dependencies.\n// ------------------------------------------------------------\n__device__ float blockReduceSum(float val, float* shared) {\n    int lane = threadIdx.x % warpSize;\n    int wid  = threadIdx.x / warpSize;\n\n    // Warp \u5185\u5f52\u7ea6\n    for (int offset = warpSize / 2; offset > 0; offset >>= 1) {\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    }\n\n    // \u6bcf\u4e2a warp \u7684\u7b2c\u4e00\u4e2a\u7ebf\u7a0b\u5199\u5165 shared\n    if (lane == 0) shared[wid] = val;\n    __syncthreads();\n\n    // \u7b2c\u4e00\u4e2a warp \u5b8c\u6210\u6700\u7ec8\u5f52\u7ea6\n    val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0.0f;\n    if (wid == 0) {\n        for (int offset = warpSize / 2; offset > 0; offset >>= 1) {\n            val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n        }\n    }\n    return val;\n}\n\n// ------------------------------------------------------------\n// NEW: Tiled 3-D MaxPooling kernel (\u22642 KB dyn. shared-mem)\n// ------------------------------------------------------------\n__global__ __launch_bounds__(32, 8) __attribute__((maxrregcount(32)))\nvoid MaxPool3dKernelTile(const float* __restrict__ input,\n                         float*       __restrict__ output) {\n    extern __shared__ float sPatch[];\n\n    const int ip_h = gP.k + (TILE_OH - 1) * gP.stride;\n    const int ip_w = gP.k + (TILE_OW - 1) * gP.stride;\n    const int ip_d = gP.k;\n    const int patch_elems = ip_d * ip_h * ip_w;\n\n    const int tid = threadIdx.x;\n\n    // --- Decode blockIdx ---------------------------------------------------\n    const int ow_tile = blockIdx.x;             // tiles in width  dimension\n    const int oh_tile = blockIdx.y;             // tiles in height dimension\n    int tmp          = blockIdx.z;              // pack (n,c,od)\n    const int od     = tmp % gP.OD;  tmp /= gP.OD;\n    const int  c     = tmp % gP.C;   tmp /= gP.C;\n    const int  n     = tmp;                     // remaining\n\n    const int ow_base = ow_tile * TILE_OW;\n    const int oh_base = oh_tile * TILE_OH;\n\n    // -----------------------------------------------------------------------\n    // Cooperative patch load (all 32 threads)\n    // -----------------------------------------------------------------------\n    for (int idx = tid; idx < patch_elems; idx += blockDim.x) {\n        int t = idx;\n        const int iw = t % ip_w; t /= ip_w;\n        const int ih = t % ip_h; t /= ip_h;\n        const int id = t;                       // 0 .. k-1\n\n        const int g_w = ow_base * gP.stride - gP.pad + iw;\n        const int g_h = oh_base * gP.stride - gP.pad + ih;\n        const int g_d = od       * gP.stride - gP.pad + id * gP.dil;\n\n        float v = -FLT_MAX;\n        if ((unsigned)g_w < (unsigned)gP.W &&\n            (unsigned)g_h < (unsigned)gP.H &&\n            (unsigned)g_d < (unsigned)gP.D) {\n            int gidx = ((((n * gP.C + c) * gP.D + g_d) * gP.H + g_h) * gP.W + g_w);\n            v = input[gidx];\n        }\n        sPatch[idx] = v;\n    }\n    __syncthreads();\n\n    // -----------------------------------------------------------------------\n    // First 4 threads compute the TILE_OH\u00d7TILE_OW outputs\n    // -----------------------------------------------------------------------\n    const int t_h = tid & 1;          //      0 / 1\n    const int t_w = (tid >> 1) & 1;   //      0 / 1\n\n    if (t_h >= TILE_OH || t_w >= TILE_OW) return; // remaining threads exit\n\n    const int oh = oh_base + t_h;\n    const int ow = ow_base + t_w;\n    if (oh >= gP.OH || ow >= gP.OW) return;       // boundary guard\n\n    float maxVal = -FLT_MAX;\n\n    for (int kd = 0; kd < gP.k; ++kd) {\n        for (int kh = 0; kh < gP.k; ++kh) {\n            for (int kw = 0; kw < gP.k; ++kw) {\n                int ip_h_off = kh + t_h * gP.stride;\n                int ip_w_off = kw + t_w * gP.stride;\n                int sidx = ((kd * ip_h) + ip_h_off) * ip_w + ip_w_off;\n                maxVal = fmaxf(maxVal, sPatch[sidx]);\n            }\n        }\n    }\n\n    int out_idx = ((((n * gP.C + c) * gP.OD + od) * gP.OH + oh) * gP.OW + ow);\n    output[out_idx] = maxVal;\n}\n\n// ------------------------------------------------------------\n// Reference 3-D MaxPooling kernel (unchanged)\n// ------------------------------------------------------------\n__global__ __launch_bounds__(256, 4) __attribute__((maxrregcount(32)))\nvoid MaxPool3dKernel(const float* __restrict__ input,\n                     float*       __restrict__ output) {\n    const int total = gP.total;                      // constant-mem fetch\n    const int strideGrid = blockDim.x * gridDim.x;   // 32-bit\n    int tid = blockIdx.x * blockDim.x + threadIdx.x; // 32-bit\n\n    for (int linear = tid; linear < total; linear += strideGrid) {\n        int tmp = linear;\n\n        // De-linearise (OW,OH,OD,C,N)\n        const int ow = tmp % gP.OW; tmp /= gP.OW;\n        const int oh = tmp % gP.OH; tmp /= gP.OH;\n        const int od = tmp % gP.OD; tmp /= gP.OD;\n        const int  c = tmp % gP.C;  tmp /= gP.C;\n        const int  n = tmp;                    // remaining\n\n        const int nc = n * gP.C + c;           // reused composite\n        const int hStart = oh * gP.stride - gP.pad;\n        const int wStart = ow * gP.stride - gP.pad;\n        const int dStart = od * gP.stride - gP.pad;\n\n        float maxVal = -FLT_MAX;\n\n        // depth loop\n        for (int kd = 0, id = dStart; kd < gP.k; ++kd, id += gP.dil) {\n            if (static_cast<unsigned>(id) >= static_cast<unsigned>(gP.D)) continue;\n\n            int dOffset = (nc * gP.D + id) * gP.H; // ((n*C+c)*D + id) * H\n\n            // height loop\n            for (int kh = 0, ih = hStart; kh < gP.k; ++kh, ih += gP.dil) {\n                if (static_cast<unsigned>(ih) >= static_cast<unsigned>(gP.H)) continue;\n\n                int hOffset = (dOffset + ih) * gP.W; // ... + ih) * W\n\n                // width loop\n                for (int kw = 0, iw = wStart; kw < gP.k; ++kw, iw += gP.dil) {\n                    if (static_cast<unsigned>(iw) >= static_cast<unsigned>(gP.W)) continue;\n\n                    float v = input[hOffset + iw];\n                    if (v > maxVal) maxVal = v;\n                }\n            }\n        }\n\n        output[linear] = maxVal;\n    }\n}\n\n// ------------------------------------------------------------\n// C++ Wrapper \u2013 interface remains unchanged\n// ------------------------------------------------------------\ntorch::Tensor kb_43_Max_Pooling_3D_wrapper(torch::Tensor arg0,\n                                           int64_t       arg1,\n                                           int64_t       arg2,\n                                           int64_t       arg3,\n                                           int64_t       arg4) {\n    // ---- Sanity checks ----------------------------------------------------\n    TORCH_CHECK(arg0.is_cuda(),     \"Input tensor must be on CUDA device\");\n    TORCH_CHECK(arg0.dtype() == torch::kFloat32, \"Input tensor must be float32\");\n    TORCH_CHECK(arg0.dim()  == 5,   \"Input must be a 5D tensor in NCDHW format\");\n    TORCH_CHECK(arg1 > 0,           \"kernel_size must be > 0\");\n    TORCH_CHECK(arg2 > 0,           \"stride must be > 0\");\n    TORCH_CHECK(arg3 >= 0,          \"padding must be >= 0\");\n    TORCH_CHECK(arg4 > 0,           \"dilation must be > 0\");\n\n    auto x = arg0.contiguous();\n    TORCH_CHECK(x.numel() < INT_MAX, \"Tensor is too large for 32-bit indexing\");\n\n    // ---- Gather dimensions ------------------------------------------------\n    const int64_t N = x.size(0);\n    const int64_t C = x.size(1);\n    const int64_t D = x.size(2);\n    const int64_t H = x.size(3);\n    const int64_t W = x.size(4);\n\n    const int64_t kernel_size = arg1;\n    const int64_t stride      = arg2;\n    const int64_t padding     = arg3;\n    const int64_t dilation    = arg4;\n\n    // ---- Compute output size (ceil_mode = false) --------------------------\n    const int64_t k_eff = 1 + (kernel_size - 1) * dilation;\n    auto out_size = [&](int64_t in) {\n        int64_t out = (in + 2 * padding - k_eff) / stride + 1;\n        return std::max<int64_t>(out, 0);\n    };\n    const int64_t OD = out_size(D);\n    const int64_t OH = out_size(H);\n    const int64_t OW = out_size(W);\n    TORCH_CHECK(OD > 0 && OH > 0 && OW > 0,\n                \"Computed output dimensions must be > 0\");\n\n    // ---- Allocate output --------------------------------------------------\n    auto out = torch::empty({N, C, OD, OH, OW}, x.options());\n\n    // ---- Prepare constant-memory payload ----------------------------------\n    Pool3DConst hostP;\n    hostP.N  = static_cast<int>(N);\n    hostP.C  = static_cast<int>(C);\n    hostP.D  = static_cast<int>(D);\n    hostP.H  = static_cast<int>(H);\n    hostP.W  = static_cast<int>(W);\n\n    hostP.OD = static_cast<int>(OD);\n    hostP.OH = static_cast<int>(OH);\n    hostP.OW = static_cast<int>(OW);\n\n    hostP.k      = static_cast<int>(kernel_size);\n    hostP.stride = static_cast<int>(stride);\n    hostP.pad    = static_cast<int>(padding);\n    hostP.dil    = static_cast<int>(dilation);\n\n    int64_t total64 = N * C * OD * OH * OW;\n    TORCH_CHECK(total64 < INT_MAX,\n                \"Total number of output elements exceeds 32-bit range\");\n    hostP.total = static_cast<int>(total64);\n\n    // copy to device constant memory\n    cudaMemcpyToSymbolAsync(gP, &hostP, sizeof(Pool3DConst), 0,\n                            cudaMemcpyHostToDevice,\n                            at::cuda::getCurrentCUDAStream());\n\n    // ---- Launch kernel ----------------------------------------------------\n    const int threads = 256;\n    int blocks = static_cast<int>((hostP.total + threads - 1) / threads);\n    blocks = std::min(blocks, 65535); // stay within grid-size limit\n\n    auto stream = at::cuda::getCurrentCUDAStream();\n    MaxPool3dKernel<<<blocks, threads, 0, stream.stream()>>>(\n        x.data_ptr<float>(), out.data_ptr<float>());\n\n    // ---- Error check ------------------------------------------------------\n    cudaError_t err = cudaGetLastError();\n    TORCH_CHECK(err == cudaSuccess,\n                \"MaxPool3dKernel launch failed: \",\n                cudaGetErrorString(err));\n\n    return out;\n}"
  }
]