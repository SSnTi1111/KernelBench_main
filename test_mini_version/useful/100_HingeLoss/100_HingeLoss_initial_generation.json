[
  {
    "attempt": 0,
    "type": "generation",
    "response": "--- C++ 签名 (`cpp`) ---\n```cpp\n#include <torch/extension.h>\n\n// C++ Wrapper 函数声明 (签名)\ntorch::Tensor kb_100_HingeLoss_wrapper(torch::Tensor arg0, torch::Tensor arg1);\n```\n\n--- C++/CUDA 组合实现 (`cu`) ---\n```cu\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <cmath>\n#include <ATen/cuda/CUDAContext.h>\n\n// Block reduction helper using shared memory and warp shuffle\n__device__ float blockReduceSum(float val, float* shared) {\n    const int warpSize = 32;\n    int lane = threadIdx.x % warpSize;\n    int wid = threadIdx.x / warpSize;\n\n    // Reduce within each warp\n    for (int offset = warpSize / 2; offset > 0; offset /= 2) {\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    }\n\n    // Write warp results to shared memory\n    if (lane == 0) {\n        shared[wid] = val;\n    }\n    __syncthreads();\n\n    // First warp reduces the shared memory results\n    if (wid == 0) {\n        val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0.0f;\n        for (int offset = warpSize / 2; offset > 0; offset /= 2) {\n            val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n        }\n    }\n    return val;\n}\n\n// CUDA kernel to compute hinge loss: mean(clamp(1 - predictions * targets, min=0))\n__global__ void kernel(\n    const float* __restrict__ predictions,\n    const float* __restrict__ targets,\n    float* __restrict__ output,\n    int64_t N,\n    int64_t M\n) {\n    extern __shared__ float shared_mem[];\n    float* shared = shared_mem;\n\n    float sum = 0.0f;\n    int64_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    int64_t total_threads = gridDim.x * blockDim.x;\n\n    // Grid-stride loop over all elements\n    for (int64_t idx = tid; idx < N * M; idx += total_threads) {\n        int64_t i = idx / M; // row index in predictions\n        int64_t j = idx % M; // column index in predictions\n\n        // Broadcast targets[j] across the row\n        float pred = predictions[idx];\n        float target = targets[j];\n        float loss_val = fmaxf(0.0f, 1.0f - pred * target);\n        sum += loss_val;\n    }\n\n    // Reduce sum within block\n    sum = blockReduceSum(sum, shared);\n\n    // Write block result to global memory (only first thread of first warp)\n    if (threadIdx.x == 0) {\n        atomicAdd(output, sum);\n    }\n}\n\n// C++ Wrapper implementation\ntorch::Tensor kb_100_HingeLoss_wrapper(torch::Tensor arg0, torch::Tensor arg1) {\n    // Input validation\n    TORCH_CHECK(arg0.is_cuda(), \"arg0 must be a CUDA tensor\");\n    TORCH_CHECK(arg1.is_cuda(), \"arg1 must be a CUDA tensor\");\n    TORCH_CHECK(arg0.dtype() == torch::kFloat32, \"arg0 must be float32\");\n    TORCH_CHECK(arg1.dtype() == torch::kFloat32, \"arg1 must be float32\");\n    TORCH_CHECK(arg0.dim() == 2, \"arg0 must be 2D\");\n    TORCH_CHECK(arg1.dim() == 1, \"arg1 must be 1D\");\n\n    auto N = arg0.size(0);\n    auto M = arg0.size(1);\n    TORCH_CHECK(arg1.size(0) == M, \"arg1 size must match arg0's second dimension\");\n\n    // Allocate output tensor (scalar on GPU)\n    auto options = torch::TensorOptions().dtype(torch::kFloat32).device(arg0.device());\n    torch::Tensor output = torch::zeros({}, options);\n\n    // Configure kernel launch parameters\n    const int threads_per_block = 256;\n    const int max_blocks = 65535; // GPU limit\n    int64_t total_elements = N * M;\n    int64_t blocks_needed = (total_elements + threads_per_block - 1) / threads_per_block;\n    int blocks = static_cast<int>(std::min(blocks_needed, static_cast<int64_t>(max_blocks)));\n\n    // Shared memory for block reduction\n    size_t shared_mem_size = (threads_per_block / 32) * sizeof(float);\n\n    // Launch kernel\n    kernel<<<blocks, threads_per_block, shared_mem_size, at::cuda::getCurrentCUDAStream()>>>(\n        arg0.data_ptr<float>(),\n        arg1.data_ptr<float>(),\n        output.data_ptr<float>(),\n        N,\n        M\n    );\n\n    // Normalize by total number of elements to get mean\n    float scale = 1.0f / static_cast<float>(total_elements);\n    output.mul_(scale);\n\n    return output;\n}\n```",
    "cpp_code_extracted": true,
    "cuda_code_extracted": true,
    "error": ""
  }
]