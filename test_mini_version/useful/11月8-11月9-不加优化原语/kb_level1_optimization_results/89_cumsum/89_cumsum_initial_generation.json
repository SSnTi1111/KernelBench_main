[
  {
    "attempt": 0,
    "type": "generation",
    "response": "--- C++ 签名 (`cpp`) ---\n```cpp\n#include <torch/extension.h>\n\n// C++ Wrapper 函数声明 (签名)\ntorch::Tensor kb_89_cumsum_wrapper(torch::Tensor arg0, int64_t arg1);\n```\n\n--- C++/CUDA 组合实现 (`cu`) ---\n```cu\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <cmath>\n#include <vector>\n\n// PyTorch ≥2.1\n#include <ATen/cuda/CUDAContext.h>\n\n// 可选：块级求和辅助函数（当前实现未在扫描中使用，保留示例）\n__device__ float blockReduceSum(float val, float* shared) {\n    int lane = threadIdx.x & 31;       // warp 内线程索引\n    int wid  = threadIdx.x >> 5;       // warp 号\n\n    // Warp 内归约\n    for (int offset = 16; offset > 0; offset >>= 1) {\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    }\n\n    // 每个 warp 的 lane0 写入共享内存\n    if (lane == 0) shared[wid] = val;\n    __syncthreads();\n\n    // 仅第一个 warp 继续归约\n    val = (threadIdx.x < blockDim.x / 32) ? shared[lane] : 0.f;\n    if (wid == 0) {\n        for (int offset = 16; offset > 0; offset >>= 1) {\n            val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n        }\n    }\n    return val;\n}\n\n/*\n * 简易 2-D 累加和（prefix-sum）Kernel\n *  - dim == 0 : 沿行方向 (跨 batch) 前缀和\n *  - dim == 1 : 沿列方向 (每 batch 内) 前缀和\n *\n * 说明:\n *   • 为演示/教学目的，每行(或每列)由单线程顺序扫描。\n *   • 对于大规模数据，可采用平衡树扫描、分块扫描等算法加速。\n */\n__global__ void cumsum_kernel_2d(\n        const float* __restrict__ input,\n        float*       __restrict__ output,\n        int64_t rows,            // size(0)\n        int64_t cols,            // size(1)\n        int64_t dim)             // 0 或 1\n{\n    if (dim == 1) {\n        // 每个线程处理一整行\n        int64_t row = blockIdx.x * blockDim.x + threadIdx.x;\n        if (row >= rows) return;\n\n        int64_t offset = row * cols;\n        float acc = 0.f;\n        #pragma unroll 4\n        for (int64_t j = 0; j < cols; ++j) {\n            acc += input[offset + j];\n            output[offset + j] = acc;\n        }\n    } else { // dim == 0\n        // 每个线程处理一整列\n        int64_t col = blockIdx.x * blockDim.x + threadIdx.x;\n        if (col >= cols) return;\n\n        float acc = 0.f;\n        #pragma unroll 4\n        for (int64_t i = 0; i < rows; ++i) {\n            int64_t idx = i * cols + col;\n            acc += input[idx];\n            output[idx] = acc;\n        }\n    }\n}\n\n// C++ Wrapper 实现\ntorch::Tensor kb_89_cumsum_wrapper(torch::Tensor arg0, int64_t arg1) {\n    TORCH_CHECK(arg0.is_cuda(), \"Input tensor must reside on CUDA device\");\n    TORCH_CHECK(arg0.scalar_type() == at::kFloat,\n                \"Only float32 tensors are currently supported\");\n    TORCH_CHECK(arg0.dim() == 2,\n                \"This reference implementation only supports 2-D tensors\");\n\n    // 处理负 dim\n    int64_t dim = arg1;\n    if (dim < 0) dim += arg0.dim();\n    TORCH_CHECK(dim == 0 || dim == 1,\n                \"Dimension out of range (expected 0 or 1)\");\n\n    // 保证连续内存布局\n    auto input  = arg0.contiguous();\n    auto output = at::empty_like(input);\n\n    const int64_t rows = input.size(0);\n    const int64_t cols = input.size(1);\n\n    // 配置 launch 参数\n    constexpr int THREADS = 256;\n    dim3 block(THREADS);\n    dim3 grid;\n\n    if (dim == 1) {          // 每行一个线程\n        grid.x = (rows + THREADS - 1) / THREADS;\n    } else {                 // 每列一个线程\n        grid.x = (cols + THREADS - 1) / THREADS;\n    }\n\n    // 获取当前 CUDA stream\n    cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n\n    // Kernel launch\n    cumsum_kernel_2d<<<grid, block, 0, stream>>>(\n        input.data_ptr<float>(),\n        output.data_ptr<float>(),\n        rows,\n        cols,\n        dim\n    );\n\n    // 异步错误检查（仅在调试时建议开启）\n    // cudaError_t err = cudaGetLastError();\n    // TORCH_CHECK(err == cudaSuccess, \"CUDA kernel failed : \", cudaGetErrorString(err));\n\n    return output;\n}\n```\n",
    "cpp_code_extracted": true,
    "cuda_code_extracted": true,
    "error": ""
  }
]