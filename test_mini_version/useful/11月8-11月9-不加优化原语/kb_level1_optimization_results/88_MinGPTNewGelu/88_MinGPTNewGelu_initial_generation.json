[
  {
    "attempt": 0,
    "type": "generation",
    "response": "--- C++ 签名 (`cpp`) ---\n```cpp\n#include <torch/extension.h>\n\n// C++ Wrapper 函数声明 (签名)\ntorch::Tensor kb_88_MinGPTNewGelu_wrapper(torch::Tensor arg0);\n```\n\n--- C++/CUDA 组合实现 (`cu`) ---\n```cu\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <cmath>\n#include <vector>\n#include <ATen/cuda/CUDAContext.h>\n\n// ======================================\n// CUDA 常量\n// ======================================\nconstexpr float kSqrt2OverPi = 0.7978845608028654f;   // √(2/π)\nconstexpr float kCoeff       = 0.044715f;             // 0.044715\n\n// ======================================\n// (可选) 归约辅助函数示例 —— 当前内核未使用\n// 但给出一个 blockReduceSum 的参考实现，以备复用\n// ======================================\n__device__ float blockReduceSum(float val, float* shared)\n{\n    int lane = threadIdx.x & 31;          // 线程在 warp 内的索引\n    int warp = threadIdx.x >> 5;          // warp 索引\n\n    // Warp 内归约\n    #pragma unroll\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n\n    // 每个 warp 的 lane0 把结果写入 shared\n    if (lane == 0)\n        shared[warp] = val;\n\n    __syncthreads();\n\n    // blockDim.x / 32 个 warp 参与最终归约\n    val = (threadIdx.x < (blockDim.x >> 5)) ? shared[lane] : 0.f;\n\n    // 第 0 个 warp 完成最后一次归约\n    if (warp == 0) {\n        #pragma unroll\n        for (int offset = 16; offset > 0; offset >>= 1)\n            val += __shfl_down_sync(0xffffffff, val, offset);\n    }\n    return val;\n}\n\n// ======================================\n// GELU 核心计算的 __device__ 内联函数\n// ======================================\n__device__ __forceinline__ float gelu(float x)\n{\n    float x_cube = x * x * x;\n    float inner  = kSqrt2OverPi * (x + kCoeff * x_cube);\n    return 0.5f * x * (1.0f + tanhf(inner));\n}\n\n// ======================================\n// CUDA Kernel\n// ======================================\n__global__ void gelu_kernel(\n        const float* __restrict__ input,\n        float*       __restrict__ output,\n        int64_t                numel)\n{\n    int64_t idx    = blockIdx.x * blockDim.x + threadIdx.x;\n    int64_t stride = gridDim.x * blockDim.x;\n\n    while (idx < numel) {\n        output[idx] = gelu(input[idx]);\n        idx += stride;\n    }\n}\n\n// ======================================\n// C++ 包装函数\n// ======================================\ntorch::Tensor kb_88_MinGPTNewGelu_wrapper(torch::Tensor arg0)\n{\n    TORCH_CHECK(arg0.is_cuda(), \"Input must reside on CUDA device\");\n    TORCH_CHECK(arg0.scalar_type() == at::kFloat,\n                \"Only float32 type is supported\");\n    TORCH_CHECK(arg0.is_contiguous(),\n                \"Input tensor must be contiguous\");\n\n    auto out = at::empty_like(arg0);\n\n    const int64_t numel = arg0.numel();\n    const int     threads = 256;\n    const int     blocks  = static_cast<int>((numel + threads - 1) / threads);\n\n    // 为了避免启动过多 block，限制上限 (硬件相关，可按需调整)\n    const int     maxBlocks = 1024;\n    const int     grid      = std::min(blocks, maxBlocks);\n\n    cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n\n    gelu_kernel<<<grid, threads, 0, stream>>>(\n        arg0.data_ptr<float>(),\n        out.data_ptr<float>(),\n        numel);\n\n    // CUDA 内核调用后检查错误（可选）\n    #ifdef __CUDA_ARCH__\n    #else\n    cudaError_t err = cudaGetLastError();\n    if (err != cudaSuccess)\n        throw std::runtime_error(cudaGetErrorString(err));\n    #endif\n\n    return out;\n}\n```\n",
    "cpp_code_extracted": true,
    "cuda_code_extracted": true,
    "error": ""
  }
]