[
  {
    "attempt": 0,
    "type": "generation",
    "response": "```cpp\n#include <torch/extension.h>\n\n// C++ Wrapper 函数声明 (签名)\ntorch::Tensor kb_34_InstanceNorm_wrapper(torch::Tensor arg0);\n```\n\n```cu\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <cmath>\n#include <vector>\n#include <ATen/cuda/CUDAContext.h>\n#include <c10/cuda/CUDAException.h>\n\n// CUDA 辅助函数：Warp 内求和\n__device__ __forceinline__ float warpReduceSum(float val) {\n    for (int offset = warpSize >> 1; offset > 0; offset >>= 1) {\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    }\n    return val;\n}\n\n// CUDA 辅助函数：Block 内求和并广播给所有线程\n// shared 的大小至少为 numWarps = ceil(blockDim.x / warpSize)\n__device__ float blockReduceSumAll(float val, float* shared) {\n    int lane = threadIdx.x & (warpSize - 1);\n    int wid  = threadIdx.x >> 5; // / warpSize\n\n    // 先做 warp 内归约\n    val = warpReduceSum(val);\n\n    // 每个 warp 的 lane 0 写入共享内存\n    if (lane == 0) {\n        shared[wid] = val;\n    }\n    __syncthreads();\n\n    // 由第一个 warp 对所有 warp 的部分和进行归约\n    float res = 0.0f;\n    if (wid == 0) {\n        // 计算本 block 的 warp 数\n        int numWarps = (blockDim.x + warpSize - 1) / warpSize;\n        float v = (lane < numWarps) ? shared[lane] : 0.0f;\n        v = warpReduceSum(v);\n        if (lane == 0) {\n            shared[0] = v; // 将最终结果放到 shared[0]\n        }\n    }\n    __syncthreads();\n    // 广播结果给所有线程\n    res = shared[0];\n    return res;\n}\n\n// CUDA 内核：对输入做 InstanceNorm2d（每个 (n,c) 独立在 HxW 上做均值/方差归一化）\n__global__ void instance_norm_nchw_kernel(const float* __restrict__ x,\n                                          float* __restrict__ y,\n                                          int N, int C, int H, int W,\n                                          float eps) {\n    int nc = blockIdx.x; // 每个 block 处理一个 (n, c)\n    if (nc >= N * C) return;\n\n    int n = nc / C;\n    int c = nc % C;\n\n    int HW = H * W;\n    size_t base = (static_cast<size_t>(n) * C + static_cast<size_t>(c)) * static_cast<size_t>(HW);\n\n    extern __shared__ float sshared[]; // 大小为 numWarps 的共享内存\n\n    // 1) 计算 sum 和 sumsq\n    float thread_sum = 0.0f;\n    float thread_sumsq = 0.0f;\n\n    for (int idx = threadIdx.x; idx < HW; idx += blockDim.x) {\n        float v = x[base + static_cast<size_t>(idx)];\n        thread_sum   += v;\n        thread_sumsq += v * v;\n    }\n\n    // Block 内归约得到总和与平方和（广播到所有线程）\n    float sum   = blockReduceSumAll(thread_sum, sshared);\n    float sumsq = blockReduceSumAll(thread_sumsq, sshared);\n\n    float invHW = 1.0f / static_cast<float>(HW);\n    float mean = sum * invHW;\n    float var = fmaxf(sumsq * invHW - mean * mean, 0.0f);\n    float inv_std = rsqrtf(var + eps);\n\n    // 2) 写归一化输出\n    for (int idx = threadIdx.x; idx < HW; idx += blockDim.x) {\n        float v = x[base + static_cast<size_t>(idx)];\n        y[base + static_cast<size_t>(idx)] = (v - mean) * inv_std;\n    }\n}\n\n// C++ Wrapper 实现\ntorch::Tensor kb_34_InstanceNorm_wrapper(torch::Tensor arg0) {\n    TORCH_CHECK(arg0.is_cuda(), \"kb_34_InstanceNorm_wrapper: arg0 must be a CUDA tensor\");\n    TORCH_CHECK(arg0.dtype() == torch::kFloat32, \"kb_34_InstanceNorm_wrapper: only float32 is supported\");\n    TORCH_CHECK(arg0.dim() == 4, \"kb_34_InstanceNorm_wrapper: expected 4D NCHW tensor\");\n\n    auto x = arg0.contiguous();\n    int64_t N = x.size(0);\n    int64_t C = x.size(1);\n    int64_t H = x.size(2);\n    int64_t W = x.size(3);\n\n    auto y = torch::empty_like(x);\n\n    // 配置 CUDA 启动参数\n    int threads = 256;\n    int64_t blocks64 = N * C;\n    TORCH_CHECK(blocks64 > 0, \"kb_34_InstanceNorm_wrapper: invalid grid size\");\n    TORCH_CHECK(blocks64 <= static_cast<int64_t>(std::numeric_limits<int>::max()),\n                \"kb_34_InstanceNorm_wrapper: grid size exceeds int range\");\n    int blocks = static_cast<int>(blocks64);\n\n    int numWarps = (threads + 31) / 32;\n    size_t shmem = static_cast<size_t>(numWarps) * sizeof(float);\n\n    float eps = 1e-5f;\n\n    auto stream = at::cuda::getCurrentCUDAStream();\n\n    instance_norm_nchw_kernel<<<blocks, threads, shmem, stream.stream()>>>(\n        x.data_ptr<float>(),\n        y.data_ptr<float>(),\n        static_cast<int>(N),\n        static_cast<int>(C),\n        static_cast<int>(H),\n        static_cast<int>(W),\n        eps\n    );\n\n    C10_CUDA_KERNEL_LAUNCH_CHECK();\n    return y;\n}\n```",
    "cpp_code_extracted": true,
    "cuda_code_extracted": true,
    "error": ""
  }
]