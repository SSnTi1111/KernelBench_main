[
  {
    "round": 0,
    "goal": "Baseline",
    "status": "Success",
    "time_ms": 4.371046447753907,
    "ptxas_metrics": {
      "registers_used": 25.0,
      "shared_mem_bytes": 0.0,
      "spill_bytes": 0.0
    },
    "all_ncu_metrics": {
      "DRAMFrequency": 1510626195.69,
      "SMFrequency": 1063794910.2,
      "ElapsedCycles": 4644456.0,
      "MemoryThroughput": 50.74,
      "DRAMThroughput": 50.74,
      "Duration": 4365664.0,
      "L1TEXCacheThroughput": 39.66,
      "L2CacheThroughput": 61.72,
      "SMActiveCycles": 4618823.48,
      "ComputeSMThroughput": 78.74,
      "BlockSize": 256.0,
      "GridSize": 8192.0,
      "RegistersPerThread": 25.0,
      "SharedMemoryConfigurationSize": 32768.0,
      "DriverSharedMemoryPerBlock": 1024.0,
      "DynamicSharedMemoryPerBlock": 0.0,
      "StaticSharedMemoryPerBlock": 0.0,
      "Threads": 2097152.0,
      "WavesPerSM": 9.48,
      "BlockLimitSM": 32.0,
      "BlockLimitRegisters": 8.0,
      "BlockLimitSharedMem": 32.0,
      "BlockLimitWarps": 8.0,
      "TheoreticalActiveWarpsperSM": 64.0,
      "TheoreticalOccupancy": 100.0,
      "AchievedOccupancy": 96.26,
      "AchievedActiveWarpsPerSM": 61.61
    },
    "selected_ncu_metrics": [],
    "details": "Initial baseline measurement",
    "code": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <cmath>\n#include <vector>\n#include <ATen/cuda/CUDAContext.h>\n\n// [\u91cd\u8981] CUDA \u8f85\u52a9\u51fd\u6570\u5728 kernel \u4e4b\u524d\u5b9a\u4e49/\u58f0\u660e\n__device__ float blockReduceSum(float val, float* shared) {\n    int lane = threadIdx.x % warpSize;\n    int wid = threadIdx.x / warpSize;\n\n    // Warp \u5185\u5f52\u7ea6\n    for (int offset = warpSize / 2; offset > 0; offset /= 2) {\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    }\n\n    // \u6bcf\u4e2a warp \u7684\u7b2c\u4e00\u4e2a\u7ebf\u7a0b\u5199\u5165\u5171\u4eab\u5185\u5b58\n    if (lane == 0) {\n        shared[wid] = val;\n    }\n    __syncthreads();\n\n    // \u7b2c\u4e00\u4e2a warp \u8fdb\u884c\u6700\u7ec8\u5f52\u7ea6\n    val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0.0f;\n    if (wid == 0) {\n        for (int offset = warpSize / 2; offset > 0; offset /= 2) {\n            val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n        }\n    }\n    return val;\n}\n\n// CUDA \u5185\u6838\u5b9e\u73b0: 1D Average Pooling (count_include_pad=True)\n__global__ void avg_pool1d_kernel(\n    const float* __restrict__ x,\n    float* __restrict__ y,\n    int N, int C, int L_in,\n    int K, int stride, int pad,\n    int L_out\n) {\n    int c = blockIdx.x;\n    int n = blockIdx.y;\n\n    if (n >= N || c >= C) return;\n\n    int base_in = ((n * C) + c) * L_in;\n    int base_out = ((n * C) + c) * L_out;\n\n    // \u6bcf\u4e2a block \u8986\u76d6\u4e00\u4e2a (n, c) \u5207\u7247\uff0c\u7ebf\u7a0b\u6cbf\u7740\u8f93\u51fa\u957f\u5ea6\u904d\u5386\n    for (int out_idx = threadIdx.x; out_idx < L_out; out_idx += blockDim.x) {\n        int start = out_idx * stride - pad;\n        float sum = 0.0f;\n\n        // \u5bf9\u7a97\u53e3\u6c42\u548c\uff08\u5305\u542b pad\uff0cpad \u533a\u57df\u89c6\u4e3a 0\uff09\n        #pragma unroll\n        for (int k = 0; k < K; ++k) {\n            int idx = start + k;\n            float val = 0.0f;\n            if (idx >= 0 && idx < L_in) {\n                val = x[base_in + idx];\n            }\n            sum += val;\n        }\n\n        y[base_out + out_idx] = sum / static_cast<float>(K);\n    }\n}\n\n// C++ Wrapper \u5b9e\u73b0\ntorch::Tensor kb_1_44_Average_Pooling_1D_wrapper(torch::Tensor arg0, int64_t arg1, int64_t arg2, int64_t arg3) {\n    TORCH_CHECK(arg0.is_cuda(), \"Input tensor must be on CUDA device\");\n    TORCH_CHECK(arg0.dtype() == torch::kFloat32, \"Input tensor must be float32\");\n    TORCH_CHECK(arg0.dim() == 3, \"Input tensor must be 3D (N, C, L_in)\");\n\n    int64_t N64 = arg0.size(0);\n    int64_t C64 = arg0.size(1);\n    int64_t L_in64 = arg0.size(2);\n\n    TORCH_CHECK(arg1 > 0, \"kernel_size must be > 0\");\n    TORCH_CHECK(arg2 > 0, \"stride must be > 0\");\n    TORCH_CHECK(arg3 >= 0, \"padding must be >= 0\");\n\n    int K = static_cast<int>(arg1);\n    int stride = static_cast<int>(arg2);\n    int pad = static_cast<int>(arg3);\n\n    int N = static_cast<int>(N64);\n    int C = static_cast<int>(C64);\n    int L_in = static_cast<int>(L_in64);\n\n    // PyTorch AvgPool1d \u8f93\u51fa\u957f\u5ea6\u516c\u5f0f: floor((L_in + 2*pad - K) / stride) + 1\n    TORCH_CHECK((L_in + 2 * pad - K) >= 0, \"Invalid parameters lead to negative output length numerator\");\n    int L_out = (L_in + 2 * pad - K) / stride + 1;\n\n    auto input = arg0.contiguous();\n    auto options = input.options();\n    torch::Tensor output = torch::empty({N64, C64, static_cast<int64_t>(L_out)}, options);\n\n    const float* x = input.data_ptr<float>();\n    float* y = output.data_ptr<float>();\n\n    dim3 block(256);\n    dim3 grid(C, N);\n\n    cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n\n    avg_pool1d_kernel<<<grid, block, 0, stream>>>(\n        x, y,\n        N, C, L_in,\n        K, stride, pad,\n        L_out\n    );\n\n    TORCH_CHECK(cudaGetLastError() == cudaSuccess, \"avg_pool1d_kernel launch failed\");\n\n    return output;\n}"
  },
  {
    "round": 1,
    "goal": "Refactor the kernel to use shared memory tiling for the input slice to enable intra-block data reuse and minimize global memory traffic.",
    "status": "Failed (Performance Regression)",
    "time_ms": 7.743590545654297,
    "ptxas_metrics": {
      "registers_used": 30.0,
      "shared_mem_bytes": 4096.0,
      "spill_bytes": 0.0
    },
    "all_ncu_metrics": {
      "DRAMFrequency": 1511247622.98,
      "SMFrequency": 1064398198.05,
      "ElapsedCycles": 8239547.0,
      "MemoryThroughput": 28.6,
      "DRAMThroughput": 28.6,
      "Duration": 7740800.0,
      "L1TEXCacheThroughput": 23.54,
      "L2CacheThroughput": 35.55,
      "SMActiveCycles": 8194330.02,
      "ComputeSMThroughput": 79.03,
      "BlockSize": 256.0,
      "GridSize": 8192.0,
      "RegistersPerThread": 30.0,
      "SharedMemoryConfigurationSize": 102400.0,
      "DriverSharedMemoryPerBlock": 1024.0,
      "DynamicSharedMemoryPerBlock": 0.0,
      "StaticSharedMemoryPerBlock": 4096.0,
      "Threads": 2097152.0,
      "WavesPerSM": 9.48,
      "BlockLimitSM": 32.0,
      "BlockLimitRegisters": 8.0,
      "BlockLimitSharedMem": 20.0,
      "BlockLimitWarps": 8.0,
      "TheoreticalActiveWarpsperSM": 64.0,
      "TheoreticalOccupancy": 100.0,
      "AchievedOccupancy": 96.26,
      "AchievedActiveWarpsPerSM": 61.61
    },
    "selected_ncu_metrics": [
      "DRAMThroughput",
      "L1TEXCacheThroughput",
      "SharedMemoryConfigurationSize",
      "StaticSharedMemoryPerBlock",
      "DynamicSharedMemoryPerBlock"
    ],
    "details": "New time 7.744 ms is not better than best time 4.371 ms.",
    "code": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <cmath>\n#include <vector>\n#include <ATen/cuda/CUDAContext.h>\n\n// [\u91cd\u8981] CUDA \u8f85\u52a9\u51fd\u6570\u5728 kernel \u4e4b\u524d\u5b9a\u4e49/\u58f0\u660e\n__device__ float blockReduceSum(float val, float* shared) {\n    int lane = threadIdx.x % warpSize;\n    int wid = threadIdx.x / warpSize;\n\n    // Warp \u5185\u5f52\u7ea6\n    for (int offset = warpSize / 2; offset > 0; offset /= 2) {\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    }\n\n    // \u6bcf\u4e2a warp \u7684\u7b2c\u4e00\u4e2a\u7ebf\u7a0b\u5199\u5165\u5171\u4eab\u5185\u5b58\n    if (lane == 0) {\n        shared[wid] = val;\n    }\n    __syncthreads();\n\n    // \u7b2c\u4e00\u4e2a warp \u8fdb\u884c\u6700\u7ec8\u5f52\u7ea6\n    val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0.0f;\n    if (wid == 0) {\n        for (int offset = warpSize / 2; offset > 0; offset /= 2) {\n            val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n        }\n    }\n    return val;\n}\n\n#ifndef TILE_SIZE\n#define TILE_SIZE 1024  // \u7ea6 4KB \u7684\u5171\u4eab\u5185\u5b58\uff1b\u4fdd\u6301\u8f83\u9ad8\u5360\u7528\n#endif\n\n// CUDA \u5185\u6838\u5b9e\u73b0: 1D Average Pooling (count_include_pad=True) - \u4f7f\u7528\u5171\u4eab\u5185\u5b58\u5206\u5757\u52a0\u901f\n__global__ void avg_pool1d_kernel(\n    const float* __restrict__ x,\n    float* __restrict__ y,\n    int N, int C, int L_in,\n    int K, int stride, int pad,\n    int L_out\n) {\n    int c = blockIdx.x;\n    int n = blockIdx.y;\n\n    if (n >= N || c >= C) return;\n\n    int base_in = ((n * C) + c) * L_in;\n    int base_out = ((n * C) + c) * L_out;\n\n    // \u6bcf\u4e2a block \u5904\u7406\u4e00\u4e2a (n, c) \u5207\u7247\n    __shared__ float s_input[TILE_SIZE];\n\n    // \u8ba1\u7b97\u6bcf\u4e2a tile \u7684\u8f93\u51fa\u6570\u91cf\uff0c\u4f7f\u5176\u8986\u76d6\u7684\u8f93\u5165\u957f\u5ea6\u4e0d\u8d85\u8fc7\u5171\u4eab\u5185\u5b58\u5bb9\u91cf\n    int tile_outs_per_tile;\n    if (K <= TILE_SIZE) {\n        int max_outs_fit = (TILE_SIZE - K) / stride + 1;  // \u6ee1\u8db3 (outs-1)*stride + K <= TILE_SIZE\n        if (max_outs_fit < 1) max_outs_fit = 1;\n        tile_outs_per_tile = blockDim.x;\n        if (tile_outs_per_tile > max_outs_fit) tile_outs_per_tile = max_outs_fit;\n    } else {\n        // K \u8d85\u8fc7\u5171\u4eab\u5185\u5b58\u5bb9\u91cf\uff0c\u9000\u5316\u4e3a\u6bcf tile \u8ba1\u7b97\u4e00\u4e2a\u8f93\u51fa\uff0c\u5e76\u90e8\u5206\u4f7f\u7528\u5171\u4eab\u5185\u5b58\n        tile_outs_per_tile = 1;\n    }\n\n    int num_tiles = (L_out + tile_outs_per_tile - 1) / tile_outs_per_tile;\n\n    for (int tile = 0; tile < num_tiles; ++tile) {\n        int out_start = tile * tile_outs_per_tile;\n        int outs_in_tile = tile_outs_per_tile;\n        int remaining_outs = L_out - out_start;\n        if (outs_in_tile > remaining_outs) outs_in_tile = remaining_outs;\n\n        int in_start = out_start * stride - pad;\n\n        // \u6b64 tile \u9700\u8981\u7684\u8f93\u5165\u8986\u76d6\u957f\u5ea6\uff1a(outs_in_tile - 1) * stride + K\n        int tile_in_len_needed = (outs_in_tile - 1) * stride + K;\n        int tile_in_len_loaded = tile_in_len_needed;\n        if (tile_in_len_loaded > TILE_SIZE) tile_in_len_loaded = TILE_SIZE;\n\n        // \u534f\u4f5c\u52a0\u8f7d\u5171\u4eab\u5185\u5b58\n        for (int offset = threadIdx.x; offset < tile_in_len_loaded; offset += blockDim.x) {\n            int idx = in_start + offset;\n            float val = 0.0f;\n            if (idx >= 0 && idx < L_in) {\n                val = x[base_in + idx];\n            }\n            s_input[offset] = val;\n        }\n        __syncthreads();\n\n        // \u8ba1\u7b97\u6b64 tile \u7684\u8f93\u51fa\n        for (int out_offset = threadIdx.x; out_offset < outs_in_tile; out_offset += blockDim.x) {\n            int out_idx = out_start + out_offset;\n\n            float sum = 0.0f;\n            int s_base = out_offset * stride;\n\n            #pragma unroll\n            for (int k = 0; k < K; ++k) {\n                int s_idx = s_base + k;\n                float v;\n                if (s_idx >= 0 && s_idx < tile_in_len_loaded) {\n                    v = s_input[s_idx];\n                } else {\n                    // \u8d85\u51fa\u5171\u4eab\u5185\u5b58\u5df2\u52a0\u8f7d\u8303\u56f4\uff0c\u56de\u9000\u5230\u5168\u5c40\u5185\u5b58\u8bbf\u95ee\n                    int g_idx = in_start + s_idx;\n                    if (g_idx >= 0 && g_idx < L_in) {\n                        v = x[base_in + g_idx];\n                    } else {\n                        v = 0.0f;\n                    }\n                }\n                sum += v;\n            }\n            y[base_out + out_idx] = sum / static_cast<float>(K);\n        }\n        __syncthreads();\n    }\n}\n\n// C++ Wrapper \u5b9e\u73b0\ntorch::Tensor kb_1_44_Average_Pooling_1D_wrapper(torch::Tensor arg0, int64_t arg1, int64_t arg2, int64_t arg3) {\n    TORCH_CHECK(arg0.is_cuda(), \"Input tensor must be on CUDA device\");\n    TORCH_CHECK(arg0.dtype() == torch::kFloat32, \"Input tensor must be float32\");\n    TORCH_CHECK(arg0.dim() == 3, \"Input tensor must be 3D (N, C, L_in)\");\n\n    int64_t N64 = arg0.size(0);\n    int64_t C64 = arg0.size(1);\n    int64_t L_in64 = arg0.size(2);\n\n    TORCH_CHECK(arg1 > 0, \"kernel_size must be > 0\");\n    TORCH_CHECK(arg2 > 0, \"stride must be > 0\");\n    TORCH_CHECK(arg3 >= 0, \"padding must be >= 0\");\n\n    int K = static_cast<int>(arg1);\n    int stride = static_cast<int>(arg2);\n    int pad = static_cast<int>(arg3);\n\n    int N = static_cast<int>(N64);\n    int C = static_cast<int>(C64);\n    int L_in = static_cast<int>(L_in64);\n\n    // PyTorch AvgPool1d \u8f93\u51fa\u957f\u5ea6\u516c\u5f0f: floor((L_in + 2*pad - K) / stride) + 1\n    TORCH_CHECK((L_in + 2 * pad - K) >= 0, \"Invalid parameters lead to negative output length numerator\");\n    int L_out = (L_in + 2 * pad - K) / stride + 1;\n\n    auto input = arg0.contiguous();\n    auto options = input.options();\n    torch::Tensor output = torch::empty({N64, C64, static_cast<int64_t>(L_out)}, options);\n\n    const float* x = input.data_ptr<float>();\n    float* y = output.data_ptr<float>();\n\n    dim3 block(256);\n    dim3 grid(C, N);\n\n    cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n\n    avg_pool1d_kernel<<<grid, block, 0, stream>>>(\n        x, y,\n        N, C, L_in,\n        K, stride, pad,\n        L_out\n    );\n\n    TORCH_CHECK(cudaGetLastError() == cudaSuccess, \"avg_pool1d_kernel launch failed\");\n\n    return output;\n}"
  },
  {
    "round": 2,
    "goal": "Refactor the kernel to use prefix sums for sliding window sum computation (with padding handling) to load each input element only once, curing redundant global memory accesses.",
    "status": "Failed (Performance Regression)",
    "time_ms": 22.098329162597658,
    "ptxas_metrics": {
      "registers_used": 32.0,
      "shared_mem_bytes": 32780.0,
      "spill_bytes": 0.0
    },
    "all_ncu_metrics": {
      "DRAMFrequency": 1511646774.93,
      "SMFrequency": 1064750133.03,
      "ElapsedCycles": 23509196.0,
      "MemoryThroughput": 53.67,
      "DRAMThroughput": 10.02,
      "Duration": 22079520.0,
      "L1TEXCacheThroughput": 53.92,
      "L2CacheThroughput": 12.1,
      "SMActiveCycles": 23398082.83,
      "ComputeSMThroughput": 51.64,
      "BlockSize": 256.0,
      "GridSize": 8192.0,
      "RegistersPerThread": 32.0,
      "SharedMemoryConfigurationSize": 167936.0,
      "DriverSharedMemoryPerBlock": 1024.0,
      "DynamicSharedMemoryPerBlock": 0.0,
      "StaticSharedMemoryPerBlock": 32780.0,
      "Threads": 2097152.0,
      "WavesPerSM": 18.96,
      "BlockLimitSM": 32.0,
      "BlockLimitRegisters": 8.0,
      "BlockLimitSharedMem": 4.0,
      "BlockLimitWarps": 8.0,
      "TheoreticalActiveWarpsperSM": 32.0,
      "TheoreticalOccupancy": 50.0,
      "AchievedOccupancy": 49.55,
      "AchievedActiveWarpsPerSM": 31.71
    },
    "selected_ncu_metrics": [
      "DRAMThroughput",
      "MemoryThroughput",
      "L1TEXCacheThroughput",
      "L2CacheThroughput"
    ],
    "details": "New time 22.098 ms is not better than best time 4.371 ms.",
    "code": "#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <cmath>\n#include <vector>\n#include <ATen/cuda/CUDAContext.h>\n#include <climits>\n\n// [\u91cd\u8981] CUDA \u8f85\u52a9\u51fd\u6570\u5728 kernel \u4e4b\u524d\u5b9a\u4e49/\u58f0\u660e\n__device__ float blockReduceSum(float val, float* shared) {\n    int lane = threadIdx.x % warpSize;\n    int wid = threadIdx.x / warpSize;\n\n    // Warp \u5185\u5f52\u7ea6\n    for (int offset = warpSize / 2; offset > 0; offset /= 2) {\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    }\n\n    // \u6bcf\u4e2a warp \u7684\u7b2c\u4e00\u4e2a\u7ebf\u7a0b\u5199\u5165\u5171\u4eab\u5185\u5b58\n    if (lane == 0) {\n        shared[wid] = val;\n    }\n    __syncthreads();\n\n    // \u7b2c\u4e00\u4e2a warp \u8fdb\u884c\u6700\u7ec8\u5f52\u7ea6\n    val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0.0f;\n    if (wid == 0) {\n        for (int offset = warpSize / 2; offset > 0; offset /= 2) {\n            val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n        }\n    }\n    return val;\n}\n\n// CUDA \u5185\u6838\u5b9e\u73b0: 1D Average Pooling (count_include_pad=True)\n__global__ void avg_pool1d_kernel(\n    const float* __restrict__ x,\n    float* __restrict__ y,\n    int N, int C, int L_in,\n    int K, int stride, int pad,\n    int L_out\n) {\n    int c = blockIdx.x;\n    int n = blockIdx.y;\n\n    if (n >= N || c >= C) return;\n\n    int base_in = ((n * C) + c) * L_in;\n    int base_out = ((n * C) + c) * L_out;\n\n    // \u4f7f\u7528\u5171\u4eab\u5185\u5b58\u5bf9\u6bcf\u4e00\u6279\u6b21\u7684\u8f93\u51fa\u5757\u8fdb\u884c\u534f\u540c\u52a0\u8f7d\u4e0e\u524d\u7f00\u548c\u8ba1\u7b97\n    const int MAX_TILE = 4096;  // \u5171\u4eab\u5185\u5b58 tile \u7684\u6700\u5927\u957f\u5ea6\uff08\u53ef\u6839\u636e\u8bbe\u5907\u8d44\u6e90\u8c03\u6574\uff09\n    __shared__ float s_tile[MAX_TILE];\n    __shared__ float s_prefix[MAX_TILE + 1];\n    __shared__ int s_min_start;\n    __shared__ int s_max_start;\n\n    // \u4ee5 blockDim.x \u4e3a\u6b65\u957f\u5206\u6279\u5904\u7406\u8f93\u51fa\u4f4d\u7f6e\uff0c\u4fbf\u4e8e\u534f\u540c\u52a0\u8f7d\u8f93\u5165\u8303\u56f4\n    for (int base_out_idx = 0; base_out_idx < L_out; base_out_idx += blockDim.x) {\n        int out_idx = base_out_idx + threadIdx.x;\n\n        // \u521d\u59cb\u5316\u5171\u4eab\u7684\u6700\u5c0f/\u6700\u5927\u8d77\u70b9\n        if (threadIdx.x == 0) {\n            s_min_start = INT_MAX;\n            s_max_start = INT_MIN;\n        }\n        __syncthreads();\n\n        // \u8ba1\u7b97\u5f53\u524d\u7ebf\u7a0b\u7684\u7a97\u53e3\u8d77\u70b9\n        int start = 0;\n        if (out_idx < L_out) {\n            start = out_idx * stride - pad;\n            atomicMin(&s_min_start, start);\n            atomicMax(&s_max_start, start);\n        }\n        __syncthreads();\n\n        // \u8ba1\u7b97\u9700\u8981\u534f\u540c\u52a0\u8f7d\u7684\u8f93\u5165\u533a\u95f4 [eff_start, eff_end)\n        int eff_start = max(0, s_min_start);\n        int eff_end = min(L_in, s_max_start + K);\n        int tile_len = max(0, eff_end - eff_start);\n\n        if (tile_len > 0 && tile_len <= MAX_TILE) {\n            // \u534f\u540c\u52a0\u8f7d\u8f93\u5165\u5230\u5171\u4eab\u5185\u5b58\n            for (int i = threadIdx.x; i < tile_len; i += blockDim.x) {\n                s_tile[i] = x[base_in + eff_start + i];\n            }\n            __syncthreads();\n\n            // \u8ba1\u7b97\u8be5 tile \u7684\u524d\u7f00\u548c\uff08s_prefix[0] = 0, s_prefix[i+1] = sum(s_tile[0..i])\uff09\n            if (threadIdx.x == 0) {\n                s_prefix[0] = 0.0f;\n                float acc = 0.0f;\n                for (int i = 0; i < tile_len; ++i) {\n                    acc += s_tile[i];\n                    s_prefix[i + 1] = acc;\n                }\n            }\n            __syncthreads();\n\n            // \u4f7f\u7528\u524d\u7f00\u548c\u8ba1\u7b97\u6bcf\u4e2a\u8f93\u51fa\u7a97\u53e3\u7684\u548c\n            if (out_idx < L_out) {\n                int effective_start = start;\n                int effective_end = start + K;\n                if (effective_start < 0) effective_start = 0;\n                if (effective_end > L_in) effective_end = L_in;\n\n                float window_sum = 0.0f;\n                if (effective_end > effective_start) {\n                    int rel_a = effective_start - eff_start;\n                    int rel_b = effective_end - eff_start;\n                    window_sum = s_prefix[rel_b] - s_prefix[rel_a];\n                }\n\n                y[base_out + out_idx] = window_sum / static_cast<float>(K);\n            }\n        } else {\n            // \u56de\u9000\u5230\u76f4\u63a5\u8bbf\u95ee\u5168\u5c40\u5185\u5b58\u7684\u5b9e\u73b0\uff08\u7a97\u53e3\u6c42\u548c\uff09\n            if (out_idx < L_out) {\n                int s = out_idx * stride - pad;\n                float sum = 0.0f;\n                #pragma unroll\n                for (int k = 0; k < K; ++k) {\n                    int idx = s + k;\n                    if (idx >= 0 && idx < L_in) {\n                        sum += x[base_in + idx];\n                    }\n                }\n                y[base_out + out_idx] = sum / static_cast<float>(K);\n            }\n        }\n\n        __syncthreads();\n    }\n}\n\n// C++ Wrapper \u5b9e\u73b0\ntorch::Tensor kb_1_44_Average_Pooling_1D_wrapper(torch::Tensor arg0, int64_t arg1, int64_t arg2, int64_t arg3) {\n    TORCH_CHECK(arg0.is_cuda(), \"Input tensor must be on CUDA device\");\n    TORCH_CHECK(arg0.dtype() == torch::kFloat32, \"Input tensor must be float32\");\n    TORCH_CHECK(arg0.dim() == 3, \"Input tensor must be 3D (N, C, L_in)\");\n\n    int64_t N64 = arg0.size(0);\n    int64_t C64 = arg0.size(1);\n    int64_t L_in64 = arg0.size(2);\n\n    TORCH_CHECK(arg1 > 0, \"kernel_size must be > 0\");\n    TORCH_CHECK(arg2 > 0, \"stride must be > 0\");\n    TORCH_CHECK(arg3 >= 0, \"padding must be >= 0\");\n\n    int K = static_cast<int>(arg1);\n    int stride = static_cast<int>(arg2);\n    int pad = static_cast<int>(arg3);\n\n    int N = static_cast<int>(N64);\n    int C = static_cast<int>(C64);\n    int L_in = static_cast<int>(L_in64);\n\n    // PyTorch AvgPool1d \u8f93\u51fa\u957f\u5ea6\u516c\u5f0f: floor((L_in + 2*pad - K) / stride) + 1\n    TORCH_CHECK((L_in + 2 * pad - K) >= 0, \"Invalid parameters lead to negative output length numerator\");\n    int L_out = (L_in + 2 * pad - K) / stride + 1;\n\n    auto input = arg0.contiguous();\n    auto options = input.options();\n    torch::Tensor output = torch::empty({N64, C64, static_cast<int64_t>(L_out)}, options);\n\n    const float* x = input.data_ptr<float>();\n    float* y = output.data_ptr<float>();\n\n    dim3 block(256);\n    dim3 grid(C, N);\n\n    cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n\n    avg_pool1d_kernel<<<grid, block, 0, stream>>>(\n        x, y,\n        N, C, L_in,\n        K, stride, pad,\n        L_out\n    );\n\n    TORCH_CHECK(cudaGetLastError() == cudaSuccess, \"avg_pool1d_kernel launch failed\");\n\n    return output;\n}"
  }
]