[
  {
    "attempt": 0,
    "type": "generation",
    "response": "--- C++ 签名 (`cpp`) ---\n```cpp\n#include <torch/extension.h>\n\n// C++ Wrapper 函数声明 (签名)\ntorch::Tensor kb_92_cumsum_exclusive_wrapper(torch::Tensor arg0, int64_t arg1);\n```\n\n--- C++/CUDA 组合实现 (`cu`) ---\n```cu\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <cmath>\n#include <vector>\n#include <ATen/cuda/CUDAContext.h>\n\n// ───────────────────────────  宏 / 辅助检查  ────────────────────────────\n#define CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x \" must be a CUDA tensor\")\n#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n#define CHECK_INPUT(x)  \\\n    CHECK_CUDA(x);      \\\n    CHECK_CONTIGUOUS(x)\n\n// ───────────────────────────  可选的 blockReduceSum  ──────────────────────────\n__device__ float blockReduceSum(float val, float* shared) {\n    int lane = threadIdx.x & 31;          // warp 内索引\n    int wid  = threadIdx.x >> 5;          // warp id\n\n    // Warp 内归约\n    for (int offset = 16; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n\n    // 每个 warp 的线程 0 写共享内存\n    if (lane == 0) shared[wid] = val;\n    __syncthreads();\n\n    // block 内最终归约（仅第一个 warp）\n    val = (threadIdx.x < blockDim.x / 32) ? shared[lane] : 0.0f;\n    if (wid == 0) {\n        for (int offset = 16; offset > 0; offset >>= 1)\n            val += __shfl_down_sync(0xffffffff, val, offset);\n    }\n    return val;\n}\n\n// ───────────────────────────  Kernel for dim == 1  ───────────────────────────\n//  input : (rows_in , cols_in)\n//  output: (rows_in-1 , cols_in+1)\n__global__ void exclusive_cumsum_dim1_kernel(const float* __restrict__ input,\n                                             float* __restrict__ output,\n                                             int rows_in,\n                                             int cols_in) {\n    const int row = blockIdx.x * blockDim.x + threadIdx.x;\n    const int rows_out = rows_in - 1;\n    const int cols_out = cols_in + 1;\n\n    if (row >= rows_out) return;\n\n    const float* in_row  = input  + row * cols_in;\n    float*       out_row = output + row * cols_out;\n\n    float acc = 0.0f;\n    // 第 0 列恒为 0\n    out_row[0] = 0.0f;\n\n    // col 1..cols_out-1\n    for (int col = 1; col < cols_out; ++col) {\n        float val = in_row[col - 1];   // 对应输入列 (col-1)\n        acc += val;\n        out_row[col] = acc;\n    }\n}\n\n// ───────────────────────────  Kernel for dim == 0  ───────────────────────────\n//  input : (rows_in , cols_in)\n//  output: (rows_in , cols_in)\n__global__ void exclusive_cumsum_dim0_kernel(const float* __restrict__ input,\n                                             float* __restrict__ output,\n                                             int rows_in,\n                                             int cols_in) {\n    const int col = blockIdx.x * blockDim.x + threadIdx.x;\n    if (col >= cols_in) return;\n\n    float acc = 0.0f;\n    for (int row = 0; row < rows_in; ++row) {\n        const int idx = row * cols_in + col;\n        output[idx] = acc;          // 写入当前累积和（不含自身 -> exclusive）\n        acc += input[idx];          // 更新累积和\n    }\n}\n\n// ───────────────────────────  C++ Wrapper  ────────────────────────────\ntorch::Tensor kb_92_cumsum_exclusive_wrapper(torch::Tensor arg0, int64_t arg1) {\n    /*\n        arg0 : 输入 Tensor，必须位于 CUDA 上、float32、contiguous\n        arg1 : 维度 dim，仅支持 0 或 1\n    */\n    CHECK_INPUT(arg0);\n    TORCH_CHECK(arg0.scalar_type() == at::kFloat, \"Only float32 is supported\");\n\n    const int64_t dim = arg1;\n    TORCH_CHECK(dim == 0 || dim == 1,\n                \"Only dim == 0 or dim == 1 is currently supported\");\n\n    const int rows_in = static_cast<int>(arg0.size(0));\n    const int cols_in = static_cast<int>(arg0.size(1));\n\n    torch::Tensor output;\n    if (dim == 1) {\n        TORCH_CHECK(rows_in >= 1, \"rows_in must be >= 1 for dim == 1\");\n        output = torch::empty({rows_in - 1, cols_in + 1}, arg0.options());\n    } else { // dim == 0\n        output = torch::empty({rows_in, cols_in}, arg0.options());\n    }\n\n    const int threads = 256;\n    dim3 blocks;\n\n    auto stream = at::cuda::getCurrentCUDAStream();\n\n    const float* input_ptr = arg0.data_ptr<float>();\n    float* output_ptr      = output.data_ptr<float>();\n\n    if (dim == 1) {\n        blocks = dim3((rows_in - 1 + threads - 1) / threads);\n        exclusive_cumsum_dim1_kernel<<<blocks, threads, 0, stream>>>(\n            input_ptr, output_ptr, rows_in, cols_in);\n    } else { // dim == 0\n        blocks = dim3((cols_in + threads - 1) / threads);\n        exclusive_cumsum_dim0_kernel<<<blocks, threads, 0, stream>>>(\n            input_ptr, output_ptr, rows_in, cols_in);\n    }\n\n    // 同步错误检查（可选）\n#ifndef NDEBUG\n    cudaError_t err = cudaGetLastError();\n    TORCH_CHECK(err == cudaSuccess, \"CUDA kernel failed : \", cudaGetErrorString(err));\n#endif\n\n    return output;\n}\n```\n",
    "cpp_code_extracted": true,
    "cuda_code_extracted": true,
    "error": ""
  }
]