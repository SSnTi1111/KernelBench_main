[
  {
    "attempt": 0,
    "type": "generation",
    "response": "--- C++ 签名 (`cpp`) ---\n```cpp\n#include <torch/extension.h>\n\n// C++ Wrapper 函数声明 (签名)\ntorch::Tensor kb_1_39_L2Norm__wrapper(torch::Tensor arg0);\n```\n\n--- C++/CUDA 组合实现 (`cu`) ---\n\n```cu\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <cmath>\n#include <vector>\n// PyTorch 2.1+ 移除了 c10::cuda::getCurrentCUDAStream\n// 使用 at::cuda::getCurrentCUDAStream() 代替\n#include <ATen/cuda/CUDAContext.h>\n\n// CUDA 辅助函数: Warp 内归约\n__device__ __forceinline__ float warpReduceSum(float val) {\n    // 使用全掩码进行 shuffle 归约\n    for (int offset = warpSize / 2; offset > 0; offset /= 2) {\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    }\n    return val;\n}\n\n// CUDA 辅助函数: Block 内归约 (对所有线程返回相同结果)\n__device__ float blockReduceSum(float val, float* shared) {\n    int lane = threadIdx.x & (warpSize - 1);   // 线程在 warp 中的索引\n    int wid  = threadIdx.x / warpSize;         // 当前线程所属 warp 的编号\n\n    // 先做 Warp 内归约\n    val = warpReduceSum(val);\n\n    // 每个 warp 的 lane 0 将部分和写入共享内存\n    if (lane == 0) {\n        shared[wid] = val;\n    }\n    __syncthreads();\n\n    // 由 warp 0 进行最终归约\n    float total = 0.0f;\n    if (wid == 0) {\n        int numWarps = (blockDim.x + warpSize - 1) / warpSize;\n        float warp_sum = (lane < numWarps) ? shared[lane] : 0.0f;\n        total = warpReduceSum(warp_sum);\n        if (lane == 0) {\n            shared[0] = total;  // 广播到共享内存\n        }\n    }\n    __syncthreads();\n    return shared[0];\n}\n\n// CUDA 内核实现: 对二维张量按 dim=1 (列维度) 做 L2 归一化\n__global__ void l2norm_dim1_kernel(\n    const float* __restrict__ x,\n    float* __restrict__ y,\n    int rows,\n    int cols\n) {\n    int row = blockIdx.x;\n    if (row >= rows) return;\n\n    extern __shared__ float shmem[]; // 大小为 numWarps 个 float\n\n    // 计算当前行的平方和\n    float sumsq = 0.0f;\n    const int base = row * cols;\n    for (int col = threadIdx.x; col < cols; col += blockDim.x) {\n        float v = x[base + col];\n        sumsq += v * v;\n    }\n\n    // 归约得到整行的平方和\n    float total = blockReduceSum(sumsq, shmem);\n\n    // 计算 1 / sqrt(total)\n    // 注意：当 total == 0 时，rsqrtf(0) = inf，此时 0 * inf = NaN，符合 x / 0 的浮点行为\n    float inv_norm = rsqrtf(total);\n\n    // 写出归一化后的结果\n    for (int col = threadIdx.x; col < cols; col += blockDim.x) {\n        float v = x[base + col];\n        y[base + col] = v * inv_norm;\n    }\n}\n\n// C++ Wrapper 实现\ntorch::Tensor kb_1_39_L2Norm__wrapper(torch::Tensor arg0) {\n    TORCH_CHECK(arg0.is_cuda(), \"kb_1_39_L2Norm__wrapper: input must be a CUDA tensor\");\n    TORCH_CHECK(arg0.scalar_type() == at::kFloat, \"kb_1_39_L2Norm__wrapper: only float32 is supported\");\n    TORCH_CHECK(arg0.dim() == 2, \"kb_1_39_L2Norm__wrapper: expected a 2D tensor [rows, cols]\");\n\n    auto x = arg0.contiguous();\n    const int rows = static_cast<int>(x.size(0));\n    const int cols = static_cast<int>(x.size(1));\n\n    auto y = torch::empty_like(x);\n\n    const int threads = 256; // 每个 block 的线程数\n    const dim3 block(threads);\n    const dim3 grid(rows);\n\n    // 共享内存需要为每个 warp 分配一个 float\n    const int numWarps = (threads + 31) / 32;\n    const size_t shmem_bytes = static_cast<size_t>(numWarps) * sizeof(float);\n\n    const float* x_ptr = x.data_ptr<float>();\n    float* y_ptr = y.data_ptr<float>();\n\n    auto stream = at::cuda::getCurrentCUDAStream();\n\n    l2norm_dim1_kernel<<<grid, block, shmem_bytes, stream>>>(\n        x_ptr, y_ptr, rows, cols\n    );\n\n    cudaError_t err = cudaGetLastError();\n    TORCH_CHECK(err == cudaSuccess, \"kb_1_39_L2Norm__wrapper: CUDA kernel launch failed with error: \", cudaGetErrorString(err));\n\n    return y;\n}\n```",
    "cpp_code_extracted": true,
    "cuda_code_extracted": true,
    "error": ""
  }
]