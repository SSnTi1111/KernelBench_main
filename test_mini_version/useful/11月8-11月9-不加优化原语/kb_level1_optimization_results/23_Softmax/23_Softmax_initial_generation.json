[
  {
    "attempt": 0,
    "type": "generation",
    "response": "--- C++ 签名 (`cpp`) ---\n```cpp\n#include <torch/extension.h>\n#include <vector> // 如果返回多个张量\n\n// C++ Wrapper 函数声明 (签名)\ntorch::Tensor kb_23_Softmax_wrapper(torch::Tensor arg0);\n```\n\n--- C++/CUDA 组合实现 (`cu`) ---\n```cu\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <cmath>\n#include <cfloat>\n#include <vector>\n// PyTorch 2.1+ 移除了 c10::cuda::getCurrentCUDAStream\n// 使用 at::cuda::getCurrentCUDAStream() 代替\n#include <ATen/cuda/CUDAContext.h>\n\n// 辅助：warp 级别归约\n__inline__ __device__ float warpReduceSum(float val) {\n    unsigned mask = 0xFFFFFFFFu;\n    for (int offset = warpSize / 2; offset > 0; offset >>= 1) {\n        val += __shfl_down_sync(mask, val, offset);\n    }\n    return val;\n}\n\n__inline__ __device__ float warpReduceMax(float val) {\n    unsigned mask = 0xFFFFFFFFu;\n    for (int offset = warpSize / 2; offset > 0; offset >>= 1) {\n        float other = __shfl_down_sync(mask, val, offset);\n        val = fmaxf(val, other);\n    }\n    return val;\n}\n\n// 块级别归约（和）\n__device__ float blockReduceSum(float val, float* shared) {\n    int lane = threadIdx.x & (warpSize - 1);\n    int wid  = threadIdx.x / warpSize;\n    int nWarps = (blockDim.x + warpSize - 1) / warpSize;\n\n    val = warpReduceSum(val);\n    if (lane == 0) shared[wid] = val;\n    __syncthreads();\n\n    float res = 0.0f;\n    if (wid == 0) {\n        float v = (lane < nWarps) ? shared[lane] : 0.0f;\n        float sum = warpReduceSum(v);\n        if (lane == 0) shared[0] = sum;\n    }\n    __syncthreads();\n    res = shared[0];\n    return res;\n}\n\n// 块级别归约（最大）\n__device__ float blockReduceMax(float val, float* shared) {\n    int lane = threadIdx.x & (warpSize - 1);\n    int wid  = threadIdx.x / warpSize;\n    int nWarps = (blockDim.x + warpSize - 1) / warpSize;\n\n    val = warpReduceMax(val);\n    if (lane == 0) shared[wid] = val;\n    __syncthreads();\n\n    float res = -FLT_MAX;\n    if (wid == 0) {\n        float v = (lane < nWarps) ? shared[lane] : -FLT_MAX;\n        float mx = warpReduceMax(v);\n        if (lane == 0) shared[0] = mx;\n    }\n    __syncthreads();\n    res = shared[0];\n    return res;\n}\n\n// CUDA 内核：按行计算 softmax (dim=1)\n__global__ void softmax_rowwise_kernel(const float* __restrict__ x,\n                                       float* __restrict__ y,\n                                       int rows, int cols) {\n    int row = blockIdx.x;\n    if (row >= rows) return;\n\n    const float* row_in = x + static_cast<size_t>(row) * cols;\n    float* row_out = y + static_cast<size_t>(row) * cols;\n\n    extern __shared__ float sdata[]; // 至少需要 nWarps 个 float\n\n    // 1) 求每行最大值\n    float local_max = -FLT_MAX;\n    for (int c = threadIdx.x; c < cols; c += blockDim.x) {\n        local_max = fmaxf(local_max, row_in[c]);\n    }\n    float row_max = blockReduceMax(local_max, sdata);\n\n    // 2) 计算 e^(x - max) 并累加求和\n    float local_sum = 0.0f;\n    for (int c = threadIdx.x; c < cols; c += blockDim.x) {\n        float v = __expf(row_in[c] - row_max);\n        row_out[c] = v;\n        local_sum += v;\n    }\n    float row_sum = blockReduceSum(local_sum, sdata);\n\n    // 3) 归一化\n    for (int c = threadIdx.x; c < cols; c += blockDim.x) {\n        row_out[c] = row_out[c] / row_sum;\n    }\n}\n\n// C++ Wrapper 实现\ntorch::Tensor kb_23_Softmax_wrapper(torch::Tensor arg0) {\n    TORCH_CHECK(arg0.is_cuda(), \"kb_23_Softmax_wrapper: input must be a CUDA tensor\");\n    TORCH_CHECK(arg0.dtype() == torch::kFloat32, \"kb_23_Softmax_wrapper: only float32 is supported\");\n    TORCH_CHECK(arg0.dim() == 2, \"kb_23_Softmax_wrapper: input must be 2D (batch_size, num_features)\");\n\n    // 保证连续内存\n    auto x = arg0.contiguous();\n\n    int rows = static_cast<int>(x.size(0));\n    int cols = static_cast<int>(x.size(1));\n\n    auto y = torch::empty_like(x);\n\n    // 配置 kernel\n    int threads = 256; // 多数 GPU 的良好默认值，需为 32 的倍数\n    dim3 block(threads);\n    dim3 grid(rows);\n    int nWarps = (threads + 31) / 32;\n    size_t shm_bytes = nWarps * sizeof(float); // 共享内存用于块归约\n\n    // 启动 kernel\n    const float* x_ptr = x.data_ptr<float>();\n    float* y_ptr = y.data_ptr<float>();\n    auto stream = at::cuda::getCurrentCUDAStream();\n\n    softmax_rowwise_kernel<<<grid, block, shm_bytes, stream>>>(\n        x_ptr, y_ptr, rows, cols\n    );\n\n    // 可选：错误检查（在扩展中通常由调用方/同步处理）\n    // cudaError_t err = cudaGetLastError();\n    // TORCH_CHECK(err == cudaSuccess, \"CUDA kernel failed with error: \", cudaGetErrorString(err));\n\n    return y;\n}\n```",
    "cpp_code_extracted": true,
    "cuda_code_extracted": true,
    "error": ""
  }
]