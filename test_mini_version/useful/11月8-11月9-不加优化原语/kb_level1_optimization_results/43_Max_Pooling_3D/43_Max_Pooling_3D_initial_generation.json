[
  {
    "attempt": 0,
    "type": "generation",
    "response": "```cpp\n#include <torch/extension.h>\n\n// C++ Wrapper 函数声明 (签名)\ntorch::Tensor kb_43_Max_Pooling_3D_wrapper(torch::Tensor arg0, int64_t arg1, int64_t arg2, int64_t arg3, int64_t arg4);\n```\n\n```cu\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <cmath>\n#include <cfloat>\n#include <vector>\n#include <ATen/cuda/CUDAContext.h>\n\n// CUDA 辅助函数示例（未在本内核中使用，但按要求提供）\n__device__ float blockReduceSum(float val, float* shared) {\n    int lane = threadIdx.x % warpSize;\n    int wid = threadIdx.x / warpSize;\n\n    // Warp内归约\n    for (int offset = warpSize / 2; offset > 0; offset /= 2) {\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    }\n\n    // 每个warp的第一个线程将结果写入共享内存\n    if (lane == 0) {\n        shared[wid] = val;\n    }\n    __syncthreads();\n\n    // 第一个warp进行最终归约\n    val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0.0f;\n    if (wid == 0) {\n        for (int offset = warpSize / 2; offset > 0; offset /= 2) {\n            val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n        }\n    }\n    return val;\n}\n\n// 3D MaxPooling CUDA 内核实现 (NCDHW)\n__global__ void MaxPool3dKernel(\n    const float* __restrict__ input,\n    float* __restrict__ output,\n    int64_t N, int64_t C,\n    int64_t D, int64_t H, int64_t W,\n    int64_t kernel_size,\n    int64_t stride,\n    int64_t padding,\n    int64_t dilation,\n    int64_t OD, int64_t OH, int64_t OW\n) {\n    int64_t total = N * C * OD * OH * OW;\n    int64_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    int64_t stride_grid = (int64_t)blockDim.x * gridDim.x;\n\n    for (int64_t linear = tid; linear < total; linear += stride_grid) {\n        // 反解线性索引为 (n, c, od, oh, ow)\n        int64_t tmp = linear;\n        int64_t ow = tmp % OW; tmp /= OW;\n        int64_t oh = tmp % OH; tmp /= OH;\n        int64_t od = tmp % OD; tmp /= OD;\n        int64_t c  = tmp % C;  tmp /= C;\n        int64_t n  = tmp;\n\n        float maxVal = -FLT_MAX;\n\n        // 计算输入起始位置（带填充）\n        int64_t dstart = od * stride - padding;\n        int64_t hstart = oh * stride - padding;\n        int64_t wstart = ow * stride - padding;\n\n        // 遍历带膨胀的核\n        for (int64_t kd = 0; kd < kernel_size; ++kd) {\n            int64_t id = dstart + kd * dilation;\n            if (id < 0 || id >= D) continue;\n\n            for (int64_t kh = 0; kh < kernel_size; ++kh) {\n                int64_t ih = hstart + kh * dilation;\n                if (ih < 0 || ih >= H) continue;\n\n                for (int64_t kw = 0; kw < kernel_size; ++kw) {\n                    int64_t iw = wstart + kw * dilation;\n                    if (iw < 0 || iw >= W) continue;\n\n                    int64_t in_index = (((n * C + c) * D + id) * H + ih) * W + iw;\n                    float val = input[in_index];\n                    if (val > maxVal) {\n                        maxVal = val;\n                    }\n                }\n            }\n        }\n\n        output[linear] = maxVal;\n    }\n}\n\n// C++ Wrapper 实现\ntorch::Tensor kb_43_Max_Pooling_3D_wrapper(torch::Tensor arg0, int64_t arg1, int64_t arg2, int64_t arg3, int64_t arg4) {\n    // 验证输入\n    TORCH_CHECK(arg0.is_cuda(), \"Input tensor must be on CUDA device\");\n    TORCH_CHECK(arg0.dtype() == torch::kFloat32, \"Input tensor must be float32\");\n    TORCH_CHECK(arg0.dim() == 5, \"Input must be a 5D tensor in NCDHW format\");\n    TORCH_CHECK(arg1 > 0, \"kernel_size must be > 0\");\n    TORCH_CHECK(arg2 > 0, \"stride must be > 0\");\n    TORCH_CHECK(arg3 >= 0, \"padding must be >= 0\");\n    TORCH_CHECK(arg4 > 0, \"dilation must be > 0\");\n\n    auto x = arg0.contiguous();\n\n    int64_t N = x.size(0);\n    int64_t C = x.size(1);\n    int64_t D = x.size(2);\n    int64_t H = x.size(3);\n    int64_t W = x.size(4);\n\n    int64_t kernel_size = arg1;\n    int64_t stride = arg2;\n    int64_t padding = arg3;\n    int64_t dilation = arg4;\n\n    // 计算输出尺寸 (ceil_mode=false)\n    int64_t k_eff = 1 + (kernel_size - 1) * dilation;\n\n    auto compute_out = [&](int64_t in_size) -> int64_t {\n        int64_t out = (in_size + 2 * padding - k_eff) / stride + 1;\n        return std::max<int64_t>(out, 0);\n    };\n\n    int64_t OD = compute_out(D);\n    int64_t OH = compute_out(H);\n    int64_t OW = compute_out(W);\n\n    TORCH_CHECK(OD > 0 && OH > 0 && OW > 0, \"Computed output dimensions must be > 0\");\n\n    // 分配输出张量\n    auto out = torch::empty({N, C, OD, OH, OW}, x.options());\n\n    // 计算网格/块维度\n    int threads = 256;\n    int64_t total = N * C * OD * OH * OW;\n    int64_t blocks64 = (total + threads - 1) / threads;\n    // 限制block数量以使用grid-stride loop\n    int blocks = static_cast<int>(std::min<int64_t>(blocks64, 65535));\n\n    // 调用内核\n    auto stream = at::cuda::getCurrentCUDAStream();\n    MaxPool3dKernel<<<blocks, threads, 0, stream.stream()>>>(\n        x.data_ptr<float>(),\n        out.data_ptr<float>(),\n        N, C, D, H, W,\n        kernel_size, stride, padding, dilation,\n        OD, OH, OW\n    );\n\n    // 错误检查\n    cudaError_t err = cudaGetLastError();\n    TORCH_CHECK(err == cudaSuccess, \"MaxPool3dKernel launch failed: \", cudaGetErrorString(err));\n\n    return out;\n}\n```",
    "cpp_code_extracted": true,
    "cuda_code_extracted": true,
    "error": ""
  }
]