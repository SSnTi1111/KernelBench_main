[
  {
    "attempt": 0,
    "type": "generation",
    "response": "--- C++ 签名 (`cpp`) ---\n```cpp\n#include <torch/extension.h>\n\n// C++ Wrapper 函数声明 (签名)\ntorch::Tensor kb_1_32_HardTanh_wrapper(torch::Tensor arg0);\n```\n\n--- C++/CUDA 组合实现 (`cu`) ---\n```cu\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <cmath>\n#include <vector>\n#include <ATen/cuda/CUDAContext.h>\n#include <c10/cuda/CUDAGuard.h>\n\n// [重要] 在此放置所有 CUDA 辅助函数 (例如 blockReduceSum)\n// (确保它们在使用它们的 kernel 之前被定义)\n__device__ float blockReduceSum(float val, float* shared) {\n    int lane = threadIdx.x % warpSize;\n    int wid = threadIdx.x / warpSize;\n\n    // Warp 内归约\n    for (int offset = warpSize / 2; offset > 0; offset >>= 1) {\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    }\n\n    // 每个 warp 的 lane 0 写入共享内存\n    if (lane == 0) {\n        shared[wid] = val;\n    }\n    __syncthreads();\n\n    // 使用第一个 warp 完成块内归约\n    val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0.0f;\n    if (wid == 0) {\n        for (int offset = warpSize / 2; offset > 0; offset >>= 1) {\n            val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n        }\n    }\n    return val;\n}\n\n__device__ __forceinline__ float hardtanh_op(float x, float minv, float maxv) {\n    // 使用快速设备函数实现 clamp\n    x = fminf(x, maxv);\n    x = fmaxf(x, minv);\n    return x;\n}\n\n// CUDA 内核实现\n__global__ void hardtanh_kernel_f32(const float* __restrict__ in,\n                                    float* __restrict__ out,\n                                    size_t N,\n                                    float minv,\n                                    float maxv) {\n    size_t idx = blockIdx.x * (size_t)blockDim.x + threadIdx.x;\n    size_t stride = (size_t)blockDim.x * gridDim.x;\n\n    for (size_t i = idx; i < N; i += stride) {\n        float v = in[i];\n        out[i] = hardtanh_op(v, minv, maxv);\n    }\n}\n\n// C++ Wrapper 实现\ntorch::Tensor kb_1_32_HardTanh_wrapper(torch::Tensor arg0) {\n    TORCH_CHECK(arg0.device().is_cuda(), \"kb_1_32_HardTanh_wrapper: input must be a CUDA tensor\");\n    TORCH_CHECK(arg0.scalar_type() == at::kFloat, \"kb_1_32_HardTanh_wrapper: only float32 tensors are supported\");\n\n    // 设备保护，确保在输入张量所在设备上分配与执行\n    c10::cuda::CUDAGuard device_guard(arg0.device());\n\n    // 保证内存连续\n    auto x = arg0.contiguous();\n\n    // 分配输出\n    auto out = torch::empty_like(x);\n\n    const float* in_ptr = x.data_ptr<float>();\n    float* out_ptr = out.data_ptr<float>();\n\n    const size_t N = static_cast<size_t>(x.numel());\n    if (N == 0) {\n        return out;\n    }\n\n    // 配置 kernel 启动参数\n    int threads = 256;\n    // 使用网格-步长循环，限制 grid 大小以兼容性最佳\n    int64_t blocks64 = (static_cast<int64_t>(N) + threads - 1) / threads;\n    if (blocks64 <= 0) blocks64 = 1;\n    int max_blocks = 65535; // 兼容性好的上限\n    int blocks = static_cast<int>(std::min<int64_t>(blocks64, max_blocks));\n\n    // 获取当前 CUDA 流\n    cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n\n    const float minv = -1.0f;\n    const float maxv = 1.0f;\n\n    // 启动 kernel\n    hardtanh_kernel_f32<<<blocks, threads, 0, stream>>>(in_ptr, out_ptr, N, minv, maxv);\n\n    // 检查 kernel 启动错误\n    cudaError_t err = cudaGetLastError();\n    TORCH_CHECK(err == cudaSuccess, \"kb_1_32_HardTanh_wrapper: CUDA kernel launch failed with error: \",\n                cudaGetErrorString(err));\n\n    return out;\n}\n```",
    "cpp_code_extracted": true,
    "cuda_code_extracted": true,
    "error": ""
  }
]