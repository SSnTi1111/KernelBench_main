[
  {
    "attempt": 0,
    "type": "generation",
    "response": "--- C++ 签名 (`cpp`) ---\n```cpp\n#include <torch/extension.h>\n\n// C++ Wrapper 函数声明 (签名)\ntorch::Tensor kb_1_52_Argmin_over_a_dimension_wrapper(torch::Tensor arg0, int64_t arg1);\n```\n\n--- C++/CUDA 组合实现 (`cu`) ---\n```cu\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <cmath>\n#include <vector>\n#include <cstdint>\n\n// PyTorch ≥2.1 需使用 at::cuda::getCurrentCUDAStream()\n#include <ATen/cuda/CUDAContext.h>\n\n/*\n * ---------------------------------------------------------------------------\n * （可选）CUDA 辅助函数示例（此示例未在当前 kernel 中使用，保留作模板）\n * ---------------------------------------------------------------------------\n */\n__device__ float blockReduceSum(float val, float* shared)\n{\n    const unsigned int lane = threadIdx.x & 0x1f;      // 0–31\n    const unsigned int wid  = threadIdx.x >> 5;        // warp ID\n\n    // ── warp 内求和 ──\n    for (int offset = warpSize >> 1; offset > 0; offset >>= 1)\n        val += __shfl_down_sync(0xffffffff, val, offset);\n\n    // 每个 warp 的第一个线程把结果写到 shared memory\n    if (lane == 0) shared[wid] = val;\n    __syncthreads();\n\n    // ── 第一个 warp 继续对所有 warp 的部分和做归约 ──\n    val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0.f;\n    if (wid == 0) {\n        for (int offset = warpSize >> 1; offset > 0; offset >>= 1)\n            val += __shfl_down_sync(0xffffffff, val, offset);\n    }\n    return val;\n}\n\n/*\n * ---------------------------------------------------------------------------\n * Argmin kernel : 对输入张量在指定维度执行 argmin\n * 仅支持连续 (contiguous) Tensor。\n * 维度拆分:\n *    input shape  : [outer_size, reduce_size, inner_size]\n *    output shape : [outer_size, inner_size]\n * 每个线程处理一个 (outer_idx, inner_idx) 对，应在 reduce_size 上顺序扫描。\n * ---------------------------------------------------------------------------\n */\ntemplate <typename scalar_t>\n__global__ void argmin_dim_kernel(const scalar_t* __restrict__ input,\n                                  int64_t* __restrict__ output,\n                                  int64_t outer_size,\n                                  int64_t reduce_size,\n                                  int64_t inner_size,\n                                  int64_t total_threads)\n{\n    const int64_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid >= total_threads) return;\n\n    // 反算坐标\n    const int64_t outer_idx = tid / inner_size;\n    const int64_t inner_idx = tid % inner_size;\n\n    // 该 (outer_idx, inner_idx) 对应到 input 中的起始位置\n    int64_t base_offset = (outer_idx * reduce_size * inner_size) + inner_idx;\n\n    scalar_t min_val  = input[base_offset]; // r == 0\n    int64_t  min_idx  = 0;\n\n    // 顺序扫描 reduce 维\n    for (int64_t r = 1; r < reduce_size; ++r) {\n        scalar_t val = input[base_offset + r * inner_size];\n        if (val < min_val) {\n            min_val = val;\n            min_idx = r;\n        }\n    }\n\n    output[tid] = min_idx; // 写入 argmin 结果\n}\n\n/*\n * ---------------------------------------------------------------------------\n * C++ 包装函数\n * ---------------------------------------------------------------------------\n */\ntorch::Tensor kb_1_52_Argmin_over_a_dimension_wrapper(torch::Tensor arg0,\n                                                      int64_t      dim_in)\n{\n    TORCH_CHECK(arg0.is_cuda(), \"Input tensor must be on CUDA\");\n    TORCH_CHECK(arg0.is_contiguous(),\n                \"Only contiguous tensors are supported for this op\");\n\n    // 处理负 dim\n    int64_t dim = dim_in;\n    if (dim < 0) dim += arg0.dim();\n    TORCH_CHECK(dim >= 0 && dim < arg0.dim(),\n                \"Reduction dim is out of bounds\");\n\n    // 计算 outer / reduce / inner size\n    const auto sizes = arg0.sizes();\n    int64_t outer_size  = 1;\n    int64_t inner_size  = 1;\n    int64_t reduce_size = sizes[dim];\n\n    for (int64_t i = 0; i < dim; ++i) outer_size *= sizes[i];\n    for (int64_t i = dim + 1; i < arg0.dim(); ++i) inner_size *= sizes[i];\n\n    // 输出尺寸 = 输入尺寸移除 reduce 维\n    std::vector<int64_t> out_sizes;\n    out_sizes.reserve(arg0.dim() - 1);\n    for (int64_t i = 0; i < arg0.dim(); ++i)\n        if (i != dim) out_sizes.push_back(sizes[i]);\n\n    torch::Tensor output = torch::empty(\n        out_sizes,\n        torch::TensorOptions()\n            .dtype(torch::kInt64)\n            .device(arg0.device()));\n\n    // 调度 kernel\n    const int64_t total_threads = outer_size * inner_size;\n    const int threads = 256;\n    const int blocks  = (total_threads + threads - 1) / threads;\n\n    AT_DISPATCH_FLOATING_TYPES(arg0.scalar_type(), \"argmin_dim_kernel\", ([&] {\n        argmin_dim_kernel<scalar_t>\n            <<<blocks, threads, 0, at::cuda::getCurrentCUDAStream()>>>(\n                arg0.data_ptr<scalar_t>(),\n                output.data_ptr<int64_t>(),\n                outer_size,\n                reduce_size,\n                inner_size,\n                total_threads);\n    }));\n\n    // CUDA 内核错误检查（可选）\n    cudaError_t err = cudaGetLastError();\n    TORCH_CHECK(err == cudaSuccess, \"CUDA kernel failed : \",\n                cudaGetErrorString(err));\n\n    return output;\n}\n```\n",
    "cpp_code_extracted": true,
    "cuda_code_extracted": true,
    "error": ""
  }
]