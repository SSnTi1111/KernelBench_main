[
  {
    "attempt": 0,
    "type": "generation",
    "response": "```cpp\n#include <torch/extension.h>\n#include <vector> // 如果返回多个张量\n\n// C++ Wrapper 函数声明 (签名)\ntorch::Tensor kb_1_42_Max_Pooling_2D_wrapper(torch::Tensor arg0, int64_t arg1, int64_t arg2, int64_t arg3, int64_t arg4);\n```\n\n```cu\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <cmath>\n#include <cfloat>\n#include <vector>\n// PyTorch 2.1+ 移除了 c10::cuda::getCurrentCUDAStream\n// 使用 at::cuda::getCurrentCUDAStream() 代替\n#include <ATen/cuda/CUDAContext.h>\n\n// [重要] 在此放置所有 CUDA 辅助函数 (例如 blockReduceSum)\n// (确保它们在使用它们的 kernel 之前被定义)\n__device__ float blockReduceSum(float val, float* shared) {\n    int lane = threadIdx.x % warpSize;\n    int wid = threadIdx.x / warpSize;\n\n    // Warp内归约\n    for (int offset = warpSize / 2; offset > 0; offset /= 2) {\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    }\n\n    // 每个warp的第一个线程将结果写入共享内存\n    if (lane == 0) {\n        shared[wid] = val;\n    }\n    __syncthreads();\n\n    // 第一个warp进行最终归约\n    val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0.0f;\n    if (wid == 0) {\n        for (int offset = warpSize / 2; offset > 0; offset /= 2) {\n            val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n        }\n    }\n    return val;\n}\n\n// CUDA 内核实现: NCHW 布局的一般化 MaxPool2D\n__global__ void maxpool2d_nchw_kernel(\n    const float* __restrict__ input,\n    float* __restrict__ output,\n    int N, int C, int H, int W,\n    int outH, int outW,\n    int kernel, int stride, int padding, int dilation\n) {\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    int total = N * C * outH * outW;\n    if (tid >= total) return;\n\n    int ow = tid % outW;\n    int tmp = tid / outW;\n    int oh = tmp % outH;\n    tmp /= outH;\n    int c = tmp % C;\n    int n = tmp / C;\n\n    int start_h = oh * stride - padding;\n    int start_w = ow * stride - padding;\n\n    float maxval = -FLT_MAX;\n\n    for (int kh = 0; kh < kernel; ++kh) {\n        int ih = start_h + kh * dilation;\n        if (ih < 0 || ih >= H) continue;\n        for (int kw = 0; kw < kernel; ++kw) {\n            int iw = start_w + kw * dilation;\n            if (iw < 0 || iw >= W) continue;\n            int in_idx = ((n * C + c) * H + ih) * W + iw;\n            float v = input[in_idx];\n            if (v > maxval) maxval = v;\n        }\n    }\n\n    int out_idx = ((n * C + c) * outH + oh) * outW + ow;\n    output[out_idx] = maxval;\n}\n\n// C++ Wrapper 实现\ntorch::Tensor kb_1_42_Max_Pooling_2D_wrapper(torch::Tensor arg0, int64_t arg1, int64_t arg2, int64_t arg3, int64_t arg4) {\n    TORCH_CHECK(arg0.is_cuda(), \"arg0 must be a CUDA tensor\");\n    TORCH_CHECK(arg0.dtype() == torch::kFloat32, \"arg0 must be float32\");\n    TORCH_CHECK(arg0.dim() == 4, \"arg0 must be 4D (N, C, H, W)\");\n\n    // 参数: kernel_size, stride, padding, dilation\n    int64_t kernel = arg1;\n    int64_t stride = arg2;\n    int64_t padding = arg3;\n    int64_t dilation = arg4;\n\n    TORCH_CHECK(kernel > 0, \"kernel_size must be > 0\");\n    TORCH_CHECK(stride > 0, \"stride must be > 0\");\n    TORCH_CHECK(dilation > 0, \"dilation must be > 0\");\n    TORCH_CHECK(padding >= 0, \"padding must be >= 0\");\n\n    auto x = arg0.contiguous();\n\n    int64_t N64 = x.size(0);\n    int64_t C64 = x.size(1);\n    int64_t H64 = x.size(2);\n    int64_t W64 = x.size(3);\n\n    int64_t effective_kernel = dilation * (kernel - 1) + 1;\n    int64_t outH64 = (H64 + 2 * padding - effective_kernel) / stride + 1;\n    int64_t outW64 = (W64 + 2 * padding - effective_kernel) / stride + 1;\n    if (outH64 < 0) outH64 = 0;\n    if (outW64 < 0) outW64 = 0;\n\n    torch::Tensor y = torch::empty({N64, C64, outH64, outW64}, x.options());\n\n    int total = static_cast<int>(N64 * C64 * outH64 * outW64);\n    if (total == 0) {\n        return y;\n    }\n\n    const float* x_ptr = x.data_ptr<float>();\n    float* y_ptr = y.data_ptr<float>();\n\n    int N = static_cast<int>(N64);\n    int C = static_cast<int>(C64);\n    int H = static_cast<int>(H64);\n    int W = static_cast<int>(W64);\n    int outH = static_cast<int>(outH64);\n    int outW = static_cast<int>(outW64);\n    int ksz = static_cast<int>(kernel);\n    int str = static_cast<int>(stride);\n    int pad = static_cast<int>(padding);\n    int dil = static_cast<int>(dilation);\n\n    int threads = 256;\n    int blocks = (total + threads - 1) / threads;\n\n    cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n    maxpool2d_nchw_kernel<<<blocks, threads, 0, stream>>>(\n        x_ptr, y_ptr,\n        N, C, H, W,\n        outH, outW,\n        ksz, str, pad, dil\n    );\n    cudaError_t err = cudaGetLastError();\n    TORCH_CHECK(err == cudaSuccess, \"maxpool2d_nchw_kernel launch failed: \", cudaGetErrorString(err));\n\n    return y;\n}\n```",
    "cpp_code_extracted": true,
    "cuda_code_extracted": true,
    "error": ""
  }
]