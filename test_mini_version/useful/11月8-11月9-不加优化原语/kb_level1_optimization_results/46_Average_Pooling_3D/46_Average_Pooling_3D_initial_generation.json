[
  {
    "attempt": 0,
    "type": "generation",
    "response": "```cpp\n#include <torch/extension.h>\n#include <vector> // 如果返回多个张量\n\n// C++ Wrapper 函数声明 (签名)\ntorch::Tensor kb_46_Average_Pooling_3D_wrapper(torch::Tensor arg0, int64_t arg1, int64_t arg2, int64_t arg3);\n```\n\n```cu\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <cmath>\n#include <vector>\n// PyTorch 2.1+ 使用 at::cuda::getCurrentCUDAStream()\n#include <ATen/cuda/CUDAContext.h>\n\n// [可选] 归约辅助函数示例（本内核未使用，但示例性提供）\n__device__ float blockReduceSum(float val, float* shared) {\n    int lane = threadIdx.x % warpSize;\n    int wid = threadIdx.x / warpSize;\n\n    // Warp 内归约\n    for (int offset = warpSize / 2; offset > 0; offset >>= 1) {\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    }\n\n    // 每个 warp 的 lane 0 写入共享内存\n    if (lane == 0) {\n        shared[wid] = val;\n    }\n    __syncthreads();\n\n    // 第一个 warp 完成剩余归约\n    val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0.0f;\n    if (wid == 0) {\n        for (int offset = warpSize / 2; offset > 0; offset >>= 1) {\n            val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n        }\n    }\n    return val;\n}\n\n// CUDA 内核实现: NCDHW -> NCDHW 平均池化 (count_include_pad = true)\n__global__ void avg_pool3d_kernel(\n    const float* __restrict__ in,\n    float* __restrict__ out,\n    int64_t N, int64_t C, int64_t D, int64_t H, int64_t W,\n    int64_t outD, int64_t outH, int64_t outW,\n    int k, int s, int p\n) {\n    int64_t total = N * C * outD * outH * outW;\n    int64_t idx = blockIdx.x * (int64_t)blockDim.x + threadIdx.x;\n    if (idx >= total) return;\n\n    // 反解线性索引 -> (n, c, od, oh, ow)\n    int64_t ow = idx % outW;\n    int64_t tmp = idx / outW;\n    int64_t oh = tmp % outH;\n    tmp /= outH;\n    int64_t od = tmp % outD;\n    tmp /= outD;\n    int64_t c = tmp % C;\n    int64_t n = tmp / C;\n\n    // 输入/输出基址\n    int64_t in_stride_C = D * H * W;\n    int64_t in_stride_N = C * in_stride_C;\n    int64_t in_base = n * in_stride_N + c * in_stride_C;\n\n    // 计算池化窗口起点（含 padding）\n    int64_t d_start = od * (int64_t)s - (int64_t)p;\n    int64_t h_start = oh * (int64_t)s - (int64_t)p;\n    int64_t w_start = ow * (int64_t)s - (int64_t)p;\n\n    float acc = 0.0f;\n\n    // 累加有效范围内的元素；分母按 k^3（count_include_pad = true）\n    for (int kd = 0; kd < k; ++kd) {\n        int64_t id = d_start + kd;\n        if (id < 0 || id >= D) continue;\n        int64_t d_off = id * (int64_t)(H * W);\n        for (int kh = 0; kh < k; ++kh) {\n            int64_t ih = h_start + kh;\n            if (ih < 0 || ih >= H) continue;\n            int64_t h_off = ih * (int64_t)W;\n            for (int kw = 0; kw < k; ++kw) {\n                int64_t iw = w_start + kw;\n                if (iw < 0 || iw >= W) continue;\n                int64_t in_index = in_base + d_off + h_off + iw;\n                acc += in[in_index];\n            }\n        }\n    }\n\n    float denom = static_cast<float>(k) * static_cast<float>(k) * static_cast<float>(k);\n    float out_val = acc / denom;\n\n    int64_t out_stride_C = outD * outH * outW;\n    int64_t out_stride_N = C * out_stride_C;\n    int64_t out_index = n * out_stride_N + c * out_stride_C + od * (outH * outW) + oh * outW + ow;\n\n    out[out_index] = out_val;\n}\n\n// C++ Wrapper 实现\ntorch::Tensor kb_46_Average_Pooling_3D_wrapper(torch::Tensor arg0, int64_t arg1, int64_t arg2, int64_t arg3) {\n    // 验证输入\n    TORCH_CHECK(arg0.is_cuda(), \"Input tensor must be on CUDA device\");\n    TORCH_CHECK(arg0.dtype() == torch::kFloat32, \"Only float32 is supported\");\n    TORCH_CHECK(arg0.dim() == 5, \"Input must be 5D NCDHW\");\n    TORCH_CHECK(arg1 > 0 && arg2 > 0 && arg3 >= 0, \"kernel, stride must be > 0 and padding >= 0\");\n\n    auto input = arg0.contiguous();\n\n    int64_t N = input.size(0);\n    int64_t C = input.size(1);\n    int64_t D = input.size(2);\n    int64_t H = input.size(3);\n    int64_t W = input.size(4);\n\n    int64_t k = arg1;\n    int64_t s = arg2;\n    int64_t p = arg3;\n\n    // 计算输出尺寸 (floor)\n    TORCH_CHECK((D + 2 * p - k) >= 0 && (H + 2 * p - k) >= 0 && (W + 2 * p - k) >= 0,\n                \"Invalid kernel/padding relative to input size\");\n    int64_t outD = (D + 2 * p - k) / s + 1;\n    int64_t outH = (H + 2 * p - k) / s + 1;\n    int64_t outW = (W + 2 * p - k) / s + 1;\n\n    auto options = input.options();\n    torch::Tensor output = torch::empty({N, C, outD, outH, outW}, options);\n\n    int64_t total = N * C * outD * outH * outW;\n    if (total == 0) {\n        return output;\n    }\n\n    const int threads = 256;\n    const int blocks = static_cast<int>((total + threads - 1) / threads);\n\n    const float* in_ptr = input.data_ptr<float>();\n    float* out_ptr = output.data_ptr<float>();\n\n    cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n\n    avg_pool3d_kernel<<<blocks, threads, 0, stream>>>(\n        in_ptr, out_ptr,\n        N, C, D, H, W,\n        outD, outH, outW,\n        static_cast<int>(k),\n        static_cast<int>(s),\n        static_cast<int>(p)\n    );\n\n    // 可选：进行错误检查\n    // cudaError_t err = cudaGetLastError();\n    // TORCH_CHECK(err == cudaSuccess, \"avg_pool3d_kernel launch failed: \", cudaGetErrorString(err));\n\n    return output;\n}\n```",
    "cpp_code_extracted": true,
    "cuda_code_extracted": true,
    "error": ""
  }
]