[
  {
    "attempt": 0,
    "type": "generation",
    "response": "```cpp\n#include <torch/extension.h>\n#include <vector> // 如果返回多个张量\n\n// C++ Wrapper 函数声明 (签名)\ntorch::Tensor kb_21_8_HardSigmoid_wrapper(torch::Tensor arg0);\n```\n\n```cu\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <cmath>\n#include <vector>\n// [!!! 关键 !!!] \n// PyTorch 2.1+ 移除了 c10::cuda::getCurrentCUDAStream\n// 使用 at::cuda::getCurrentCUDAStream() 代替\n#include <ATen/cuda/CUDAContext.h>\n\n// [重要] 在此放置所有 CUDA 辅助函数 (例如 blockReduceSum)\n// (确保它们在使用它们的 kernel 之前被定义)\n__device__ float blockReduceSum(float val, float* shared) {\n    // 示例 Warp 内归约\n    int lane = threadIdx.x % warpSize;\n    int wid = threadIdx.x / warpSize;\n  \n    // Warp内归约\n    for (int offset = warpSize / 2; offset > 0; offset /= 2) {\n        val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n    }\n  \n    // 每个warp的第一个线程将结果写入共享内存\n    if (lane == 0) {\n        shared[wid] = val;\n    }\n    __syncthreads();\n  \n    // 第一个warp进行最终归约\n    val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0.0f;\n    if (wid == 0) {\n        for (int offset = warpSize / 2; offset > 0; offset /= 2) {\n            val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n        }\n    }\n    return val;\n}\n\n// CUDA 内核实现: HardSigmoid = clamp((x + 3) / 6, 0, 1)\n__global__ void hardsigmoid_kernel(const float* __restrict__ in,\n                                   float* __restrict__ out,\n                                   int64_t N) {\n    int64_t idx = static_cast<int64_t>(blockIdx.x) * blockDim.x + threadIdx.x;\n    int64_t stride = static_cast<int64_t>(blockDim.x) * gridDim.x;\n    const float inv6 = 1.0f / 6.0f;\n\n    for (int64_t i = idx; i < N; i += stride) {\n        float v = in[i];\n        float y = (v + 3.0f) * inv6;\n        // clamp to [0, 1]\n        y = y < 0.0f ? 0.0f : (y > 1.0f ? 1.0f : y);\n        out[i] = y;\n    }\n}\n\n// C++ Wrapper 实现\ntorch::Tensor kb_21_8_HardSigmoid_wrapper(torch::Tensor arg0) {\n    TORCH_CHECK(arg0.is_cuda(), \"kb_21_8_HardSigmoid_wrapper: input must be a CUDA tensor\");\n    TORCH_CHECK(arg0.scalar_type() == at::kFloat, \"kb_21_8_HardSigmoid_wrapper: only float32 is supported\");\n\n    auto input = arg0.contiguous();\n    auto N = input.numel();\n\n    auto output = at::empty_like(input);\n\n    const int threads = 256;\n    // 选择合理的块数，避免过大，同时使用网格步长循环覆盖所有元素\n    int64_t blocks64 = (N + threads - 1) / threads;\n    // 限制 gridDim.x 以获得较好的占用率与兼容性\n    int blocks = static_cast<int>(std::min<int64_t>(blocks64, 65535));\n\n    // 获取当前 CUDA 流并启动内核\n    cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n\n    hardsigmoid_kernel<<<blocks, threads, 0, stream>>>(\n        input.data_ptr<float>(),\n        output.data_ptr<float>(),\n        N\n    );\n\n    cudaError_t err = cudaGetLastError();\n    TORCH_CHECK(err == cudaSuccess, \"kb_21_8_HardSigmoid_wrapper: kernel launch failed with error: \",\n                cudaGetErrorString(err));\n\n    return output;\n}\n```",
    "cpp_code_extracted": true,
    "cuda_code_extracted": true,
    "error": ""
  }
]