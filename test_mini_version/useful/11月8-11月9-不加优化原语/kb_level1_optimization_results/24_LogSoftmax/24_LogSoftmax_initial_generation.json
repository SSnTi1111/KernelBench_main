[
  {
    "attempt": 0,
    "type": "generation",
    "response": "```cpp\n#include <torch/extension.h>\n\n// C++ Wrapper 函数声明 (签名)\ntorch::Tensor kb_24_LogSoftmax_wrapper(torch::Tensor arg0);\n```\n\n```cu\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <cmath>\n#include <vector>\n#include <ATen/cuda/CUDAContext.h>\n\n// [重要] CUDA 辅助归约函数先于 kernel 定义\n\n// Warp 内归约 - 求和\n__device__ inline float warpReduceSum(float val) {\n    unsigned mask = 0xFFFFFFFFu;\n    for (int offset = warpSize / 2; offset > 0; offset >>= 1) {\n        val += __shfl_down_sync(mask, val, offset);\n    }\n    return val;\n}\n\n// Warp 内归约 - 最大值\n__device__ inline float warpReduceMax(float val) {\n    unsigned mask = 0xFFFFFFFFu;\n    for (int offset = warpSize / 2; offset > 0; offset >>= 1) {\n        val = fmaxf(val, __shfl_down_sync(mask, val, offset));\n    }\n    return val;\n}\n\n// Block 级归约 - 求和\n__device__ float blockReduceSum(float val, float* shm) {\n    int lane = threadIdx.x & (warpSize - 1);\n    int wid  = threadIdx.x / warpSize;\n    int numWarps = (blockDim.x + warpSize - 1) / warpSize;\n\n    // 每个 warp 内归约\n    val = warpReduceSum(val);\n\n    // 每个 warp 的 lane 0 写入共享内存\n    if (lane == 0) {\n        shm[wid] = val;\n    }\n    __syncthreads();\n\n    // 第一个 warp 进行最终归约\n    float blockVal = 0.0f;\n    if (wid == 0) {\n        blockVal = (lane < numWarps) ? shm[lane] : 0.0f;\n        blockVal = warpReduceSum(blockVal);\n        if (lane == 0) {\n            shm[0] = blockVal;\n        }\n    }\n    __syncthreads();\n    return shm[0];\n}\n\n// Block 级归约 - 最大值\n__device__ float blockReduceMax(float val, float* shm) {\n    int lane = threadIdx.x & (warpSize - 1);\n    int wid  = threadIdx.x / warpSize;\n    int numWarps = (blockDim.x + warpSize - 1) / warpSize;\n\n    // 每个 warp 内归约\n    val = warpReduceMax(val);\n\n    // 每个 warp 的 lane 0 写入共享内存\n    if (lane == 0) {\n        shm[wid] = val;\n    }\n    __syncthreads();\n\n    // 第一个 warp 进行最终归约\n    float blockVal = -INFINITY;\n    if (wid == 0) {\n        blockVal = (lane < numWarps) ? shm[lane] : -INFINITY;\n        blockVal = warpReduceMax(blockVal);\n        if (lane == 0) {\n            shm[0] = blockVal;\n        }\n    }\n    __syncthreads();\n    return shm[0];\n}\n\n// CUDA 内核实现：沿着 dim=1（列）进行 LogSoftmax\n__global__ void logsoftmax_rowwise_kernel(const float* __restrict__ x,\n                                          float* __restrict__ y,\n                                          int rows,\n                                          int cols) {\n    int row = blockIdx.x;\n    if (row >= rows) return;\n\n    const float* row_x = x + static_cast<size_t>(row) * cols;\n    float* row_y = y + static_cast<size_t>(row) * cols;\n\n    extern __shared__ float shm[]; // 大小至少为 warp 数\n\n    // 第一步：计算行最大值 (数值稳定)\n    float local_max = -INFINITY;\n    for (int c = threadIdx.x; c < cols; c += blockDim.x) {\n        local_max = fmaxf(local_max, row_x[c]);\n    }\n    float row_max = blockReduceMax(local_max, shm);\n\n    // 第二步：计算 sum(exp(x - max))\n    float local_sum = 0.0f;\n    for (int c = threadIdx.x; c < cols; c += blockDim.x) {\n        float val = row_x[c] - row_max;\n        local_sum += __expf(val);\n    }\n    float sum_exp = blockReduceSum(local_sum, shm);\n\n    // 计算 logsumexp\n    float lse = logf(sum_exp) + row_max;\n\n    // 第三步：写出 y = x - logsumexp\n    for (int c = threadIdx.x; c < cols; c += blockDim.x) {\n        row_y[c] = row_x[c] - lse;\n    }\n}\n\n// C++ Wrapper 实现\ntorch::Tensor kb_24_LogSoftmax_wrapper(torch::Tensor arg0) {\n    TORCH_CHECK(arg0.is_cuda(), \"Input tensor must be on CUDA device.\");\n    TORCH_CHECK(arg0.dtype() == torch::kFloat32, \"Input tensor must be float32.\");\n    TORCH_CHECK(arg0.dim() == 2, \"Input tensor must be 2D (batch_size, dim).\");\n    auto input = arg0.contiguous();\n\n    const int rows = static_cast<int>(input.size(0));\n    const int cols = static_cast<int>(input.size(1));\n\n    auto output = torch::empty_like(input);\n\n    const int threads = 256; // 保持为 warpSize 的倍数\n    const int blocks = rows;\n    const int warps = (threads + 31) / 32;\n    const size_t shmem_bytes = warps * sizeof(float);\n\n    auto stream = at::cuda::getCurrentCUDAStream();\n    logsoftmax_rowwise_kernel<<<blocks, threads, shmem_bytes, stream.stream()>>>(\n        input.data_ptr<float>(),\n        output.data_ptr<float>(),\n        rows,\n        cols\n    );\n\n    cudaError_t err = cudaGetLastError();\n    TORCH_CHECK(err == cudaSuccess, \"logsoftmax_rowwise_kernel launch failed: \", cudaGetErrorString(err));\n\n    return output;\n}\n```",
    "cpp_code_extracted": true,
    "cuda_code_extracted": true,
    "error": ""
  }
]