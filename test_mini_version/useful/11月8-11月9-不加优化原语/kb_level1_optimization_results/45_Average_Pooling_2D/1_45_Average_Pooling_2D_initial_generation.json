[
  {
    "attempt": 0,
    "type": "generation",
    "response": "```cpp\n#include <torch/extension.h>\n\n// C++ Wrapper 函数声明 (签名)\ntorch::Tensor kb_1_45_Average_Pooling_2D_wrapper(torch::Tensor arg0, int64_t arg1);\n```\n\n```cu\n#include <torch/extension.h>\n#include <cuda_runtime.h>\n#include <cuda.h>\n#include <cmath>\n#include <vector>\n#include <cstdint>\n#include <ATen/cuda/CUDAContext.h>\n\n// CUDA 内核实现: NCHW 格式的 2D 平均池化 (padding=0, stride=kernel_size)\n__global__ void avg_pool2d_nchw_kernel(\n    const float* __restrict__ input,\n    float* __restrict__ output,\n    int64_t N, int64_t C, int64_t H, int64_t W,\n    int64_t H_out, int64_t W_out,\n    int64_t K, int64_t S\n) {\n    int64_t index = static_cast<int64_t>(blockIdx.x) * blockDim.x + threadIdx.x;\n    int64_t total = N * C * H_out * W_out;\n    if (index >= total) return;\n\n    int64_t w_out = index % W_out;\n    int64_t tmp = index / W_out;\n    int64_t h_out = tmp % H_out;\n    tmp /= H_out;\n    int64_t c = tmp % C;\n    int64_t n = tmp / C;\n\n    int64_t h_start = h_out * S;\n    int64_t w_start = w_out * S;\n    int64_t h_end = h_start + K;\n    int64_t w_end = w_start + K;\n\n    int64_t base = ((n * C + c) * H) * W;\n\n    float sum = 0.0f;\n    for (int64_t h = h_start; h < h_end; ++h) {\n        int64_t row = base + h * W;\n        for (int64_t w = w_start; w < w_end; ++w) {\n            sum += input[row + w];\n        }\n    }\n    output[index] = sum / static_cast<float>(K * K);\n}\n\n// C++ Wrapper 实现\ntorch::Tensor kb_1_45_Average_Pooling_2D_wrapper(torch::Tensor arg0, int64_t arg1) {\n    TORCH_CHECK(arg0.is_cuda(), \"arg0 must be a CUDA tensor\");\n    TORCH_CHECK(arg0.scalar_type() == at::kFloat, \"arg0 must be float32\");\n    TORCH_CHECK(arg0.dim() == 4, \"arg0 must be 4D tensor in NCHW format\");\n\n    // kernel_size = arg1; stride = kernel_size; padding = 0\n    int64_t K = arg1;\n    TORCH_CHECK(K > 0, \"kernel_size (arg1) must be > 0\");\n    int64_t S = K;\n    int64_t P = 0;\n\n    auto x = arg0.contiguous();\n    int64_t N = x.size(0);\n    int64_t C = x.size(1);\n    int64_t H = x.size(2);\n    int64_t W = x.size(3);\n\n    TORCH_CHECK(H + 2 * P >= K && W + 2 * P >= K, \"kernel_size is larger than input spatial dimensions\");\n\n    int64_t H_out = (H + 2 * P - K) / S + 1;\n    int64_t W_out = (W + 2 * P - K) / S + 1;\n    TORCH_CHECK(H_out > 0 && W_out > 0, \"Computed output size is non-positive; check kernel_size/stride/padding\");\n\n    auto y = torch::empty({N, C, H_out, W_out}, x.options());\n\n    int threads = 256;\n    int64_t total = N * C * H_out * W_out;\n    int blocks = static_cast<int>((total + threads - 1) / threads);\n\n    cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n\n    avg_pool2d_nchw_kernel<<<blocks, threads, 0, stream>>>(\n        x.data_ptr<float>(),\n        y.data_ptr<float>(),\n        N, C, H, W,\n        H_out, W_out,\n        K, S\n    );\n\n    auto err = cudaGetLastError();\n    TORCH_CHECK(err == cudaSuccess, \"avg_pool2d_nchw_kernel launch failed: \", cudaGetErrorString(err));\n\n    return y;\n}\n```",
    "cpp_code_extracted": true,
    "cuda_code_extracted": true,
    "error": ""
  }
]